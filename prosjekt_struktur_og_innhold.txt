Prosjektstruktur og filinnhold (fra mappen: C:\Users\kriny\andre_fag\pattern_recognition\Directory\relearning)

Skriptet 'all.py' og outputfilen 'prosjekt_struktur_og_innhold.txt' er ekskludert.
Følgende mappenavn blir også fullstendig ignorert (i tillegg til andre skjulte mapper som starter med '.'): .git, __pycache__, .vscode, .idea, venv, env, .venv, node_modules

================================================================================

    |-- description.md
        --- START INNHOLD (description.md) ---
        *Turn-based Reinforcement-Learning playground*
        
        The goal is to train an AI to play free-for-all W.O.R.M.S. This means that each agent controls one worm, and the goal is to kill all other opponents.
        
        I want to be able to test the game against the AI agents. That is why I chose to modularize into components that communicate through WebSockets:
        - server / environment: manages communication with clients and simulates the environment  
        - CLI client / AI agent: chooses actions based on the given environment, and learns  
        - Web client: user can take the role as a player and manually choose actions against the AI; visualized via a simple browser interface
        
        ---
        
        ## Scoring
        
        Each action yields a numeric **reward** which agents use to learn:
        
        - **Damage points**  
          You earn points equal to the actual HP removed from any opponent.  
          - _Example_: you bazooka-hit a worm at 30 HP with a 60-damage shot → you earn **30 points**.
        
        - **Kill bonus**  
          If an action reduces an opponent’s HP to exactly 0, you get an extra **100 points**.  
          - _Example_: you kick-hit a worm at 1 HP (kick = 80 damage) → you earn **1 (damage) + 100 (kill bonus) = 101 points**.
        
        ---
        
        ## 1  High-level Picture
        
        ```
        
        ┌──────────┐      WebSocket/JSON       ┌─────────────┐
        │  Browser │◀─────────────────────────▶│  Server     │
        │ (Pixi UI)│                           │ (env + core)│
        └────┬─────┘                           └────┬────────┘
        │ WebSocket/JSON                        │
        │                                       │
        ▼                                       ▼
        ┌──────────┐      WebSocket/JSON       ┌─────────────┐
        │  RL Bot  │◀─────────────────────────▶│  GameCore   │
        │ (Python) │                           │  (pure logic)
        └──────────┘                           └─────────────┘
        
        ````
        
        * **Server / Environment** (`environment/`)  
          Runs a match, owns the authoritative **`GameCore`** logic, and speaks the JSON protocol over WebSockets.  
        
        * **Agents** (`agents/`)  
          CLI clients—human or reinforcement-learning models—that connect, read `TURN_BEGIN`, decide an `ACTION`, and learn from `TURN_RESULT`.  
        
        * **Web Client** (`frontend/`)  
          Pixi.js interface for humans: shows the map full-width, lets you pick actions via dropdown + dynamic parameter inputs, and renders worms in sub-tile positions.
        
        Everything communicates through **one common surface**: the JSON messages documented in `json-docs.md`.
        
        ---
        
        ## 2  Runtime Flow
        
        1. **CONNECT** Clients open `ws://127.0.0.1:8765` and send  
           ```json
           { "type": "CONNECT", "nick": "<name>" }
        ````
        
        The server replies `ASSIGN_ID`.
        
        2. **Turn loop**
        
           ```
           TURN_BEGIN → ACTION → TURN_RESULT → TURN_END
           ```
        
           repeats until `GAME_OVER`.
        
        3. Clients render or learn from the **Game State** snapshot inside each `TURN_BEGIN` / `TURN_RESULT`. The state contains:
        
           * `worms` with **world-unit** `(x,y)` floats
           * `map` grid with `1 = terrain`, `0 = empty`
        
        4. **Physics & rules** (fully in `GameCore.step()`).
        
        ---
        
        ## 3  Coordinate System
        
        * **Tile grid** Index `(0,0)` is top-left.
        * **World units** 1 world-unit ≙ 1 tile; worms can be at `3.2, 1.75`, etc.
        * **Screen mapping** (client side):
        
          ```js
          tile   = Math.min(canvasW / cols, canvasH / rows);
          offset = [(canvasW - tile*cols)/2, (canvasH - tile*rows)/2];
          pixelX = offset[0] + xWorld * tile;
          pixelY = offset[1] + yWorld * tile;
          ```
        
        ---
        
        ## 4  Components in Detail
        
        | Folder         | Key files                            | Role                                  |
        | -------------- | ------------------------------------ | ------------------------------------- |
        | `environment/` | `server.py`, `game_core.py`          | Match orchestration + pure game logic |
        | `agents/`      | `client.py` (baseline random bot)    | Example RL agent; train & act here    |
        | `frontend/`    | `src/script.js`, `public/index.html` | Pixi.js UI + dynamic action form      |
        | root           | `json-docs.md`                       | Authoritative protocol spec           |
        
        ### Build / Run Quick-start
        
        ```bash
        # Python side
        cd environment && python server.py
        
        # Web UI
        cd frontend
        npm install        # (once)
        npx esbuild src/script.js --bundle --outfile=public/bundle.js --format=esm
        npx serve public
        
        # Bot
        cd agents && python client.py
        ```
        --- SLUTT INNHOLD (description.md) ---

    |-- json-docs.md
        --- START INNHOLD (json-docs.md) ---
        # W.O.R.M.S. WebSocket JSON Protocol
        
        All messages are JSON objects sent over a WebSocket.  
        Each object **must** include a `"type"` field that selects one of the structures below.  
        Clients **must ignore** unknown keys so the protocol can evolve.
        
        ---
        
        ## Overall flow
        
        ```text
                    Client                                  Server
                     │                                         │
                     ├─▶  CONNECT                              │
                     │     (nick)                              │
                     │                                         │
                     │   ASSIGN_ID  ◀──────────────────────────┤
                     │                                         │
              repeat │                                         │
              until  │  TURN_BEGIN ───────────────────────────▶│
           GAME_OVER │  ACTION       ◀──────────────────────── │
                     │  TURN_RESULT ─────────────────────────▶ │
                     │  TURN_END    ─────────────────────────▶ │  ← Now sent after each turn
                     │                                         │
                     └─▶ GAME_OVER   ◀─────────────────────────┤
        ````
        
        ---
        
        ## 1  Messages
        
        ### 1.1 CONNECT  (client → server)
        
        | Field       | Type    | Required | Description                       |
        | ----------- | ------- | -------- | --------------------------------- |
        | `type`      | string  | ✓        | `"CONNECT"`                       |
        | `nick`      | string  | ✗        | Human nickname (for logs / UI)    |
        | `spectator` | boolean | ✗        | `true` to observe without playing |
        
        ```json
        { "type": "CONNECT", "nick": "bot-42" }
        ```
        
        ---
        
        ### 1.2 ASSIGN\_ID  (server → client)
        
        | Field       | Type    | Required | Description                                     |
        | ----------- | ------- | -------- | ----------------------------------------------- |
        | `type`      | string  | ✓        | `"ASSIGN_ID"`                                   |
        | `player_id` | integer | ✗        | Omitted for spectators; starts at 1 for players |
        
        ```json
        { "type": "ASSIGN_ID", "player_id": 2 }
        ```
        
        ---
        
        ### 1.3 TURN\_BEGIN  (server → all)
        
        | Field           | Type       | Required | Description                        |
        | --------------- | ---------- | -------- | ---------------------------------- |
        | `type`          | string     | ✓        | `"TURN_BEGIN"`                     |
        | `turn_index`    | integer    | ✓        | 0-based global turn counter        |
        | `player_id`     | integer    | ✓        | ID whose turn it is                |
        | `state`         | Game State | ✓        | Full snapshot                      |
        | `time_limit_ms` | integer    | ✓        | How long that player has to answer |
        
        ---
        
        ### 1.4 ACTION  (client → server)
        
        Sent **only** by the `player_id` that just received `TURN_BEGIN`.
        
        | Field       | Type   | Required | Description               |
        | ----------- | ------ | -------- | ------------------------- |
        | `type`      | string | ✓        | `"ACTION"`                |
        | `player_id` | int    | ✓        | Must match sender’s ID    |
        | `action`    | object | ✓        | One of the variants below |
        
        #### `action` variants
        
        | Variant         | Required keys                                                     | Notes                                                 |
        | --------------- | ----------------------------------------------------------------- | ----------------------------------------------------- |
        | stand           | `{ "action": "stand" }`                                           |                                                       |
        | walk            | `{ "action": "walk", "dx": float }`                               | `dx` in world units (+ → right); **max dx = 2**       |
        | attack\:kick    | `{ "action": "attack", "weapon": "kick" }`                        | Flat 80 damage to the first living worm within 1 unit |
        | attack\:bazooka | `{ "action": "attack", "weapon": "bazooka", "angle_deg": float }` | 0° = right, CCW positive                              |
        | attack\:grenade | `{ "action": "attack", "weapon": "grenade", "dx": float }`        | Single `dx` (max dx = 5); horizontal distance only    |
        
        Gravity is applied **instantly** only after a **walk** action: the worm falls in the same column until it lands on the first solid tile (`map[row][col] == 1`) or exits below the last row (water), which sets its `health` to `0`.
        
        ---
        
        ### 1.5 TURN\_RESULT  (server → all)
        
        | Field        | Type       | Required | Description                                    |
        | ------------ | ---------- | -------- | ---------------------------------------------- |
        | `type`       | string     | ✓        | `"TURN_RESULT"`                                |
        | `turn_index` | integer    | ✓        | Same index as the triggering turn              |
        | `player_id`  | integer    | ✓        | The acting player                              |
        | `state`      | Game State | ✓        | Resulting state                                |
        | `reward`     | number     | ✗        | Points earned this turn:                       |
        |              |            |          | • actual HP removed from any target(s)         |
        |              |            |          | • **+100** if the action killed a worm         |
        | `effects`    | object     | ✗        | Optional visual-only data (weapon, trajectory) |
        
        ---
        
        ### 1.6 TURN\_END  (server → all)
        
        | Field            | Type    | Required | Description          |
        | ---------------- | ------- | -------- | -------------------- |
        | `type`           | string  | ✓        | `"TURN_END"`         |
        | `next_player_id` | integer | ✓        | ID who will act next |
        
        ---
        
        ### 1.7 PLAYER\_ELIMINATED  (server → all)
        
        | Field       | Type    | Required | Description                 |
        | ----------- | ------- | -------- | --------------------------- |
        | `type`      | string  | ✓        | `"PLAYER_ELIMINATED"`       |
        | `player_id` | integer | ✓        | ID of the eliminated player |
        
        ---
        
        ### 1.8 GAME\_OVER  (server → all)
        
        | Field         | Type       | Required | Description        |
        | ------------- | ---------- | -------- | ------------------ |
        | `type`        | string     | ✓        | `"GAME_OVER"`      |
        | `winner_id`   | integer    | ✗        | Winner’s ID if any |
        | `final_state` | Game State | ✓        | Final snapshot     |
        
        ---
        
        ### 1.9 ERROR  (server → client)
        
        | Field  | Type   | Required | Description         |
        | ------ | ------ | -------- | ------------------- |
        | `type` | string | ✓        | `"ERROR"`           |
        | `msg`  | string | ✓        | Human-readable text |
        
        ---
        
        ## 2  Game State
        
        ```jsonc
        {
          "worms": [
            { "id": 0, "nick": "Alice", "health": 100, "x": 3.20, "y": 1.75 },
            { "id": 1, "nick": "Bot-42", "health":  90, "x": 6.00, "y": 2.00 }
          ],
          "map": [
            [1,0,0,0,0,0,0,0],
            [1,0,0,0,0,0,0,0],
            [1,1,1,0,0,0,1,0],
            [1,1,1,0,0,1,1,1]
          ]
        }
        ```
        
        | Key     | Type          | Description                                                                                                        |
        | ------- | ------------- | ------------------------------------------------------------------------------------------------------------------ |
        | `worms` | array         | Each worm’s **`id`**, **`nick`**, **`health`**, and **position** in world units (`x`, `y` floats; 1 unit = 1 tile) |
        | `map`   | 2-D int array | `1` = solid terrain, `0` = empty air. Values below the last row kill a worm instantly when fallen into water.      |
        
        Clients **must ignore** any extra keys they do not understand.
        
        ---
        
        ## 3  Rendering rule (client hint)
        
        ```text
        tile = min(canvasW / cols, canvasH / rows)
        xMargin = (canvasW − tile*cols)/2
        yMargin = (canvasH − tile*rows)/2
        draw tile(i,j) at (xMargin + i*tile , yMargin + j*tile)
        ```
        --- SLUTT INNHOLD (json-docs.md) ---

    |-- plan.md
        --- START INNHOLD (plan.md) ---
        - **learn**: runs and saves state of environment
        - **data**: saved here
        - **frontend**: reads data and plays it in a browser
        --- SLUTT INNHOLD (plan.md) ---

    |-- todo.md
        --- START INNHOLD (todo.md) ---
        max time for a game to last, incase of bugs
        --- SLUTT INNHOLD (todo.md) ---

agents/
    |-- __init__.py
        [--- INNHOLD: Tom fil ---]

    |-- client.py
        --- START INNHOLD (client.py) ---
        #!/usr/bin/env python3
        """
        Reference bot that now keeps its WebSocket open across many games.
        It rests while eliminated and wakes up when NEW_GAME arrives.
        """
        import argparse
        import asyncio
        import json
        import logging
        import random
        
        import websockets
        
        def setup_logging() -> logging.Logger:
            parser = argparse.ArgumentParser(description="W.O.R.M.S. bot client")
            parser.add_argument(
                "--log-level",
                choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                default="INFO",
                help="set logging level",
            )
            args = parser.parse_args()
            level = getattr(logging, args.log_level)
            logging.basicConfig(
                level=level,
                format="%(asctime)s %(levelname)-8s %(name)s: %(message)s",
                datefmt="%H:%M:%S",
            )
            return logging.getLogger("client")
        
        logger = setup_logging()
        
        HOST, PORT = "127.0.0.1", 8765
        
        ACTIONS = [
            {"action": "stand"},
            {"action": "walk", "dx": 1.0},
            {"action": "attack", "weapon": "kick"},
            {"action": "attack", "weapon": "bazooka", "angle_deg": 30.0},
            {"action": "attack", "weapon": "grenade", "dx": 2.0},
        ]
        
        async def start_client() -> None:
            uri = f"ws://{HOST}:{PORT}"
            logger.info("connecting to %s", uri)
        
            async with websockets.connect(uri) as ws:
                await ws.send(json.dumps({"type": "CONNECT", "nick": "bot"}))
                player_id: int | None = None
                eliminated = False
        
                async for raw in ws:
                    msg = json.loads(raw)
                    t = msg.get("type")
        
                    if t == "ASSIGN_ID":
                        player_id = msg["player_id"]
                        logger.info("assigned player_id=%d", player_id)
        
                    elif t == "PLAYER_ELIMINATED" and msg.get("player_id") == player_id:
                        eliminated = True
                        logger.info("I have been eliminated this game")
        
                    elif t == "NEW_GAME":
                        eliminated = False
                        logger.info("new episode %s started – back in the game!", msg.get("game_id"))
        
                    elif t == "TURN_BEGIN" and msg.get("player_id") == player_id and not eliminated:
                        action = random.choice(ACTIONS)
                        payload = {"type": "ACTION", "player_id": player_id, "action": action}
                        await ws.send(json.dumps(payload))
                        logger.debug("did action: %r", payload)
        
                    elif t == "TURN_RESULT" and msg.get("player_id") == player_id:
                        reward = msg.get("reward", 0.0)
                        logger.info("received reward=%.1f", reward)
        
        if __name__ == "__main__":
            asyncio.run(start_client())
        --- SLUTT INNHOLD (client.py) ---

    a2c_manual/
        |-- __init__.py
            [--- INNHOLD: Tom fil ---]

        |-- agent.py
            --- START INNHOLD (agent.py) ---
            # agents/a2c_manual/agent.py
            import torch
            import torch.optim as optim
            import torch.nn.functional as F
            from torch.distributions import Categorical, Normal
            import numpy as np
            from pathlib import Path
            
            from . import config
            from .model import ActorCriticNetwork
            from .utils import preprocess_state, format_action
            
            
            class A2CAgent:
                def __init__(self, agent_name="A2C_Agent_Default"):
                    self.network = ActorCriticNetwork().to(config.DEVICE)
                    self.optimizer = optim.Adam(self.network.parameters(), lr=config.LEARNING_RATE)
                    self.player_id = None  # Settes av main_a2c.py ved ASSIGN_ID
                    self.agent_name = agent_name  # For logging og unike sjekkpunktfiler
            
                    # Buffere for ett læringssteg (tømmes etter hver .learn())
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.entropies_buffer = []
            
                def set_player_id(self, player_id: int):
                    self.player_id = player_id
                    # print(f"[{self.agent_name}] Player ID satt til: {self.player_id}")
            
                def select_action(self, current_game_state_json: dict):
                    if self.player_id is None:
                        # print(f"[{self.agent_name}] FEIL select_action: Player ID ikke satt. Sender 'stand'.")
                        return {"action": "stand"}
            
                    # Sjekk om vår orm er i live før vi gjør noe
                    my_worm_json_id = self.player_id - 1
                    is_my_worm_alive = any(
                        w['id'] == my_worm_json_id and w['health'] > 0
                        for w in current_game_state_json.get('worms', [])
                    )
                    if not is_my_worm_alive:
                        # print(f"[{self.agent_name}] Info select_action: Min orm (P{self.player_id}) er ikke lenger i live. Sender 'stand'.")
                        # Det er viktig å lagre noe i bufferne selv om vi sender stand, slik at learn() ikke krasjer
                        # pga. ulik bufferlengde. En dummy-verdi (f.eks. for V(s)) er nok.
                        # Siden vi ikke tar en reell handling, lagrer vi ikke log_prob eller entropi.
                        # Value for en "død" state kan anses som 0.
                        self.values_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        return {"action": "stand"}
            
                    map_tensor, worm_vector_tensor = preprocess_state(current_game_state_json, self.player_id)
            
                    try:
                        actor_outputs, state_value_tensor = self.network(map_tensor, worm_vector_tensor)
                    except Exception as e:
                        print(f"[{self.agent_name}] FEIL under network forward pass i select_action: {e}")
                        # Fallback for å unngå krasj, og lagre dummy value
                        self.values_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        return {"action": "stand"}
            
                    self.values_buffer.append(state_value_tensor.squeeze())  # Skalar tensor
            
                    action_type_probs = actor_outputs['action_type_probs']
                    action_type_dist = Categorical(action_type_probs)
                    network_action_idx_tensor = action_type_dist.sample()  # 0-dim tensor
                    network_action_idx_item = network_action_idx_tensor.item()
            
                    log_prob_action_type = action_type_dist.log_prob(network_action_idx_tensor)  # Skalar
                    entropy_action_type = action_type_dist.entropy()  # Skalar
            
                    step_log_probs = [log_prob_action_type]
                    step_entropies = [entropy_action_type]
                    params_for_formatting = {}
                    chosen_network_action_name = config.NETWORK_ACTION_ORDER[network_action_idx_item]
            
                    if chosen_network_action_name == 'walk':
                        walk_dx_probs = actor_outputs['walk_dx_probs']
                        walk_dx_dist = Categorical(walk_dx_probs)
                        walk_dx_bin_idx_tensor = walk_dx_dist.sample()
                        params_for_formatting['walk_dx_bin_idx'] = walk_dx_bin_idx_tensor.item()
                        step_log_probs.append(walk_dx_dist.log_prob(walk_dx_bin_idx_tensor))
                        step_entropies.append(walk_dx_dist.entropy())
            
                    elif chosen_network_action_name == 'attack_kick':
                        # Ingen parametere å sample for kick iht. nye specs
                        pass
            
                    elif chosen_network_action_name == 'attack_bazooka':
                        angle_mean, angle_std = actor_outputs['bazooka_params']
                        dist = Normal(angle_mean.squeeze(), angle_std.squeeze())
                        angle_val_tensor = dist.sample()
                        params_for_formatting['bazooka_angle_val'] = angle_val_tensor.item()
                        step_log_probs.append(dist.log_prob(angle_val_tensor))
                        step_entropies.append(dist.entropy())
            
                    elif chosen_network_action_name == 'attack_grenade':
                        grenade_dx_probs = actor_outputs['grenade_dx_probs']
                        grenade_dx_dist = Categorical(grenade_dx_probs)
                        grenade_dx_bin_idx_tensor = grenade_dx_dist.sample()
                        params_for_formatting['grenade_dx_bin_idx'] = grenade_dx_bin_idx_tensor.item()
                        step_log_probs.append(grenade_dx_dist.log_prob(grenade_dx_bin_idx_tensor))
                        step_entropies.append(grenade_dx_dist.entropy())
            
                    try:
                        # Sikre at alle elementer er skalar-tensorer før stack
                        step_log_probs = [lp.squeeze() for lp in step_log_probs]
                        step_entropies = [e.squeeze() for e in step_entropies]
            
                        stacked_log_probs = torch.stack(step_log_probs)
                        summed_log_probs = stacked_log_probs.sum()
                        self.log_probs_buffer.append(summed_log_probs)
            
                        stacked_entropies = torch.stack(step_entropies)
                        summed_entropies = stacked_entropies.sum()
                        self.entropies_buffer.append(summed_entropies)
                    except Exception as e_stack:
                        print(f"[{self.agent_name}] FEIL select_action stacking/summing: {e_stack}")
                        # Fallback for å opprettholde buffer-integritet
                        self.log_probs_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        self.entropies_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
            
                    action_json_to_send = format_action(network_action_idx_item, params_for_formatting)
                    return action_json_to_send
            
                def store_reward(self, reward: float):
                    self.rewards_buffer.append(reward)
            
                def learn(self, next_game_state_json: dict | None, done: bool):
                    # Kritisk sjekk: Antall lagrede verdier må matche antall lagrede rewards.
                    # Hver 'select_action' legger til i log_probs, values, entropies.
                    # Hver 'store_reward' (som kalles etter en handling) legger til i rewards.
                    # Så lengden på rewards_buffer skal være lik lengden på de andre bufferne
                    # *før* vi legger til V(s_next) eller gjør noe med returns.
            
                    num_rewards = len(self.rewards_buffer)
                    consistent_buffers = (
                            len(self.log_probs_buffer) == num_rewards and
                            len(self.values_buffer) == num_rewards and  # values_buffer har V(s_0) ... V(s_T-1)
                            len(self.entropies_buffer) == num_rewards
                    )
            
                    if not consistent_buffers or num_rewards == 0:
                        # print(f"[{self.agent_name}] DEBUG learn: Ujevne eller tomme buffere. Tømmer og returnerer.")
                        # print(f"  LP:{len(self.log_probs_buffer)} V:{len(self.values_buffer)} R:{len(self.rewards_buffer)} E:{len(self.entropies_buffer)}")
                        self.clear_buffers()
                        return None, None, None
            
                    R_bootstrap = torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32)
                    if not done:
                        if next_game_state_json and self.player_id is not None:
                            my_worm_json_id = self.player_id - 1
                            is_my_worm_alive_in_next_state = any(
                                w['id'] == my_worm_json_id and w['health'] > 0
                                for w in next_game_state_json.get('worms', [])
                            )
                            if is_my_worm_alive_in_next_state:
                                if isinstance(next_game_state_json, dict) and \
                                        next_game_state_json.get("map") and next_game_state_json.get("worms"):
                                    next_map, next_worm = preprocess_state(next_game_state_json, self.player_id)
                                    with torch.no_grad():
                                        _, next_value_tensor = self.network(next_map, next_worm)
                                        R_bootstrap = next_value_tensor.squeeze()
            
                    returns = []
                    R_discounted = R_bootstrap
                    for r_idx in range(num_rewards - 1, -1, -1):
                        reward_val = torch.tensor(self.rewards_buffer[r_idx], device=config.DEVICE, dtype=torch.float32)
                        R_discounted = reward_val + config.GAMMA * R_discounted
                        returns.insert(0, R_discounted)
            
                    if not returns:  # Bør ikke skje hvis num_rewards > 0
                        self.clear_buffers()
                        return None, None, None
            
                    try:
                        returns_tensor = torch.stack(returns).detach()
                        values_tensor = torch.stack(self.values_buffer)  # values_buffer skal ha num_rewards elementer
                        log_probs_tensor = torch.stack(self.log_probs_buffer)
                        entropies_tensor = torch.stack(self.entropies_buffer)
                    except RuntimeError as e_stack:
                        print(f"[{self.agent_name}] FEIL learn (stack): {e_stack}")
                        print(
                            f"  R:{len(returns)}, V:{len(self.values_buffer)}, LP:{len(self.log_probs_buffer)}, E:{len(self.entropies_buffer)}")
                        self.clear_buffers()
                        return None, None, None
            
                    # Nå SKAL alle tensorer ha samme lengde (num_rewards)
                    if not (returns_tensor.shape[0] == values_tensor.shape[0] == \
                            log_probs_tensor.shape[0] == entropies_tensor.shape[0] == num_rewards):
                        print(f"[{self.agent_name}] KRITISK FEIL learn: Tensorstørrelser stemmer ikke etter stack!")
                        print(
                            f"  Shape R:{returns_tensor.shape}, V:{values_tensor.shape}, LP:{log_probs_tensor.shape}, E:{entropies_tensor.shape}, Expected:{num_rewards}")
                        self.clear_buffers()
                        return None, None, None
            
                    advantages = returns_tensor - values_tensor
                    if advantages.numel() > 1:
                        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
                    policy_loss = (-log_probs_tensor * advantages.detach()).mean()
                    value_loss = F.mse_loss(values_tensor, returns_tensor)
                    entropy_loss = -entropies_tensor.mean()
            
                    total_loss = policy_loss + \
                                 config.VALUE_LOSS_COEF * value_loss + \
                                 config.ENTROPY_COEF * entropy_loss
            
                    self.optimizer.zero_grad()
                    total_loss.backward()
                    if hasattr(config, 'MAX_GRAD_NORM') and config.MAX_GRAD_NORM is not None:
                        torch.nn.utils.clip_grad_norm_(self.network.parameters(), config.MAX_GRAD_NORM)
                    self.optimizer.step()
            
                    # print(f"[{self.agent_name}] Lært. Losses - P:{policy_loss.item():.3f} V:{value_loss.item():.3f} E:{entropy_loss.item():.3f}")
                    self.clear_buffers()
                    return policy_loss.item(), value_loss.item(), entropy_loss.item()
            
                def clear_buffers(self):
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.entropies_buffer = []
            
                def save_model(self, path_str: str):
                    path = Path(path_str)
                    path.parent.mkdir(parents=True, exist_ok=True)  # Sikre at mappen finnes
                    try:
                        torch.save(self.network.state_dict(), path)
                        # print(f"[{self.agent_name}] Modell lagret til {path}")
                    except Exception as e:
                        print(f"[{self.agent_name}] Kunne ikke lagre modell til {path}: {e}")
            
                def load_model(self, path_str: str):
                    path = Path(path_str)
                    if not path.exists():
                        print(f"[{self.agent_name}] Ingen modell funnet på {path}, starter med ny/tilfeldig initialisert modell.")
                        return
            
                    try:
                        self.network.load_state_dict(torch.load(path, map_location=torch.device(config.DEVICE)))
                        self.network.eval()  # Sett til evaluation mode etter lasting
                        print(f"[{self.agent_name}] Modell lastet fra {path}")
                    except Exception as e:
                        print(f"[{self.agent_name}] Kunne ikke laste modell fra {path}: {e}. Bruker ny modell.")
            --- SLUTT INNHOLD (agent.py) ---

        |-- config.py
            --- START INNHOLD (config.py) ---
            # agents/a2c_manual/config.py
            import torch
            
            # ---- Kartdimensjoner og Normalisering ----
            MAP_WIDTH = 250  # Forventet maks bredde for CNN-padding/cropping
            MAP_HEIGHT = 250 # Forventet maks høyde for CNN-padding/cropping
            # utils.py vil håndtere faktiske kartstørrelser for normalisering.
            
            MAX_WORM_HEALTH = 100.0
            
            # ---- Modell Dimensjoner ----
            CNN_INPUT_CHANNELS = 1
            # Gitt AdaptiveMaxPool2d((6, 6)) og 32 output kanaler fra conv2, blir dette 32*6*6 = 1152
            CNN_FEATURE_DIM = 1152
            
            ACTIVE_WORM_FEATURE_DIM = 3  # Egen orms: health, x, y (normalisert)
            # TODO: Utvid senere til å inkludere info om andre ormer hvis ønskelig
            WORM_VECTOR_DIM = ACTIVE_WORM_FEATURE_DIM
            COMBINED_FEATURE_DIM = CNN_FEATURE_DIM + WORM_VECTOR_DIM
            
            # ---- Action Space Definisjoner (basert på ny json-docs.md) ----
            # Rekkefølgen nettverket ser handlingene i:
            NETWORK_ACTION_ORDER = ['stand', 'walk', 'attack_kick', 'attack_bazooka', 'attack_grenade']
            ACTION_DIM = len(NETWORK_ACTION_ORDER)
            
            # Mapping fra nettverkets actionnavn til serverens JSON-format
            SERVER_ACTION_MAPPING = {
                'stand': {'action': 'stand'},
                'walk': {'action': 'walk'},  # 'dx' parameter legges til dynamisk
                'attack_kick': {'action': 'attack', 'weapon': 'kick'}, # Ingen 'force' fra klient lenger
                'attack_bazooka': {'action': 'attack', 'weapon': 'bazooka'},  # 'angle_deg' parameter
                'attack_grenade': {'action': 'attack', 'weapon': 'grenade'}  # 'dx' parameter (erstatter angle/force)
            }
            
            # ---- Parameter Space for Nettverket ----
            # Walk 'dx'. Server maks er +/-2.0.
            WALK_DX_BINS = 5  # Gir f.eks. bins for [-2.0, -1.0, 0.0, 1.0, 2.0]
            WALK_DX_MIN = -2.0
            WALK_DX_MAX = 2.0
            
            # Kick: Ingen parametere som predikeres av nettverket (ingen 'force' fra klient).
            
            # Bazooka 'angle_deg' (kontinuerlig: nettverket outputer mean og std for Normalfordeling)
            BAZOOKA_ANGLE_PARAMS = 1 # 1 for mean, 1 for std (totalt 2 noder i modellen for dette)
            
            # Grenade 'dx'. Server maks er +/-5.0.
            GRENADE_DX_BINS = 11 # Gir f.eks. bins for [-5.0, -4.0, ..., 0.0, ..., 4.0, 5.0]
            GRENADE_DX_MIN = -5.0
            GRENADE_DX_MAX = 5.0
            
            # ---- A2C Hyperparametre ----
            LEARNING_RATE = 0.0003 # Ofte en god startverdi for A2C
            GAMMA = 0.99  # Discount factor
            ENTROPY_COEF = 0.01
            VALUE_LOSS_COEF = 0.5
            MAX_GRAD_NORM = 0.5 # For gradient clipping
            
            # ---- Trening & Lagring ----
            # Serveren kjører nå kontinuerlig spill. Klienten bør lagre periodisk.
            # NUM_EPISODES i main_a2c.py styrer hvor mange *spill* hver agent prøver å fullføre
            # før den potensielt avslutter sin egen økt (men serveren fortsetter).
            # For kontinuerlig trening kan man sette NUM_EPISODES veldig høyt eller la den kjøre evig.
            NUM_GAMES_PER_AGENT_SESSION = 10000 # Hvor mange spill en agent-instans skal sikte mot
            
            SAVE_MODEL_EVERY_N_GAMES = 50   # Lagre modell etter X antall *fullførte spill* for denne agenten
            PLOT_STATS_EVERY_N_GAMES = 10   # Hvor ofte generere/oppdatere plot
            
            # ---- Websocket ----
            SERVER_HOST = '127.0.0.1'
            SERVER_PORT = 8765
            
            # ---- Annet ----
            DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
            # DEVICE = "cpu" # Kan overstyres for debugging
            print(f"Bruker enhet: {DEVICE}")
            --- SLUTT INNHOLD (config.py) ---

        |-- main_a2c.py
            --- START INNHOLD (main_a2c.py) ---
            # agents/a2c_manual/main_a2c.py
            import os
            
            # Prøv å sette denne FØR andre importer for å håndtere OMP-feilen
            # Dette er en workaround og kan skjule underliggende problemer i sjeldne tilfeller.
            os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
            
            import asyncio
            import websockets
            import json
            import torch
            import numpy as np
            import argparse
            from pathlib import Path
            import signal
            import matplotlib
            
            matplotlib.use('Agg')  # Bytt til Agg backend FØR pyplot importeres for ikke-interaktiv plotting
            import matplotlib.pyplot as plt
            import random  # For random sleep
            
            from .agent import A2CAgent  # Relativ import
            from . import config  # Relativ import
            
            # --- Global logging for plotting ---
            TRAINING_STATS = {}  # {agent_id_str: {"game_rewards": [], "policy_losses": [], ...}}
            PLOT_DIR = Path(__file__).resolve().parent / "training_plots_output"
            PLOT_DIR.mkdir(parents=True, exist_ok=True)
            
            SHUTDOWN_FLAG = asyncio.Event()
            
            
            def signal_handler_main(signum, frame):
                if not SHUTDOWN_FLAG.is_set():
                    print("\n(Hovedkoordinator) Mottok avslutningssignal (Ctrl+C). Setter shutdown flagg...")
                    SHUTDOWN_FLAG.set()
                else:
                    print("(Hovedkoordinator) Avslutningssignal allerede mottatt...")
            
            
            async def run_single_agent_session(agent_id_str: str, checkpoint_filename: str):
                uri = f"ws://{config.SERVER_HOST}:{config.SERVER_PORT}"
                agent_name = f"A2C_Agent_{agent_id_str}"
                a2c_agent = A2CAgent(agent_name=agent_name)
            
                if agent_id_str not in TRAINING_STATS:
                    TRAINING_STATS[agent_id_str] = {
                        "game_rewards_raw": [],  # Belønning for hvert spill
                        "policy_loss_per_game": [],  # Gj.snitt policy loss for hvert spill
                        "value_loss_per_game": [],  # Gj.snitt value loss for hvert spill
                        "entropy_loss_per_game": [],  # Gj.snitt entropy loss for hvert spill
                        "games_played_count": 0,
                        "total_steps_across_games": 0
                    }
            
                checkpoint_path = Path(__file__).resolve().parent / checkpoint_filename
                a2c_agent.load_model(checkpoint_path)
            
                connection_attempts = 0
                max_connection_attempts = 10  # Litt mer tålmodig
            
                while not SHUTDOWN_FLAG.is_set() and \
                        TRAINING_STATS[agent_id_str]["games_played_count"] < config.NUM_GAMES_PER_AGENT_SESSION:
            
                    # Buffere for ett enkelt spill
                    temp_policy_losses = []
                    temp_value_losses = []
                    temp_entropy_losses = []
                    current_game_step_rewards = []  # Belønninger for hvert steg i dette spillet
            
                    a2c_agent.clear_buffers()  # Sørg for at agentens interne buffere er tomme
            
                    current_game_id = None
                    is_my_turn_flag = False
                    am_i_eliminated_this_game = False
                    last_known_state_for_learn = None
                    steps_this_game = 0
            
                    # print(f"[{agent_name}] Venter på nytt spill / prøver å koble til...")
                    try:
                        async with websockets.connect(uri, ping_interval=20, ping_timeout=30, open_timeout=15) as websocket:
                            connection_attempts = 0
                            await websocket.send(json.dumps({"type": "CONNECT", "nick": agent_name}))
            
                            assign_id_msg_str = await asyncio.wait_for(websocket.recv(), timeout=10)
                            assign_id_msg = json.loads(assign_id_msg_str)
            
                            if assign_id_msg.get("type") == "ASSIGN_ID":
                                player_id = assign_id_msg.get("player_id")
                                if player_id is None:
                                    print(f"[{agent_name}] FEIL: Fikk ikke player_id. Venter.")
                                    await asyncio.sleep(random.uniform(3, 7))
                                    continue
                                a2c_agent.set_player_id(player_id)
                            else:
                                print(f"[{agent_name}] FEIL: Forventet ASSIGN_ID, fikk: {assign_id_msg}. Venter.")
                                await asyncio.sleep(random.uniform(3, 7))
                                continue
            
                            # print(f"[{agent_name}] Tilkoblet. Min Player ID: {a2c_agent.player_id}.")
            
                            async for message_str in websocket:
                                if SHUTDOWN_FLAG.is_set(): break
                                try:
                                    msg = json.loads(message_str)
                                    msg_type = msg.get("type")
            
                                    if msg_type == "NEW_GAME":
                                        new_game_id = msg.get("game_id")
                                        # Hvis vi var midt i et spill, og det ikke ble fullført (ingen GAME_OVER/PLAYER_ELIMINATED)
                                        # og vi har data, prøv å lære fra det (mindre ideelt, men bedre enn ingenting).
                                        if current_game_id is not None and current_game_id != new_game_id and \
                                                not am_i_eliminated_this_game and a2c_agent.rewards_buffer:
                                            print(
                                                f"[{agent_name}] Uventet NEW_GAME (ID: {new_game_id}) før GAME_OVER for spill {current_game_id}. Prøver å lære fra ufullstendig data.")
                                            p_l, v_l, e_l = a2c_agent.learn(last_known_state_for_learn, True)  # Anta done=True
                                            if p_l is not None:
                                                temp_policy_losses.append(p_l)
                                                temp_value_losses.append(v_l)
                                                temp_entropy_losses.append(e_l)
                                            # Logg dette "ufullstendige" spillet
                                            TRAINING_STATS[agent_id_str]['game_rewards_raw'].append(sum(current_game_step_rewards))
                                            if temp_policy_losses: TRAINING_STATS[agent_id_str]['policy_loss_per_game'].append(
                                                np.mean(temp_policy_losses))
                                            if temp_value_losses: TRAINING_STATS[agent_id_str]['value_loss_per_game'].append(
                                                np.mean(temp_value_losses))
                                            if temp_entropy_losses: TRAINING_STATS[agent_id_str]['entropy_loss_per_game'].append(
                                                np.mean(temp_entropy_losses))
                                            TRAINING_STATS[agent_id_str]["games_played_count"] += 1
                                            TRAINING_STATS[agent_id_str]["total_steps_across_games"] += steps_this_game
            
                                        current_game_id = new_game_id
                                        # print(f"[{agent_name}] --- Nytt spill (ID: {current_game_id}) for P{a2c_agent.player_id} ---")
                                        a2c_agent.clear_buffers()
                                        temp_policy_losses, temp_value_losses, temp_entropy_losses, current_game_step_rewards = [], [], [], []
                                        am_i_eliminated_this_game = False
                                        is_my_turn_flag = False
                                        last_known_state_for_learn = msg.get("state")
                                        steps_this_game = 0
            
                                    elif msg_type == "TURN_BEGIN":
                                        if current_game_id is None: continue
            
                                        game_state_for_action = msg.get("state")
                                        last_known_state_for_learn = game_state_for_action
            
                                        if msg.get("player_id") == a2c_agent.player_id and not am_i_eliminated_this_game:
                                            is_my_turn_flag = True
                                            steps_this_game += 1
                                            if not game_state_for_action:
                                                action_to_send_obj = {"action": "stand"}
                                                a2c_agent.values_buffer.append(torch.tensor(0.0, device=config.DEVICE))
                                            else:
                                                action_to_send_obj = a2c_agent.select_action(game_state_for_action)
            
                                            await websocket.send(json.dumps({
                                                "type": "ACTION", "player_id": a2c_agent.player_id,
                                                "action": action_to_send_obj
                                            }))
                                        else:
                                            is_my_turn_flag = False
            
                                    elif msg_type == "TURN_RESULT":
                                        if current_game_id is None: continue
                                        last_known_state_for_learn = msg.get("state")
                                        if msg.get("player_id") == a2c_agent.player_id and is_my_turn_flag:
                                            reward = msg.get("reward", 0.0)
                                            a2c_agent.store_reward(reward)
                                            current_game_step_rewards.append(reward)
                                        is_my_turn_flag = False
            
                                    elif msg_type == "PLAYER_ELIMINATED":
                                        if current_game_id is None: continue
                                        elim_player_id = msg.get("player_id")
                                        if elim_player_id == a2c_agent.player_id and not am_i_eliminated_this_game:
                                            am_i_eliminated_this_game = True
                                            if a2c_agent.rewards_buffer:  # Bare lær hvis det var noen handlinger
                                                p_l, v_l, e_l = a2c_agent.learn(last_known_state_for_learn, True)  # done=True
                                                if p_l is not None:
                                                    temp_policy_losses.append(p_l)
                                                    temp_value_losses.append(v_l)
                                                    temp_entropy_losses.append(e_l)
                                            else:
                                                a2c_agent.clear_buffers()
            
                                    elif msg_type == "GAME_OVER":
                                        if current_game_id is None: continue
            
                                        final_state_for_learn = msg.get("final_state")
                                        if not am_i_eliminated_this_game and a2c_agent.rewards_buffer:
                                            p_l, v_l, e_l = a2c_agent.learn(final_state_for_learn, True)  # done=True
                                            if p_l is not None:
                                                temp_policy_losses.append(p_l)
                                                temp_value_losses.append(v_l)
                                                temp_entropy_losses.append(e_l)
                                        elif not am_i_eliminated_this_game and not a2c_agent.rewards_buffer:
                                            a2c_agent.clear_buffers()
            
                                        # Logg statistikk for det fullførte spillet
                                        TRAINING_STATS[agent_id_str]['game_rewards_raw'].append(sum(current_game_step_rewards))
                                        if temp_policy_losses: TRAINING_STATS[agent_id_str]['policy_loss_per_game'].append(
                                            np.mean(temp_policy_losses))
                                        if temp_value_losses: TRAINING_STATS[agent_id_str]['value_loss_per_game'].append(
                                            np.mean(temp_value_losses))
                                        if temp_entropy_losses: TRAINING_STATS[agent_id_str]['entropy_loss_per_game'].append(
                                            np.mean(temp_entropy_losses))
            
                                        TRAINING_STATS[agent_id_str]["games_played_count"] += 1
                                        TRAINING_STATS[agent_id_str]["total_steps_across_games"] += steps_this_game
            
                                        games_count_this_agent = TRAINING_STATS[agent_id_str]["games_played_count"]
                                        print(
                                            f"[{agent_name}] Spill {current_game_id} ferdig. Belønning: {sum(current_game_step_rewards):.2f}. (Agent totalt {games_count_this_agent} spill, {steps_this_game} steg i dette spillet)")
            
                                        if games_count_this_agent > 0 and games_count_this_agent % config.SAVE_MODEL_EVERY_N_GAMES == 0:
                                            a2c_agent.save_model(checkpoint_path)
            
                                        if games_count_this_agent > 0 and games_count_this_agent % config.PLOT_STATS_EVERY_N_GAMES == 0:
                                            plot_aggregated_training_results()
            
                                        current_game_id = None  # Klar for neste NEW_GAME melding
            
                                    elif msg_type == "TURN_END":
                                        pass
            
                                    elif msg_type == "ERROR":
                                        print(f"[{agent_name}] FEIL fra server: {msg.get('msg')}")
            
                                except json.JSONDecodeError:
                                    print(f"[{agent_name}] FEIL: Kunne ikke dekode JSON: {message_str}")
                                except websockets.exceptions.ConnectionClosed as e_ws_msg:
                                    print(f"[{agent_name}] Tilkobling lukket (under melding): {e_ws_msg}.")
                                    break
                                except Exception as e_inner:
                                    print(f"[{agent_name}] FEIL i meldingsløkke: {type(e_inner).__name__} - {e_inner}")
                                    break
            
                        # Håndter disconnect utenom GAME_OVER (hvis data finnes og spillet var i gang)
                        if current_game_id is not None and not am_i_eliminated_this_game and a2c_agent.rewards_buffer:
                            print(f"[{agent_name}] Lærer fra ufullstendig spill {current_game_id} pga. disconnect.")
                            p_l, v_l, e_l = a2c_agent.learn(last_known_state_for_learn, True)
                            if p_l is not None:
                                temp_policy_losses.append(p_l)
                                temp_value_losses.append(v_l)
                                temp_entropy_losses.append(e_l)
                            # Logg dette ufullstendige spillet også
                            TRAINING_STATS[agent_id_str]['game_rewards_raw'].append(sum(current_game_step_rewards))
                            if temp_policy_losses: TRAINING_STATS[agent_id_str]['policy_loss_per_game'].append(
                                np.mean(temp_policy_losses))
                            if temp_value_losses: TRAINING_STATS[agent_id_str]['value_loss_per_game'].append(
                                np.mean(temp_value_losses))
                            if temp_entropy_losses: TRAINING_STATS[agent_id_str]['entropy_loss_per_game'].append(
                                np.mean(temp_entropy_losses))
                            TRAINING_STATS[agent_id_str]["games_played_count"] += 1
                            TRAINING_STATS[agent_id_str]["total_steps_across_games"] += steps_this_game
            
                        if SHUTDOWN_FLAG.is_set():
                            print(f"[{agent_name}] Avslutter økt pga shutdown flagg.")
                            break
            
                    except (websockets.exceptions.WebSocketException, ConnectionRefusedError, asyncio.TimeoutError) as e_conn:
                        connection_attempts += 1
                        wait_time = min(2 ** connection_attempts, 60)
                        print(f"[{agent_name}] Tilkoblingsfeil ({type(e_conn).__name__}). Prøver igjen om {wait_time}s.")
                        if connection_attempts >= max_connection_attempts and not SHUTDOWN_FLAG.is_set():
                            print(f"[{agent_name}] Maks antall tilkoblingsforsøk nådd. Avslutter denne agent-økten.")
                            SHUTDOWN_FLAG.set()
                            break
                        try:
                            await asyncio.sleep(wait_time)
                        except asyncio.CancelledError:
                            if SHUTDOWN_FLAG.is_set(): break
                    except Exception as e_outer:
                        print(
                            f"[{agent_name}] Alvorlig FEIL (ytre løkke): {type(e_outer).__name__} - {e_outer}. Avslutter agent-økt.")
                        # import traceback; traceback.print_exc()
                        SHUTDOWN_FLAG.set()  # Signaliser at noe gikk galt
                        break
            
                print(f"[{agent_name}] Økt ferdig. Totalt {TRAINING_STATS[agent_id_str]['games_played_count']} spill behandlet.")
                a2c_agent.save_model(checkpoint_path)
            
            
            def plot_aggregated_training_results():
                try:
                    num_agents_with_data = sum(1 for agent_id in TRAINING_STATS if TRAINING_STATS[agent_id]["game_rewards_raw"])
                    if num_agents_with_data == 0:
                        print("Ingen data å plotte for noen agenter.")
                        return
            
                    # Dynamisk bestem antall rader for subplots
                    num_plot_rows = 0
                    for agent_id_str in TRAINING_STATS:
                        if TRAINING_STATS[agent_id_str]["game_rewards_raw"]:  # Bare tell agenter med data
                            num_plot_rows += 1
                    if num_plot_rows == 0: return  # Ingen data å plotte
            
                    fig, axs = plt.subplots(num_plot_rows, 3, figsize=(20, 6 * num_plot_rows), squeeze=False)
                    current_plot_row = 0
            
                    for agent_id_str, stats in TRAINING_STATS.items():
                        if not stats["game_rewards_raw"]:
                            continue
            
                        # Rewards
                        rewards_raw = stats["game_rewards_raw"]
                        policy_losses_pg = stats.get('policy_loss_per_game', [])
                        value_losses_pg = stats.get('value_loss_per_game', [])
            
                        axs[current_plot_row, 0].cla()
                        axs[current_plot_row, 0].plot(rewards_raw, label='Belønning per spill', alpha=0.7, linestyle='-',
                                                      marker='.', markersize=3)
                        if len(rewards_raw) >= 10:
                            window = min(max(10, len(rewards_raw) // 10), 100)
                            avg_rewards = np.convolve(rewards_raw, np.ones(window) / window, mode='valid')
                            axs[current_plot_row, 0].plot(np.arange(window - 1, len(rewards_raw)), avg_rewards,
                                                          label=f'Gj.snitt ({window} spill)', color='red', lw=2)
                        axs[current_plot_row, 0].set_title(f"Agent {agent_id_str} - Belønning")
                        axs[current_plot_row, 0].set_xlabel("Spill #");
                        axs[current_plot_row, 0].set_ylabel("Total Belønning")
                        axs[current_plot_row, 0].legend();
                        axs[current_plot_row, 0].grid(True)
            
                        # Policy Loss
                        axs[current_plot_row, 1].cla()
                        if policy_losses_pg: axs[current_plot_row, 1].plot(policy_losses_pg, label='Policy Loss', marker='.',
                                                                           markersize=3)
                        axs[current_plot_row, 1].set_title(f"Agent {agent_id_str} - Policy Tap (per spill)")
                        axs[current_plot_row, 1].set_xlabel("Spill #");
                        axs[current_plot_row, 1].set_ylabel("Gj.snitt Policy Tap")
                        axs[current_plot_row, 1].legend();
                        axs[current_plot_row, 1].grid(True)
            
                        # Value Loss
                        axs[current_plot_row, 2].cla()
                        if value_losses_pg: axs[current_plot_row, 2].plot(value_losses_pg, label='Value Loss', color='green',
                                                                          marker='.', markersize=3)
                        axs[current_plot_row, 2].set_title(f"Agent {agent_id_str} - Value Tap (per spill)")
                        axs[current_plot_row, 2].set_xlabel("Spill #");
                        axs[current_plot_row, 2].set_ylabel("Gj.snitt Value Tap")
                        axs[current_plot_row, 2].legend();
                        axs[current_plot_row, 2].grid(True)
            
                        current_plot_row += 1
            
                    plt.tight_layout(pad=2.5)  # Juster padding
                    plot_filename = PLOT_DIR / f"aggregated_training_plots_a2c_game_avg.png"
                    plt.savefig(plot_filename)
                    print(f"Lagret/oppdatert aggregert treningsplot: {plot_filename}")
                    plt.close(fig)
            
                except ImportError:
                    print("Matplotlib ikke installert. Kan ikke plotte resultater.")
                except Exception as plot_e:
                    print(f"Kunne ikke plotte resultater: {type(plot_e).__name__} - {plot_e}")
                    # import traceback; traceback.print_exc()
            
            
            async def main_coordinator():
                parser = argparse.ArgumentParser(description="Kjør A2C agenter for W.O.R.M.S.")
                parser.add_argument("--num_agents", type=int, default=1, choices=[1, 2],
                                    help="Antall agenter å kjøre (1 eller 2). Server må støtte dette.")
                args = parser.parse_args()
            
                signal.signal(signal.SIGINT, signal_handler_main)
                signal.signal(signal.SIGTERM, signal_handler_main)
            
                tasks = []
                if args.num_agents >= 1:
                    print("(Koordinator) Forbereder Agent 1...")
                    tasks.append(asyncio.create_task(run_single_agent_session(
                        agent_id_str="1",
                        checkpoint_filename="a2c_worms_agent1_checkpoint.pth"
                    )))
                if args.num_agents == 2:
                    print("(Koordinator) Forbereder Agent 2...")
                    # Ingen sleep her, la dem starte så samtidig som mulig
                    tasks.append(asyncio.create_task(run_single_agent_session(
                        agent_id_str="2",
                        checkpoint_filename="a2c_worms_agent2_checkpoint.pth"
                    )))
            
                if not tasks:
                    print("Ingen agenter spesifisert.")
                    return
            
                print(f"(Koordinator) Starter {len(tasks)} agent-økt(er)...")
                try:
                    await asyncio.gather(*tasks)
                except asyncio.CancelledError:
                    print("(Koordinator) Hovedoppgave (gather) kansellert.")
            
                print("\n(Koordinator) Alle agent-økter er avsluttet eller avbrutt.")
                print("(Koordinator) Genererer endelige plott...")
                plot_aggregated_training_results()
                print("(Koordinator) Programmet avsluttes.")
            
            
            if __name__ == "__main__":
                try:
                    asyncio.run(main_coordinator())
                except KeyboardInterrupt:
                    print("\n(Hovedprogram) KeyboardInterrupt fanget helt på slutten. Avslutter.")
                    SHUTDOWN_FLAG.set()
            --- SLUTT INNHOLD (main_a2c.py) ---

        |-- model.py
            --- START INNHOLD (model.py) ---
            # agents/a2c_manual/model.py
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            from . import config
            
            
            class ActorCriticNetwork(nn.Module):
                def __init__(self):
                    super(ActorCriticNetwork, self).__init__()
            
                    # --- CNN for Map Processing ---
                    self.conv1 = nn.Conv2d(config.CNN_INPUT_CHANNELS, 16, kernel_size=8, stride=4)
                    self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)
                    self.pool = nn.AdaptiveMaxPool2d((6, 6))
                    # CNN_FEATURE_DIM forblir 32*6*6 = 1152
            
                    # --- Felles Fullt Tilkoblede Lag ---
                    self.fc_shared1 = nn.Linear(config.CNN_FEATURE_DIM + config.WORM_VECTOR_DIM, 256)
                    self.fc_shared2 = nn.Linear(256, 128)
            
                    # --- Actor Hoder ---
                    # 1. Hode for diskret action type
                    self.action_type_head = nn.Linear(128, config.ACTION_DIM)
            
                    # 2. Parameterhoder
                    # Walk 'dx' (logits for diskrete bins)
                    self.walk_dx_head = nn.Linear(128, config.WALK_DX_BINS)
            
                    # Kick: Ingen parameterhoder, da 'force' ikke lenger sendes fra klienten.
            
                    # Bazooka 'angle_deg' (mean og log_std for Normalfordeling)
                    self.bazooka_angle_mean_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
                    self.bazooka_angle_log_std_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
            
                    # Grenade 'dx' (logits for diskrete bins)
                    self.grenade_dx_head = nn.Linear(128, config.GRENADE_DX_BINS)
            
                    # --- Critic Hode ---
                    self.value_head = nn.Linear(128, 1)
            
                def forward(self, map_tensor, worm_vector_tensor):
                    # CNN
                    x_map = F.relu(self.conv1(map_tensor))
                    x_map = F.relu(self.conv2(x_map))
                    x_map = self.pool(x_map)
                    x_map = x_map.view(x_map.size(0), -1)  # Flatten
            
                    # Kombiner med worm data
                    if worm_vector_tensor.dim() == 1:
                        worm_vector_tensor = worm_vector_tensor.unsqueeze(0)
            
                    try:
                        x_combined = torch.cat((x_map, worm_vector_tensor), dim=1)
                    except RuntimeError as e:
                        print(f"FEIL ved torch.cat: map_shape={x_map.shape}, worm_shape={worm_vector_tensor.shape}")
                        print(f"Forventet map flat dim: {config.CNN_FEATURE_DIM}, worm_vector_dim: {config.WORM_VECTOR_DIM}")
                        raise e
            
                    # Felles lag
                    x_shared = F.relu(self.fc_shared1(x_combined))
                    x_shared = F.relu(self.fc_shared2(x_shared))
            
                    # --- Actor Outputs ---
                    action_type_logits = self.action_type_head(x_shared)
                    action_type_probs = F.softmax(action_type_logits, dim=-1)
            
                    # Walk dx (logits for bins)
                    walk_dx_logits = self.walk_dx_head(x_shared)
                    walk_dx_probs = F.softmax(walk_dx_logits, dim=-1)
            
                    # Bazooka angle (mean, std for Normal dist)
                    bazooka_angle_mean = self.bazooka_angle_mean_head(x_shared)
                    bazooka_angle_log_std = self.bazooka_angle_log_std_head(x_shared)
                    bazooka_angle_std = torch.exp(bazooka_angle_log_std.clamp(-20, 2)) # Klem for stabilitet
            
                    # Grenade dx (logits for bins)
                    grenade_dx_logits = self.grenade_dx_head(x_shared)
                    grenade_dx_probs = F.softmax(grenade_dx_logits, dim=-1)
            
                    actor_outputs = {
                        'action_type_probs': action_type_probs,
                        'walk_dx_probs': walk_dx_probs,
                        # Kick har ingen parametere som predikeres av nettverket
                        'bazooka_params': (bazooka_angle_mean, bazooka_angle_std),
                        'grenade_dx_probs': grenade_dx_probs,
                    }
            
                    # --- Critic Output ---
                    state_value = self.value_head(x_shared)
            
                    return actor_outputs, state_value
            --- SLUTT INNHOLD (model.py) ---

        |-- utils.py
            --- START INNHOLD (utils.py) ---
            # agents/a2c_manual/utils.py
            import torch
            import numpy as np
            from . import config
            
            
            def preprocess_state(current_game_state_json, agent_player_id):
                """
                Konverterer game_state JSON (fra msg['state']) til tensorer klar for nettverket.
                Returnerer en tuple: (map_tensor, worm_vector_tensor)
                """
                try:
                    map_data = current_game_state_json['map']
                    worms_data = current_game_state_json['worms']
            
                    # --- Kart Preprocessing ---
                    map_array = np.array(map_data, dtype=np.float32)
                    actual_map_h, actual_map_w = map_array.shape
            
                    # Senter-justert padding/cropping til config.MAP_HEIGHT, config.MAP_WIDTH
                    processed_map_array = np.zeros((config.MAP_HEIGHT, config.MAP_WIDTH), dtype=np.float32)
            
                    copy_h_len = min(actual_map_h, config.MAP_HEIGHT)
                    src_start_h = max(0, (actual_map_h - copy_h_len) // 2)
                    dst_start_h = max(0, (config.MAP_HEIGHT - copy_h_len) // 2)
            
                    copy_w_len = min(actual_map_w, config.MAP_WIDTH)
                    src_start_w = max(0, (actual_map_w - copy_w_len) // 2)
                    dst_start_w = max(0, (config.MAP_WIDTH - copy_w_len) // 2)
            
                    processed_map_array[dst_start_h: dst_start_h + copy_h_len,
                    dst_start_w: dst_start_w + copy_w_len] = \
                        map_array[src_start_h: src_start_h + copy_h_len,
                        src_start_w: src_start_w + copy_w_len]
            
                    map_tensor = torch.from_numpy(processed_map_array).unsqueeze(0).unsqueeze(0).to(config.DEVICE)
            
                    # --- Worm Data Preprocessing ---
                    active_worm_features = [0.0] * config.ACTIVE_WORM_FEATURE_DIM
                    my_worm_json_id = agent_player_id - 1  # Server player_id er 1-basert, JSON orm id er 0-basert
            
                    my_worm_alive = False
                    for worm_info in worms_data:
                        if worm_info['id'] == my_worm_json_id:
                            if worm_info['health'] > 0:
                                norm_health = worm_info['health'] / config.MAX_WORM_HEALTH
                                # Normaliser x og y basert på *faktiske* kartdimensjoner for korrekthet
                                norm_x = np.clip(worm_info['x'] / float(actual_map_w - 1 if actual_map_w > 1 else 1.0), 0.0, 1.0)
                                norm_y = np.clip(worm_info['y'] / float(actual_map_h - 1 if actual_map_h > 1 else 1.0), 0.0, 1.0)
            
                                active_worm_features = [np.clip(norm_health, 0, 1), norm_x, norm_y]
                                my_worm_alive = True
                            break  # Funnet vår orm (død eller levende), ikke nødvendig å lete mer
            
                    # if not my_worm_alive:
                    #     print(f"Advarsel preprocess_state: Fant ikke levende orm data for P{agent_player_id} (JSON id {my_worm_json_id}). Bruker null-vektor.")
            
                    worm_vector_tensor = torch.FloatTensor([active_worm_features]).to(config.DEVICE)
                    return map_tensor, worm_vector_tensor
            
                except KeyError as e:
                    print(
                        f"FEIL preprocess_state: Manglende nøkkel '{e}' i game_state_json: {str(current_game_state_json)[:200]}...")
                    # Returner dummy-tensorer for å unngå krasj, men dette indikerer et problem.
                    dummy_map = torch.zeros(1, config.CNN_INPUT_CHANNELS, config.MAP_HEIGHT, config.MAP_WIDTH).to(config.DEVICE)
                    dummy_worm = torch.zeros(1, config.WORM_VECTOR_DIM).to(config.DEVICE)
                    return dummy_map, dummy_worm
                except Exception as e:
                    print(f"Uventet FEIL preprocess_state: {type(e).__name__} - {e} data: {str(current_game_state_json)[:200]}...")
                    dummy_map = torch.zeros(1, config.CNN_INPUT_CHANNELS, config.MAP_HEIGHT, config.MAP_WIDTH).to(config.DEVICE)
                    dummy_worm = torch.zeros(1, config.WORM_VECTOR_DIM).to(config.DEVICE)
                    return dummy_map, dummy_worm
            
            
            def _convert_bin_to_value(bin_idx, num_bins, min_val, max_val):
                """ Hjelpefunksjon for å konvertere en bin-indeks til en faktisk verdi. """
                if num_bins <= 1:  # Unngå divisjon med null hvis num_bins er 0 eller 1
                    return (min_val + max_val) / 2.0
                value = min_val + bin_idx * ((max_val - min_val) / float(num_bins - 1))
                return np.clip(value, min_val, max_val)
            
            
            def format_action(network_action_idx, params_from_network):
                """
                Formaterer valgt handling og parametere til JSON-formatet serveren forventer.
                """
                network_action_name = config.NETWORK_ACTION_ORDER[network_action_idx]
            
                if network_action_name not in config.SERVER_ACTION_MAPPING:
                    print(f"FEIL format_action: Ukjent nettverkshandling '{network_action_name}'. Sender 'stand'.")
                    return {"action": "stand"}
            
                action_json = config.SERVER_ACTION_MAPPING[network_action_name].copy()
            
                if network_action_name == 'walk':
                    dx_bin_idx = params_from_network.get('walk_dx_bin_idx', config.WALK_DX_BINS // 2)
                    dx_value = _convert_bin_to_value(dx_bin_idx, config.WALK_DX_BINS, config.WALK_DX_MIN, config.WALK_DX_MAX)
                    action_json['dx'] = float(np.clip(dx_value, -2.0, 2.0))  # Server klipper til +/-2.0
            
                elif network_action_name == 'attack_kick':
                    # Ingen 'force' parameter fra klienten lenger
                    pass
            
                elif network_action_name == 'attack_bazooka':
                    angle_val = params_from_network.get('bazooka_angle_val', 0.0)
                    action_json['angle_deg'] = float(angle_val)
                    # Ingen 'force' for bazooka
            
                elif network_action_name == 'attack_grenade':
                    # Bruker nå 'dx' for grenade
                    dx_bin_idx = params_from_network.get('grenade_dx_bin_idx', config.GRENADE_DX_BINS // 2)
                    dx_value = _convert_bin_to_value(dx_bin_idx, config.GRENADE_DX_BINS, config.GRENADE_DX_MIN,
                                                     config.GRENADE_DX_MAX)
                    action_json['dx'] = float(np.clip(dx_value, -5.0, 5.0))  # Server klipper til +/-5.0
            
                return action_json
            --- SLUTT INNHOLD (utils.py) ---

        training_plots_output/
environment/
    |-- game_core.py
        --- START INNHOLD (game_core.py) ---
        # File: environment/game_core.py
        
        import copy
        import json
        import logging
        import math
        import random
        from pathlib import Path
        from typing import Any, Dict, List, Tuple
        
        logger = logging.getLogger(__name__)
        
        # tweakable constants
        WALK_MAX_SPEED = 2.0
        
        KICK_RANGE = 1.0
        KICK_DAMAGE = 80.0
        
        BAZOOKA_STEP_SIZE = 0.1
        BAZOOKA_MAX_RANGE = 10.0
        BAZOOKA_DAMAGE = 60.0
        BAZOOKA_HIT_RADIUS = 0.5
        
        GRENADE_MAX_THROW = 5.0
        GRENADE_STEP_SIZE = 0.1
        GRENADE_DAMAGE = 30.0
        GRENADE_HIT_RADIUS = 0.5
        
        class GameCore:
            def __init__(self, expected_players: int = 2) -> None:
                self.expected = expected_players
                self.map = self._load_random_map()
                self.state: Dict[str, Any] = self.initial_state()
        
            def _load_random_map(self) -> List[List[int]]:
                maps_dir = Path(__file__).resolve().parent / "maps"
                files = list(maps_dir.glob("*.json"))
                if not files:
                    raise FileNotFoundError(f"No map files found in {maps_dir!r}")
                choice = random.choice(files)
                with open(choice, "r") as f:
                    data = json.load(f)
                width = len(data[0])
                if any(len(row) != width for row in data):
                    raise ValueError(f"Map {choice.name} is not rectangular")
                return data  # type: ignore
        
            def expected_players(self) -> int:
                return self.expected
        
            def initial_state(self) -> Dict[str, Any]:
                floor = []
                rows = len(self.map)
                cols = len(self.map[0])
                for r in range(1, rows):
                    for c in range(cols):
                        if self.map[r][c] == 1 and self.map[r - 1][c] == 0:
                            floor.append((r, c))
        
                if len(floor) < self.expected:
                    raise ValueError(f"Not enough floor tiles ({len(floor)}) for {self.expected} players")
        
                chosen = random.sample(floor, self.expected)
                worms = []
                for pid, (r, c) in enumerate(chosen):
                    x = c + 0.5
                    y = float(r) - 0.25
                    worms.append({"id": pid, "health": 100, "x": x, "y": y})
        
                return {"worms": worms, "map": copy.deepcopy(self.map)}
        
            def step(
                self, player_id: int, action: Dict[str, Any]
            ) -> Tuple[Dict[str, Any], float, Dict[str, Any]]:
                state = self.state
                worms = state["worms"]
                idx = player_id - 1
                worm = worms[idx]
                effects: Dict[str, Any] = {}
        
                if action.get("action") == "walk":
                    dx = float(action.get("dx", 0.0))
                    dx = max(-WALK_MAX_SPEED, min(dx, WALK_MAX_SPEED))
                    new_x = worm["x"] + dx
                    worm["x"] = max(0.0, min(new_x, len(self.map[0]) - 0.01))
                    col = int(math.floor(worm["x"]))
                    height = len(self.map)
                    OFFSET = 0.25
                    row_here = int(math.floor(worm["y"]))
                    if 0 <= row_here < height and self.map[row_here][col] == 1:
                        worm["y"] = float(row_here) - OFFSET
                    else:
                        for row in range(row_here + 1, height):
                            if self.map[row][col] == 1:
                                worm["y"] = float(row) - OFFSET
                                break
                        else:
                            worm["y"] = float(height)
                            worm["health"] = 0
                    return copy.deepcopy(state), 0.0, effects
        
                if action.get("action") == "attack":
                    damage_total = 0.0
                    kill_bonus = 0.0
                    weapon = action.get("weapon")
                    effects = {"weapon": weapon, "trajectory": []}
        
                    # ─── Kick ──────────────────────────────────────────────────────────
                    if weapon == "kick":
                        for other in worms:
                            if other["id"] == worm["id"] or other["health"] <= 0:
                                continue
                            dist = math.hypot(other["x"] - worm["x"], other["y"] - worm["y"])
                            if dist <= KICK_RANGE:
                                old_hp = other["health"]
                                new_hp = max(0.0, old_hp - KICK_DAMAGE)
                                actual_damage = old_hp - new_hp
                                other["health"] = new_hp
                                damage_total += actual_damage
                                if new_hp == 0.0:
                                    kill_bonus += 100.0
                                effects["impact"] = {"x": other["x"], "y": other["y"]}
                                break
                        return copy.deepcopy(state), damage_total + kill_bonus, effects
        
                    # ─── Bazooka ───────────────────────────────────────────────────────
                    elif weapon == "bazooka":
                        angle = math.radians(float(action.get("angle_deg", 0.0)))
                        dx_unit, dy_unit = math.cos(angle), -math.sin(angle)
                        x0, y0 = worm["x"], worm["y"]
                        t = BAZOOKA_STEP_SIZE
                        effects["impact"] = {}
                        while t <= BAZOOKA_MAX_RANGE:
                            x_proj = x0 + dx_unit * t
                            y_proj = y0 + dy_unit * t
                            effects["trajectory"].append({"x": x_proj, "y": y_proj})
                            tx, ty = int(math.floor(x_proj)), int(math.floor(y_proj))
                            if (
                                tx < 0
                                or tx >= len(self.map[0])
                                or ty < 0
                                or ty >= len(self.map)
                                or self.map[ty][tx] == 1
                            ):
                                effects["impact"] = {"x": x_proj, "y": y_proj}
                                break
                            for other in worms:
                                if other["id"] == worm["id"] or other["health"] <= 0:
                                    continue
                                if math.hypot(other["x"] - x_proj, other["y"] - y_proj) <= BAZOOKA_HIT_RADIUS:
                                    old_hp = other["health"]
                                    new_hp = max(0.0, old_hp - BAZOOKA_DAMAGE)
                                    actual_damage = old_hp - new_hp
                                    other["health"] = new_hp
                                    damage_total += actual_damage
                                    if new_hp == 0.0:
                                        kill_bonus += 100.0
                                    effects["impact"] = {"x": x_proj, "y": y_proj}
                                    t = BAZOOKA_MAX_RANGE + BAZOOKA_STEP_SIZE
                                    break
                            t += BAZOOKA_STEP_SIZE
                        return copy.deepcopy(state), damage_total + kill_bonus, effects
        
                    # ─── Grenade ──────────────────────────────────────────────────────
                    elif weapon == "grenade":
                        dx_total = float(action.get("dx", 0.0))
                        sign = 1.0 if dx_total >= 0 else -1.0
                        width = max(-GRENADE_MAX_THROW, min(dx_total, GRENADE_MAX_THROW))
                        height = abs(width) / 2.0
                        x0, y0 = worm["x"], worm["y"]
                        t = GRENADE_STEP_SIZE
                        effects["impact"] = {}
        
                        while True:
                            x_proj = x0 + sign * t
                            u = (t / abs(width)) if width != 0 else 0.0
                            y_proj = y0 - 4 * height * u * (1 - u)
        
                            effects["trajectory"].append({"x": x_proj, "y": y_proj})
                            tx, ty = int(math.floor(x_proj)), int(math.floor(y_proj))
        
                            if (
                                tx < 0
                                or tx >= len(self.map[0])
                                or ty < 0
                                or ty >= len(self.map)
                                or self.map[ty][tx] == 1
                            ):
                                effects["impact"] = {"x": x_proj, "y": y_proj}
                                break
        
                            hit = False
                            for other in worms:
                                if other["id"] == worm["id"] or other["health"] <= 0:
                                    continue
                                if math.hypot(other["x"] - x_proj, other["y"] - y_proj) <= GRENADE_HIT_RADIUS:
                                    old_hp = other["health"]
                                    new_hp = max(0.0, old_hp - GRENADE_DAMAGE)
                                    actual_damage = old_hp - new_hp
                                    other["health"] = new_hp
                                    damage_total += actual_damage
                                    if new_hp == 0.0:
                                        kill_bonus += 100.0
                                    effects["impact"] = {"x": x_proj, "y": y_proj}
                                    hit = True
                                    break
                            if hit:
                                break
        
                            t += GRENADE_STEP_SIZE
        
                        return copy.deepcopy(state), damage_total + kill_bonus, effects
        
                    else:
                        logger.warning("Unknown weapon: %s", weapon)
                        return copy.deepcopy(state), 0.0, {}
        
                # Default: no reward
                return copy.deepcopy(state), 0.0, effects
        
            def get_state_with_nicks(self, clients: Dict[Any, Dict[str, Any]]) -> Dict[str, Any]:
                state_copy = copy.deepcopy(self.state)
                for ws, info in clients.items():
                    pid = info["id"] - 1
                    if 0 <= pid < len(state_copy["worms"]):
                        state_copy["worms"][pid]["nick"] = info.get("nick", f"Player {pid + 1}")
                return state_copy
        --- SLUTT INNHOLD (game_core.py) ---

    |-- server.py
        --- START INNHOLD (server.py) ---
        #!/usr/bin/env python3
        """
        Continuous W.O.R.M.S. match server.
        Keeps the same WebSocket connections alive across many games so that RL clients can train without reconnecting.
        """
        from __future__ import annotations
        
        import argparse
        import asyncio
        import json
        import logging
        import random          # NEW
        import sys
        from enum import IntEnum
        from pathlib import Path
        from typing import Any, Dict
        
        import websockets
        from websockets.exceptions import ConnectionClosed
        
        TIME_LIMIT_MS = 15000
        
        # ensure game_core is importable
        sys.path.append(str(Path(__file__).resolve().parent))
        from game_core import GameCore
        
        HOST, PORT = "127.0.0.1", 8765
        
        
        class WSState(IntEnum):
            CONNECTING = 0
            OPEN = 1
            CLOSING = 2
            CLOSED = 3
        
        
        class WormsServer:
            def __init__(self, expected_players: int) -> None:
                self.expected = expected_players
                self.core = GameCore(expected_players=self.expected)
                self.clients: dict[Any, dict[str, Any]] = {}
                self.turn_order: list[Any] = []
                self.idx = 0
                self.turn_counter = 0
                self.game_id = 0
        
            async def accept(self, ws: Any) -> None:
                try:
                    raw = await asyncio.wait_for(ws.recv(), timeout=10)
                    msg = json.loads(raw)
                except (asyncio.TimeoutError, json.JSONDecodeError):
                    await ws.close(code=4000, reason="Expected CONNECT")
                    return
        
                if msg.get("type") != "CONNECT":
                    await ws.close(code=4002, reason="First message must be CONNECT")
                    return
        
                pid = len(self.clients) + 1
                nick = msg.get("nick", f"Player {pid}")
                self.clients[ws] = {"id": pid, "nick": nick}
                self.turn_order.append(ws)
                await ws.send(json.dumps({"type": "ASSIGN_ID", "player_id": pid}))
        
                try:
                    await ws.wait_closed()
                finally:
                    self._remove(ws)
        
            def _remove(self, ws: Any) -> None:
                if ws in self.turn_order:
                    idx = self.turn_order.index(ws)
                    self.turn_order.remove(ws)
                    if idx <= self.idx and self.idx > 0:
                        self.idx -= 1
                if ws in self.clients:
                    pid = self.clients.pop(ws)["id"]
                    logger.info("player %d disconnected", pid)
        
            async def _safe_send(self, ws: Any, msg: Dict[str, Any]) -> bool:
                try:
                    await ws.send(json.dumps(msg))
                    return True
                except ConnectionClosed:
                    self._remove(ws)
                    return False
        
            async def _broadcast(self, msg: Dict[str, Any]) -> None:
                for ws in list(self.clients):
                    await self._safe_send(ws, msg)
        
            async def _play_single_game(self) -> None:
                # ── NEW: fresh round‑robin list for the new game ────────────────────
                self.turn_order = [ws for ws in self.clients if ws.state == WSState.OPEN]
                random.shuffle(self.turn_order)  # vary who starts
                # ────────────────────────────────────────────────────────────────────
        
                self.core = GameCore(expected_players=self.expected)
                self.turn_counter = 0
                self.idx = 0
                self.game_id += 1
        
                initial = self.core.get_state_with_nicks(self.clients)
                await self._broadcast(
                    {
                        "type": "NEW_GAME",
                        "game_id": self.game_id,
                        "state": initial,
                    }
                )
        
                while True:
                    alive = [w for w in self.core.state["worms"] if w["health"] > 0]
                    if len(alive) <= 1:
                        winner = alive[0]["id"] + 1 if alive else None
                        await self._broadcast(
                            {
                                "type": "GAME_OVER",
                                "game_id": self.game_id,
                                "winner_id": winner,
                                "final_state": self.core.state,
                            }
                        )
                        return
        
                    if not self.turn_order:
                        return
        
                    if self.idx >= len(self.turn_order):
                        self.idx = 0
        
                    ws = self.turn_order[self.idx]
                    if ws.state != WSState.OPEN:
                        self._remove(ws)
                        continue
        
                    pid = self.clients[ws]["id"]
                    worm = self.core.state["worms"][pid - 1]
        
                    if worm["health"] <= 0:
                        await self._broadcast({"type": "PLAYER_ELIMINATED", "player_id": pid})
                        self.turn_order.pop(self.idx)
                        continue
        
                    begin = {
                        "type": "TURN_BEGIN",
                        "turn_index": self.turn_counter,
                        "player_id": pid,
                        "state": self.core.get_state_with_nicks(self.clients),
                        "time_limit_ms": TIME_LIMIT_MS,
                    }
                    if not await self._safe_send(ws, begin):
                        continue
        
                    try:
                        raw = await asyncio.wait_for(ws.recv(), timeout=15)
                        msg = json.loads(raw)
                        if msg.get("type") != "ACTION" or msg.get("player_id") != pid:
                            raise ValueError
                    except (asyncio.TimeoutError, ValueError):
                        self.idx += 1
                        self.turn_counter += 1
                        continue
                    except ConnectionClosed:
                        self._remove(ws)
                        continue
        
                    action = msg.get("action", {})
                    new_state, reward, effects = self.core.step(pid, action)
                    await self._broadcast(
                        {
                            "type": "TURN_RESULT",
                            "turn_index": self.turn_counter,
                            "player_id": pid,
                            "state": new_state,
                            "reward": reward,
                            "effects": effects,
                        }
                    )
        
                    next_idx = (self.idx + 1) % len(self.turn_order)
                    next_pid = self.clients[self.turn_order[next_idx]]["id"]
                    await self._broadcast({"type": "TURN_END", "next_player_id": next_pid})
        
                    self.idx += 1
                    self.turn_counter += 1
        
            async def orchestrator(self) -> None:
                while True:
                    while len(self.clients) < self.expected:
                        await asyncio.sleep(0.1)
                    await self._play_single_game()
        
        
        async def main() -> None:
            parser = argparse.ArgumentParser(description="W.O.R.M.S. continuous server")
            parser.add_argument(
                "--log-level",
                choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                default="INFO",
                help="set logging level",
            )
            parser.add_argument(
                "--max-players", type=int, default=2, help="number of worms per game"
            )
            args = parser.parse_args()
        
            logging.basicConfig(
                level=getattr(logging, args.log_level),
                format="%(asctime)s %(levelname)-8s %(name)s: %(message)s",
                datefmt="%H:%M:%S",
            )
            global logger
            logger = logging.getLogger("server")
        
            server = WormsServer(expected_players=args.max_players)
            async with websockets.serve(server.accept, HOST, PORT):
                asyncio.create_task(server.orchestrator())
                await asyncio.Future()
        
        
        if __name__ == "__main__":
            asyncio.run(main())
        --- SLUTT INNHOLD (server.py) ---

    maps/
        |-- 001.json
            --- START INNHOLD (001.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0],
              [0,1,0,0,0,0,0,0,1,1,1,0],
              [0,1,1,1,1,0,0,0,1,1,1,0],
              [0,1,1,1,1,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (001.json) ---

        |-- 002.json
            --- START INNHOLD (002.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0],
              [0,0,0,1,1,1,1,0,0,0],
              [0,0,0,1,1,1,1,1,0,0],
              [0,1,1,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (002.json) ---

        |-- 003.json
            --- START INNHOLD (003.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,1,1,0,0,0,0,0,0,0,0,0,0],
              [0,1,1,1,1,0,0,0,0,1,1,1,0],
              [0,1,1,1,1,0,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (003.json) ---

        |-- 004.json
            --- START INNHOLD (004.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,1,1,0,0,0,0,0,0,0,0,1,1,0],
              [0,1,1,0,0,0,1,1,0,0,1,1,1,0],
              [0,1,1,1,0,0,1,1,1,0,1,1,1,0]
            ]
            --- SLUTT INNHOLD (004.json) ---

        |-- 005.json
            --- START INNHOLD (005.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,1,1,1,1,0,0,0,0,0,0],
              [0,1,1,1,1,1,1,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (005.json) ---

frontend/
    |-- package-lock.json
        --- START INNHOLD (package-lock.json) ---
        {
          "name": "relearning",
          "version": "0.0.1",
          "lockfileVersion": 3,
          "requires": true,
          "packages": {
            "": {
              "name": "relearning",
              "version": "0.0.1",
              "dependencies": {
                "esbuild": "^0.25.4",
                "pixi.js": "^8.9.2"
              }
            },
            "node_modules/@esbuild/aix-ppc64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.4.tgz",
              "integrity": "sha512-1VCICWypeQKhVbE9oW/sJaAmjLxhVqacdkvPLEjwlttjfwENRSClS8EjBz0KzRyFSCPDIkuXW34Je/vk7zdB7Q==",
              "cpu": [
                "ppc64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "aix"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-arm": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.4.tgz",
              "integrity": "sha512-QNdQEps7DfFwE3hXiU4BZeOV68HHzYwGd0Nthhd3uCkkEKK7/R6MTgM0P7H7FAs5pU/DIWsviMmEGxEoxIZ+ZQ==",
              "cpu": [
                "arm"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.4.tgz",
              "integrity": "sha512-bBy69pgfhMGtCnwpC/x5QhfxAz/cBgQ9enbtwjf6V9lnPI/hMyT9iWpR1arm0l3kttTr4L0KSLpKmLp/ilKS9A==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.4.tgz",
              "integrity": "sha512-TVhdVtQIFuVpIIR282btcGC2oGQoSfZfmBdTip2anCaVYcqWlZXGcdcKIUklfX2wj0JklNYgz39OBqh2cqXvcQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/darwin-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.4.tgz",
              "integrity": "sha512-Y1giCfM4nlHDWEfSckMzeWNdQS31BQGs9/rouw6Ub91tkK79aIMTH3q9xHvzH8d0wDru5Ci0kWB8b3up/nl16g==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "darwin"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/darwin-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.4.tgz",
              "integrity": "sha512-CJsry8ZGM5VFVeyUYB3cdKpd/H69PYez4eJh1W/t38vzutdjEjtP7hB6eLKBoOdxcAlCtEYHzQ/PJ/oU9I4u0A==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "darwin"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/freebsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.4.tgz",
              "integrity": "sha512-yYq+39NlTRzU2XmoPW4l5Ifpl9fqSk0nAJYM/V/WUGPEFfek1epLHJIkTQM6bBs1swApjO5nWgvr843g6TjxuQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "freebsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/freebsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.4.tgz",
              "integrity": "sha512-0FgvOJ6UUMflsHSPLzdfDnnBBVoCDtBTVyn/MrWloUNvq/5SFmh13l3dvgRPkDihRxb77Y17MbqbCAa2strMQQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "freebsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-arm": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.4.tgz",
              "integrity": "sha512-kro4c0P85GMfFYqW4TWOpvmF8rFShbWGnrLqlzp4X1TNWjRY3JMYUfDCtOxPKOIY8B0WC8HN51hGP4I4hz4AaQ==",
              "cpu": [
                "arm"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.4.tgz",
              "integrity": "sha512-+89UsQTfXdmjIvZS6nUnOOLoXnkUTB9hR5QAeLrQdzOSWZvNSAXAtcRDHWtqAUtAmv7ZM1WPOOeSxDzzzMogiQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-ia32": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.4.tgz",
              "integrity": "sha512-yTEjoapy8UP3rv8dB0ip3AfMpRbyhSN3+hY8mo/i4QXFeDxmiYbEKp3ZRjBKcOP862Ua4b1PDfwlvbuwY7hIGQ==",
              "cpu": [
                "ia32"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-loong64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.4.tgz",
              "integrity": "sha512-NeqqYkrcGzFwi6CGRGNMOjWGGSYOpqwCjS9fvaUlX5s3zwOtn1qwg1s2iE2svBe4Q/YOG1q6875lcAoQK/F4VA==",
              "cpu": [
                "loong64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-mips64el": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.4.tgz",
              "integrity": "sha512-IcvTlF9dtLrfL/M8WgNI/qJYBENP3ekgsHbYUIzEzq5XJzzVEV/fXY9WFPfEEXmu3ck2qJP8LG/p3Q8f7Zc2Xg==",
              "cpu": [
                "mips64el"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-ppc64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.4.tgz",
              "integrity": "sha512-HOy0aLTJTVtoTeGZh4HSXaO6M95qu4k5lJcH4gxv56iaycfz1S8GO/5Jh6X4Y1YiI0h7cRyLi+HixMR+88swag==",
              "cpu": [
                "ppc64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-riscv64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.4.tgz",
              "integrity": "sha512-i8JUDAufpz9jOzo4yIShCTcXzS07vEgWzyX3NH2G7LEFVgrLEhjwL3ajFE4fZI3I4ZgiM7JH3GQ7ReObROvSUA==",
              "cpu": [
                "riscv64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-s390x": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.4.tgz",
              "integrity": "sha512-jFnu+6UbLlzIjPQpWCNh5QtrcNfMLjgIavnwPQAfoGx4q17ocOU9MsQ2QVvFxwQoWpZT8DvTLooTvmOQXkO51g==",
              "cpu": [
                "s390x"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.4.tgz",
              "integrity": "sha512-6e0cvXwzOnVWJHq+mskP8DNSrKBr1bULBvnFLpc1KY+d+irZSgZ02TGse5FsafKS5jg2e4pbvK6TPXaF/A6+CA==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/netbsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.4.tgz",
              "integrity": "sha512-vUnkBYxZW4hL/ie91hSqaSNjulOnYXE1VSLusnvHg2u3jewJBz3YzB9+oCw8DABeVqZGg94t9tyZFoHma8gWZQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "netbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/netbsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.4.tgz",
              "integrity": "sha512-XAg8pIQn5CzhOB8odIcAm42QsOfa98SBeKUdo4xa8OvX8LbMZqEtgeWE9P/Wxt7MlG2QqvjGths+nq48TrUiKw==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "netbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/openbsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.4.tgz",
              "integrity": "sha512-Ct2WcFEANlFDtp1nVAXSNBPDxyU+j7+tId//iHXU2f/lN5AmO4zLyhDcpR5Cz1r08mVxzt3Jpyt4PmXQ1O6+7A==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "openbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/openbsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.4.tgz",
              "integrity": "sha512-xAGGhyOQ9Otm1Xu8NT1ifGLnA6M3sJxZ6ixylb+vIUVzvvd6GOALpwQrYrtlPouMqd/vSbgehz6HaVk4+7Afhw==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "openbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/sunos-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.4.tgz",
              "integrity": "sha512-Mw+tzy4pp6wZEK0+Lwr76pWLjrtjmJyUB23tHKqEDP74R3q95luY/bXqXZeYl4NYlvwOqoRKlInQialgCKy67Q==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "sunos"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.4.tgz",
              "integrity": "sha512-AVUP428VQTSddguz9dO9ngb+E5aScyg7nOeJDrF1HPYu555gmza3bDGMPhmVXL8svDSoqPCsCPjb265yG/kLKQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-ia32": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.4.tgz",
              "integrity": "sha512-i1sW+1i+oWvQzSgfRcxxG2k4I9n3O9NRqy8U+uugaT2Dy7kLO9Y7wI72haOahxceMX8hZAzgGou1FhndRldxRg==",
              "cpu": [
                "ia32"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.4.tgz",
              "integrity": "sha512-nOT2vZNw6hJ+z43oP1SPea/G/6AbN6X+bGNhNuq8NtRHy4wsMhw765IKLNmnjek7GvjWBYQ8Q5VBoYTFg9y1UQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@pixi/colord": {
              "version": "2.9.6",
              "resolved": "https://registry.npmjs.org/@pixi/colord/-/colord-2.9.6.tgz",
              "integrity": "sha512-nezytU2pw587fQstUu1AsJZDVEynjskwOL+kibwcdxsMBFqPsFFNA7xl0ii/gXuDi6M0xj3mfRJj8pBSc2jCfA==",
              "license": "MIT"
            },
            "node_modules/@types/css-font-loading-module": {
              "version": "0.0.12",
              "resolved": "https://registry.npmjs.org/@types/css-font-loading-module/-/css-font-loading-module-0.0.12.tgz",
              "integrity": "sha512-x2tZZYkSxXqWvTDgveSynfjq/T2HyiZHXb00j/+gy19yp70PHCizM48XFdjBCWH7eHBD0R5i/pw9yMBP/BH5uA==",
              "license": "MIT"
            },
            "node_modules/@types/earcut": {
              "version": "2.1.4",
              "resolved": "https://registry.npmjs.org/@types/earcut/-/earcut-2.1.4.tgz",
              "integrity": "sha512-qp3m9PPz4gULB9MhjGID7wpo3gJ4bTGXm7ltNDsmOvsPduTeHp8wSW9YckBj3mljeOh4F0m2z/0JKAALRKbmLQ==",
              "license": "MIT"
            },
            "node_modules/@webgpu/types": {
              "version": "0.1.60",
              "resolved": "https://registry.npmjs.org/@webgpu/types/-/types-0.1.60.tgz",
              "integrity": "sha512-8B/tdfRFKdrnejqmvq95ogp8tf52oZ51p3f4QD5m5Paey/qlX4Rhhy5Y8tgFMi7Ms70HzcMMw3EQjH/jdhTwlA==",
              "license": "BSD-3-Clause"
            },
            "node_modules/@xmldom/xmldom": {
              "version": "0.8.10",
              "resolved": "https://registry.npmjs.org/@xmldom/xmldom/-/xmldom-0.8.10.tgz",
              "integrity": "sha512-2WALfTl4xo2SkGCYRt6rDTFfk9R1czmBvUQy12gK2KuRKIpWEhcbbzy8EZXtz/jkRqHX8bFEc6FC1HjX4TUWYw==",
              "license": "MIT",
              "engines": {
                "node": ">=10.0.0"
              }
            },
            "node_modules/earcut": {
              "version": "2.2.4",
              "resolved": "https://registry.npmjs.org/earcut/-/earcut-2.2.4.tgz",
              "integrity": "sha512-/pjZsA1b4RPHbeWZQn66SWS8nZZWLQQ23oE3Eam7aroEFGEvwKAsJfZ9ytiEMycfzXWpca4FA9QIOehf7PocBQ==",
              "license": "ISC"
            },
            "node_modules/esbuild": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.4.tgz",
              "integrity": "sha512-8pgjLUcUjcgDg+2Q4NYXnPbo/vncAY4UmyaCm0jZevERqCHZIaWwdJHkf8XQtu4AxSKCdvrUbT0XUr1IdZzI8Q==",
              "hasInstallScript": true,
              "license": "MIT",
              "bin": {
                "esbuild": "bin/esbuild"
              },
              "engines": {
                "node": ">=18"
              },
              "optionalDependencies": {
                "@esbuild/aix-ppc64": "0.25.4",
                "@esbuild/android-arm": "0.25.4",
                "@esbuild/android-arm64": "0.25.4",
                "@esbuild/android-x64": "0.25.4",
                "@esbuild/darwin-arm64": "0.25.4",
                "@esbuild/darwin-x64": "0.25.4",
                "@esbuild/freebsd-arm64": "0.25.4",
                "@esbuild/freebsd-x64": "0.25.4",
                "@esbuild/linux-arm": "0.25.4",
                "@esbuild/linux-arm64": "0.25.4",
                "@esbuild/linux-ia32": "0.25.4",
                "@esbuild/linux-loong64": "0.25.4",
                "@esbuild/linux-mips64el": "0.25.4",
                "@esbuild/linux-ppc64": "0.25.4",
                "@esbuild/linux-riscv64": "0.25.4",
                "@esbuild/linux-s390x": "0.25.4",
                "@esbuild/linux-x64": "0.25.4",
                "@esbuild/netbsd-arm64": "0.25.4",
                "@esbuild/netbsd-x64": "0.25.4",
                "@esbuild/openbsd-arm64": "0.25.4",
                "@esbuild/openbsd-x64": "0.25.4",
                "@esbuild/sunos-x64": "0.25.4",
                "@esbuild/win32-arm64": "0.25.4",
                "@esbuild/win32-ia32": "0.25.4",
                "@esbuild/win32-x64": "0.25.4"
              }
            },
            "node_modules/eventemitter3": {
              "version": "5.0.1",
              "resolved": "https://registry.npmjs.org/eventemitter3/-/eventemitter3-5.0.1.tgz",
              "integrity": "sha512-GWkBvjiSZK87ELrYOSESUYeVIc9mvLLf/nXalMOS5dYrgZq9o5OVkbZAVM06CVxYsCwH9BDZFPlQTlPA1j4ahA==",
              "license": "MIT"
            },
            "node_modules/gifuct-js": {
              "version": "2.1.2",
              "resolved": "https://registry.npmjs.org/gifuct-js/-/gifuct-js-2.1.2.tgz",
              "integrity": "sha512-rI2asw77u0mGgwhV3qA+OEgYqaDn5UNqgs+Bx0FGwSpuqfYn+Ir6RQY5ENNQ8SbIiG/m5gVa7CD5RriO4f4Lsg==",
              "license": "MIT",
              "dependencies": {
                "js-binary-schema-parser": "^2.0.3"
              }
            },
            "node_modules/ismobilejs": {
              "version": "1.1.1",
              "resolved": "https://registry.npmjs.org/ismobilejs/-/ismobilejs-1.1.1.tgz",
              "integrity": "sha512-VaFW53yt8QO61k2WJui0dHf4SlL8lxBofUuUmwBo0ljPk0Drz2TiuDW4jo3wDcv41qy/SxrJ+VAzJ/qYqsmzRw==",
              "license": "MIT"
            },
            "node_modules/js-binary-schema-parser": {
              "version": "2.0.3",
              "resolved": "https://registry.npmjs.org/js-binary-schema-parser/-/js-binary-schema-parser-2.0.3.tgz",
              "integrity": "sha512-xezGJmOb4lk/M1ZZLTR/jaBHQ4gG/lqQnJqdIv4721DMggsa1bDVlHXNeHYogaIEHD9vCRv0fcL4hMA+Coarkg==",
              "license": "MIT"
            },
            "node_modules/parse-svg-path": {
              "version": "0.1.2",
              "resolved": "https://registry.npmjs.org/parse-svg-path/-/parse-svg-path-0.1.2.tgz",
              "integrity": "sha512-JyPSBnkTJ0AI8GGJLfMXvKq42cj5c006fnLz6fXy6zfoVjJizi8BNTpu8on8ziI1cKy9d9DGNuY17Ce7wuejpQ==",
              "license": "MIT"
            },
            "node_modules/pixi.js": {
              "version": "8.9.2",
              "resolved": "https://registry.npmjs.org/pixi.js/-/pixi.js-8.9.2.tgz",
              "integrity": "sha512-oLFBkOOA/O6OpT5T8o05AxgZB9x9yWNzEQ+WTNZZFoCvfU2GdT4sFTjpVFuHQzgZPmAm/1IFhKdNiXVnlL8PRw==",
              "license": "MIT",
              "dependencies": {
                "@pixi/colord": "^2.9.6",
                "@types/css-font-loading-module": "^0.0.12",
                "@types/earcut": "^2.1.4",
                "@webgpu/types": "^0.1.40",
                "@xmldom/xmldom": "^0.8.10",
                "earcut": "^2.2.4",
                "eventemitter3": "^5.0.1",
                "gifuct-js": "^2.1.2",
                "ismobilejs": "^1.1.1",
                "parse-svg-path": "^0.1.2"
              }
            }
          }
        }
        --- SLUTT INNHOLD (package-lock.json) ---

    |-- package.json
        --- START INNHOLD (package.json) ---
        {
          "name": "relearning",
          "version": "0.0.1",
          "author": "Ask Sødal <asksodal@gmail.com>",
          "scripts": {
            "watch": "esbuild --bundle src/script.js --outfile=public/bundle.js --watch",
            "serve": "esbuild --bundle src/script.js --outfile=public/bundle.js --servedir=public"
          },
          "dependencies": {
            "pixi.js": "^8.9.2",
            "esbuild": "^0.25.4"
          }
        }
        --- SLUTT INNHOLD (package.json) ---

    public/
    src/
training_plots/