Prosjektstruktur og filinnhold (fra mappen: C:\Users\kriny\andre_fag\pattern_recognition\Directory\relearning)

Skriptet 'all.py' og outputfilen 'prosjekt_struktur_og_innhold.txt' er ekskludert.
Følgende mappenavn blir også fullstendig ignorert (i tillegg til andre skjulte mapper som starter med '.'): .git, __pycache__, .vscode, .idea, venv, env, .venv, node_modules

================================================================================

    |-- description.md
        --- START INNHOLD (description.md) ---
        *Turn-based Reinforcement-Learning playground*
        
        The goal is to train an AI to play free-for-all W.O.R.M.S. This means that each agent controls one worm, and the goal is to kill all other opponents.
        
        I want to be able to test the game against the AI agents. That is why I chose to modularize into components that communicate through WebSockets:
        - server / environment: manages communication with clients and simulates the environment  
        - CLI client / AI agent: chooses actions based on the given environment, and learns  
        - Web client: user can take the role as a player and manually choose actions against the AI; visualized via a simple browser interface
        
        ---
        
        ## Scoring
        
        Each action yields a numeric **reward** which agents use to learn:
        
        - **Damage points**  
          You earn points equal to the actual HP removed from any opponent.  
          - _Example_: you bazooka-hit a worm at 30 HP with a 60-damage shot → you earn **30 points**.
        
        - **Kill bonus**  
          If an action reduces an opponent’s HP to exactly 0, you get an extra **100 points**.  
          - _Example_: you kick-hit a worm at 1 HP (kick = 80 damage) → you earn **1 (damage) + 100 (kill bonus) = 101 points**.
        
        ---
        
        ## 1  High-level Picture
        
        ```
        
        ┌──────────┐      WebSocket/JSON       ┌─────────────┐
        │  Browser │◀─────────────────────────▶│  Server     │
        │ (Pixi UI)│                           │ (env + core)│
        └────┬─────┘                           └────┬────────┘
        │ WebSocket/JSON                        │
        │                                       │
        ▼                                       ▼
        ┌──────────┐      WebSocket/JSON       ┌─────────────┐
        │  RL Bot  │◀─────────────────────────▶│  GameCore   │
        │ (Python) │                           │  (pure logic)
        └──────────┘                           └─────────────┘
        
        ````
        
        * **Server / Environment** (`environment/`)  
          Runs a match, owns the authoritative **`GameCore`** logic, and speaks the JSON protocol over WebSockets.  
        
        * **Agents** (`agents/`)  
          CLI clients—human or reinforcement-learning models—that connect, read `TURN_BEGIN`, decide an `ACTION`, and learn from `TURN_RESULT`.  
        
        * **Web Client** (`frontend/`)  
          Pixi.js interface for humans: shows the map full-width, lets you pick actions via dropdown + dynamic parameter inputs, and renders worms in sub-tile positions.
        
        Everything communicates through **one common surface**: the JSON messages documented in `json-docs.md`.
        
        ---
        
        ## 2  Runtime Flow
        
        1. **CONNECT** Clients open `ws://127.0.0.1:8765` and send  
           ```json
           { "type": "CONNECT", "nick": "<name>" }
        ````
        
        The server replies `ASSIGN_ID`.
        
        2. **Turn loop**
        
           ```
           TURN_BEGIN → ACTION → TURN_RESULT → TURN_END
           ```
        
           repeats until `GAME_OVER`.
        
        3. Clients render or learn from the **Game State** snapshot inside each `TURN_BEGIN` / `TURN_RESULT`. The state contains:
        
           * `worms` with **world-unit** `(x,y)` floats
           * `map` grid with `1 = terrain`, `0 = empty`
        
        4. **Physics & rules** (fully in `GameCore.step()`).
        
        ---
        
        ## 3  Coordinate System
        
        * **Tile grid** Index `(0,0)` is top-left.
        * **World units** 1 world-unit ≙ 1 tile; worms can be at `3.2, 1.75`, etc.
        * **Screen mapping** (client side):
        
          ```js
          tile   = Math.min(canvasW / cols, canvasH / rows);
          offset = [(canvasW - tile*cols)/2, (canvasH - tile*rows)/2];
          pixelX = offset[0] + xWorld * tile;
          pixelY = offset[1] + yWorld * tile;
          ```
        
        ---
        
        ## 4  Components in Detail
        
        | Folder         | Key files                            | Role                                  |
        | -------------- | ------------------------------------ | ------------------------------------- |
        | `environment/` | `server.py`, `game_core.py`          | Match orchestration + pure game logic |
        | `agents/`      | `client.py` (baseline random bot)    | Example RL agent; train & act here    |
        | `frontend/`    | `src/script.js`, `public/index.html` | Pixi.js UI + dynamic action form      |
        | root           | `json-docs.md`                       | Authoritative protocol spec           |
        
        ### Build / Run Quick-start
        
        ```bash
        # Python side
        cd environment && python server.py
        
        # Web UI
        cd frontend
        npm install        # (once)
        npx esbuild src/script.js --bundle --outfile=public/bundle.js --format=esm
        npx serve public
        
        # Bot
        cd agents && python client.py
        ```
        --- SLUTT INNHOLD (description.md) ---

    |-- json-docs.md
        --- START INNHOLD (json-docs.md) ---
        # W.O.R.M.S. WebSocket JSON Protocol
        
        All messages are JSON objects sent over a WebSocket.  
        Each object **must** include a `"type"` field that selects one of the structures below.  
        Clients **must ignore** unknown keys so the protocol can evolve.
        
        ---
        
        ## Overall flow
        
        ```text
                    Client                                  Server
                     │                                         │
                     ├─▶  CONNECT                              │
                     │     (nick)                              │
                     │                                         │
                     │   ASSIGN_ID  ◀──────────────────────────┤
                     │                                         │
              repeat │                                         │
              until  │  TURN_BEGIN ───────────────────────────▶│
           GAME_OVER │  ACTION       ◀──────────────────────── │
                     │  TURN_RESULT ─────────────────────────▶ │
                     │  TURN_END    ─────────────────────────▶ │  ← Now sent after each turn
                     │                                         │
                     └─▶ GAME_OVER   ◀─────────────────────────┤
        ````
        
        ---
        
        ## 1  Messages
        
        ### 1.1 CONNECT  (client → server)
        
        | Field       | Type    | Required | Description                       |
        | ----------- | ------- | -------- | --------------------------------- |
        | `type`      | string  | ✓        | `"CONNECT"`                       |
        | `nick`      | string  | ✗        | Human nickname (for logs / UI)    |
        | `spectator` | boolean | ✗        | `true` to observe without playing |
        
        ```json
        { "type": "CONNECT", "nick": "bot-42" }
        ```
        
        ---
        
        ### 1.2 ASSIGN\_ID  (server → client)
        
        | Field       | Type    | Required | Description                                     |
        | ----------- | ------- | -------- | ----------------------------------------------- |
        | `type`      | string  | ✓        | `"ASSIGN_ID"`                                   |
        | `player_id` | integer | ✗        | Omitted for spectators; starts at 1 for players |
        
        ```json
        { "type": "ASSIGN_ID", "player_id": 2 }
        ```
        
        ---
        
        ### 1.3 TURN\_BEGIN  (server → all)
        
        | Field           | Type       | Required | Description                        |
        | --------------- | ---------- | -------- | ---------------------------------- |
        | `type`          | string     | ✓        | `"TURN_BEGIN"`                     |
        | `turn_index`    | integer    | ✓        | 0-based global turn counter        |
        | `player_id`     | integer    | ✓        | ID whose turn it is                |
        | `state`         | Game State | ✓        | Full snapshot                      |
        | `time_limit_ms` | integer    | ✓        | How long that player has to answer |
        
        ---
        
        ### 1.4 ACTION  (client → server)
        
        Sent **only** by the `player_id` that just received `TURN_BEGIN`.
        
        | Field       | Type   | Required | Description               |
        | ----------- | ------ | -------- | ------------------------- |
        | `type`      | string | ✓        | `"ACTION"`                |
        | `player_id` | int    | ✓        | Must match sender’s ID    |
        | `action`    | object | ✓        | One of the variants below |
        
        #### `action` variants
        
        | Variant         | Required keys                                                     | Notes                                                 |
        | --------------- | ----------------------------------------------------------------- | ----------------------------------------------------- |
        | stand           | `{ "action": "stand" }`                                           |                                                       |
        | walk            | `{ "action": "walk", "dx": float }`                               | `dx` in world units (+ → right); **max dx = 2**       |
        | attack\:kick    | `{ "action": "attack", "weapon": "kick" }`                        | Flat 80 damage to the first living worm within 1 unit |
        | attack\:bazooka | `{ "action": "attack", "weapon": "bazooka", "angle_deg": float }` | 0° = right, CCW positive                              |
        | attack\:grenade | `{ "action": "attack", "weapon": "grenade", "dx": float }`        | Single `dx` (max dx = 5); horizontal distance only    |
        
        Gravity is applied **instantly** only after a **walk** action: the worm falls in the same column until it lands on the first solid tile (`map[row][col] == 1`) or exits below the last row (water), which sets its `health` to `0`.
        
        ---
        
        ### 1.5 TURN\_RESULT  (server → all)
        
        | Field        | Type       | Required | Description                                    |
        | ------------ | ---------- | -------- | ---------------------------------------------- |
        | `type`       | string     | ✓        | `"TURN_RESULT"`                                |
        | `turn_index` | integer    | ✓        | Same index as the triggering turn              |
        | `player_id`  | integer    | ✓        | The acting player                              |
        | `state`      | Game State | ✓        | Resulting state                                |
        | `reward`     | number     | ✗        | Points earned this turn:                       |
        |              |            |          | • actual HP removed from any target(s)         |
        |              |            |          | • **+100** if the action killed a worm         |
        | `effects`    | object     | ✗        | Optional visual-only data (weapon, trajectory) |
        
        ---
        
        ### 1.6 TURN\_END  (server → all)
        
        | Field            | Type    | Required | Description          |
        | ---------------- | ------- | -------- | -------------------- |
        | `type`           | string  | ✓        | `"TURN_END"`         |
        | `next_player_id` | integer | ✓        | ID who will act next |
        
        ---
        
        ### 1.7 PLAYER\_ELIMINATED  (server → all)
        
        | Field       | Type    | Required | Description                 |
        | ----------- | ------- | -------- | --------------------------- |
        | `type`      | string  | ✓        | `"PLAYER_ELIMINATED"`       |
        | `player_id` | integer | ✓        | ID of the eliminated player |
        
        ---
        
        ### 1.8 GAME\_OVER  (server → all)
        
        | Field         | Type       | Required | Description        |
        | ------------- | ---------- | -------- | ------------------ |
        | `type`        | string     | ✓        | `"GAME_OVER"`      |
        | `winner_id`   | integer    | ✗        | Winner’s ID if any |
        | `final_state` | Game State | ✓        | Final snapshot     |
        
        ---
        
        ### 1.9 ERROR  (server → client)
        
        | Field  | Type   | Required | Description         |
        | ------ | ------ | -------- | ------------------- |
        | `type` | string | ✓        | `"ERROR"`           |
        | `msg`  | string | ✓        | Human-readable text |
        
        ---
        
        ## 2  Game State
        
        ```jsonc
        {
          "worms": [
            { "id": 0, "nick": "Alice", "health": 100, "x": 3.20, "y": 1.75 },
            { "id": 1, "nick": "Bot-42", "health":  90, "x": 6.00, "y": 2.00 }
          ],
          "map": [
            [1,0,0,0,0,0,0,0],
            [1,0,0,0,0,0,0,0],
            [1,1,1,0,0,0,1,0],
            [1,1,1,0,0,1,1,1]
          ]
        }
        ```
        
        | Key     | Type          | Description                                                                                                        |
        | ------- | ------------- | ------------------------------------------------------------------------------------------------------------------ |
        | `worms` | array         | Each worm’s **`id`**, **`nick`**, **`health`**, and **position** in world units (`x`, `y` floats; 1 unit = 1 tile) |
        | `map`   | 2-D int array | `1` = solid terrain, `0` = empty air. Values below the last row kill a worm instantly when fallen into water.      |
        
        Clients **must ignore** any extra keys they do not understand.
        
        ---
        
        ## 3  Rendering rule (client hint)
        
        ```text
        tile = min(canvasW / cols, canvasH / rows)
        xMargin = (canvasW − tile*cols)/2
        yMargin = (canvasH − tile*rows)/2
        draw tile(i,j) at (xMargin + i*tile , yMargin + j*tile)
        ```
        --- SLUTT INNHOLD (json-docs.md) ---

    |-- plan.md
        --- START INNHOLD (plan.md) ---
        - **learn**: runs and saves state of environment
        - **data**: saved here
        - **frontend**: reads data and plays it in a browser
        --- SLUTT INNHOLD (plan.md) ---

    |-- todo.md
        --- START INNHOLD (todo.md) ---
        max time for a game to last, incase of bugs
        --- SLUTT INNHOLD (todo.md) ---

agents/
    |-- __init__.py
        [--- INNHOLD: Tom fil ---]

    |-- client.py
        --- START INNHOLD (client.py) ---
        # File: agents/client.py
        #!/usr/bin/env python3
        """
        Reference bot that now keeps its WebSocket open across many games.
        It rests while eliminated and wakes up when NEW_GAME arrives.
        """
        import argparse
        import asyncio
        import json
        import logging
        import random
        
        import websockets
        
        def setup_logging() -> logging.Logger:
            parser = argparse.ArgumentParser(
                description="W.O.R.M.S. bot client"
            )
            parser.add_argument(
                "--log-level",
                choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                default="INFO",
                help="set logging level",
            )
            args = parser.parse_args()
            level = getattr(logging, args.log_level)
            logging.basicConfig(
                level=level,
                format="%(asctime)s %(levelname)-8s %(name)s: %(message)s",
                datefmt="%H:%M:%S",
            )
            return logging.getLogger("client")
        
        logger = setup_logging()
        
        HOST, PORT = "127.0.0.1", 8765
        
        # Base action types (templates)
        ACTIONS = [
            {"action": "stand"},
            {"action": "walk"},
            {"action": "attack", "weapon": "kick"},
            {"action": "attack", "weapon": "bazooka"},
            {"action": "attack", "weapon": "grenade"},
        ]
        
        # Probability weights for each entry in ACTIONS; must sum to 1.0
        ACTION_PROBS = [
            0.10,  # stand
            0.05,  # walk
            0.25,  # kick
            0.30,  # bazooka
            0.30,  # grenade
        ]
        
        # Validate at import time
        if len(ACTION_PROBS) != len(ACTIONS):
            raise ValueError(
                f"ACTION_PROBS length ({len(ACTION_PROBS)}) does not match "
                f"ACTIONS length ({len(ACTIONS)})"
            )
        total_prob = sum(ACTION_PROBS)
        if abs(total_prob - 1.0) > 1e-8:
            raise ValueError(f"ACTION_PROBS must sum to 1.0, but sum to {total_prob}")
        
        # Parameter ranges
        MAX_WALK_DX = 2.0
        MAX_GRENADE_DX = 5.0
        MAX_BAZOOKA_ANGLE_DEG = 360.0
        
        async def start_client() -> None:
            uri = f"ws://{HOST}:{PORT}"
            logger.info("connecting to %s", uri)
        
            async with websockets.connect(uri) as ws:
                await ws.send(json.dumps({"type": "CONNECT", "nick": "bot"}))
                player_id: int | None = None
                eliminated = False
        
                async for raw in ws:
                    msg = json.loads(raw)
                    t = msg.get("type")
        
                    if t == "ASSIGN_ID":
                        player_id = msg["player_id"]
                        logger.info("assigned player_id=%d", player_id)
        
                    elif t == "PLAYER_ELIMINATED" and msg.get("player_id") == player_id:
                        eliminated = True
                        logger.info("I have been eliminated this game")
        
                    elif t == "NEW_GAME":
                        eliminated = False
                        logger.info(
                            "new episode %s started – back in the game!",
                            msg.get("game_id"),
                        )
        
                    elif (
                        t == "TURN_BEGIN"
                        and msg.get("player_id") == player_id
                        and not eliminated
                    ):
                        # choose action template by probability
                        tmpl = random.choices(ACTIONS, weights=ACTION_PROBS, k=1)[0]
        
                        # sample parameters per action type
                        if tmpl["action"] == "stand":
                            action = {"action": "stand"}
        
                        elif tmpl["action"] == "walk":
                            dx = random.uniform(-MAX_WALK_DX, MAX_WALK_DX)
                            action = {"action": "walk", "dx": dx}
        
                        elif tmpl["action"] == "attack":
                            weapon = tmpl["weapon"]
                            if weapon == "kick":
                                action = {"action": "attack", "weapon": "kick"}
                            elif weapon == "bazooka":
                                angle_deg = random.uniform(0.0, MAX_BAZOOKA_ANGLE_DEG)
                                action = {
                                    "action": "attack",
                                    "weapon": "bazooka",
                                    "angle_deg": angle_deg,
                                }
                            elif weapon == "grenade":
                                dx = random.uniform(-MAX_GRENADE_DX, MAX_GRENADE_DX)
                                action = {
                                    "action": "attack",
                                    "weapon": "grenade",
                                    "dx": dx,
                                }
                            else:
                                # fallback to stand if unknown
                                action = {"action": "stand"}
                        else:
                            action = {"action": "stand"}
        
                        payload = {
                            "type": "ACTION",
                            "player_id": player_id,
                            "action": action,
                        }
                        await ws.send(json.dumps(payload))
                        logger.debug("did action: %r", payload)
        
                    elif t == "TURN_RESULT" and msg.get("player_id") == player_id:
                        reward = msg.get("reward", 0.0)
                        logger.info("received reward=%.1f", reward)
        
        if __name__ == "__main__":
            asyncio.run(start_client())
        --- SLUTT INNHOLD (client.py) ---

    |-- main_coordinator.py
        --- START INNHOLD (main_coordinator.py) ---
        # agents/main_coordinator.py
        import os
        
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
        
        import asyncio
        import websockets
        import json
        import torch
        import numpy as np
        import argparse
        from pathlib import Path
        import signal
        import matplotlib
        
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        import random
        
        # Felles config og utils
        from agents.common import config
        from agents.common.utils import format_action, \
            preprocess_state  # Selv om agenten gjør dette internt, greit å ha for ev. debug
        
        # Import agent klasser
        from agents.a2c_manual.agent import A2CAgent
        from agents.ppo_manual.agent import PPOAgent
        
        # --- Global logging for plotting ---
        TRAINING_STATS = {}
        PLOT_DIR = Path(__file__).resolve().parent / "training_plots_output"
        PLOT_DIR.mkdir(parents=True, exist_ok=True)
        SHUTDOWN_FLAG = asyncio.Event()
        
        
        def signal_handler_main(signum, frame):
            if not SHUTDOWN_FLAG.is_set():
                print("\n(Hovedkoordinator) Mottok avslutningssignal. Setter shutdown flagg...")
                SHUTDOWN_FLAG.set()
            else:
                print("(Hovedkoordinator) Avslutningssignal allerede mottatt.")
        
        
        async def run_single_agent_session(agent_id_str: str, agent_type: str, checkpoint_filename: str):
            uri = f"ws://{config.SERVER_HOST}:{config.SERVER_PORT}"
            agent_name = f"{agent_type.upper()}_Agent_{agent_id_str}"
        
            if agent_type == 'a2c':
                agent = A2CAgent(agent_name=agent_name)
            elif agent_type == 'ppo':
                agent = PPOAgent(agent_name=agent_name)
            else:
                raise ValueError(f"Ukjent agent_type: {agent_type}")
        
            if agent_id_str not in TRAINING_STATS:
                TRAINING_STATS[agent_id_str] = {
                    "agent_type": agent_type,  # Nytt for plotting
                    "game_rewards_raw": [],
                    "policy_loss_per_game": [],
                    "value_loss_per_game": [],
                    "entropy_loss_per_game": [],
                    "games_played_count": 0,
                    "total_steps_across_games": 0
                }
        
            checkpoint_path = Path(__file__).resolve().parent / f"{agent_type}_manual" / checkpoint_filename
            agent.load_model(checkpoint_path)
        
            connection_attempts = 0
            max_connection_attempts = 10
        
            while not SHUTDOWN_FLAG.is_set() and \
                    TRAINING_STATS[agent_id_str]["games_played_count"] < config.NUM_GAMES_PER_AGENT_SESSION:
        
                temp_policy_losses_for_game = []
                temp_value_losses_for_game = []
                temp_entropy_losses_for_game = []
                current_game_step_rewards = []
        
                if agent_type == 'a2c':
                    agent.clear_buffers()  # A2C tømmer internt etter learn()
                # PPO tømmer bufferne (og batch_count) når learn() faktisk kjører en oppdatering
        
                current_game_id = None
                is_my_turn_flag = False
                am_i_eliminated_this_game = False
                last_known_state_for_learn = None
                steps_this_game = 0
        
                try:
                    async with websockets.connect(uri, ping_interval=20, ping_timeout=30, open_timeout=15) as websocket:
                        connection_attempts = 0
                        await websocket.send(json.dumps({"type": "CONNECT", "nick": agent_name}))
        
                        assign_id_msg_str = await asyncio.wait_for(websocket.recv(), timeout=10)
                        assign_id_msg = json.loads(assign_id_msg_str)
        
                        if assign_id_msg.get("type") == "ASSIGN_ID":
                            player_id = assign_id_msg.get("player_id")
                            if player_id is None:
                                print(f"[{agent_name}] FEIL: Fikk ikke player_id.")
                                await asyncio.sleep(random.uniform(3, 7));
                                continue
                            agent.set_player_id(player_id)
                        else:
                            print(f"[{agent_name}] FEIL: Forventet ASSIGN_ID, fikk {assign_id_msg}.")
                            await asyncio.sleep(random.uniform(3, 7));
                            continue
        
                        async for message_str in websocket:
                            if SHUTDOWN_FLAG.is_set(): break
                            try:
                                msg = json.loads(message_str)
                                msg_type = msg.get("type")
        
                                if msg_type == "NEW_GAME":
                                    new_game_id = msg.get("game_id")
                                    if current_game_id is not None and current_game_id != new_game_id and \
                                            not am_i_eliminated_this_game:
                                        if agent_type == 'a2c' and agent.rewards_buffer:
                                            p_l, v_l, e_l = agent.learn(last_known_state_for_learn, True)
                                            if p_l is not None:
                                                temp_policy_losses_for_game.append(p_l)
                                                temp_value_losses_for_game.append(v_l)
                                                temp_entropy_losses_for_game.append(e_l)
                                        elif agent_type == 'ppo' and agent.batch_count > 0:  # PPO might not have enough for a batch
                                            # For PPO, vi må tvinge en læring selv om batchen ikke er full,
                                            # siden spillet slutter uventet.
                                            # La oss si at PPO's learn håndterer dette ved å enten lære på det den har,
                                            # eller bare tømme bufferne hvis det er for lite.
                                            # Dette er en forenkling. Ideelt sett burde PPO lære når batchen er full.
                                            # Men for GAME_OVER-lignende eventer, må vi håndtere det.
                                            # Vi setter ikke done=True her, da learn() for PPO sjekker dones_buffer
                                            p_l, v_l, e_l = agent.learn(
                                                last_known_state_for_learn)  # PPO uses internal dones_buffer
                                            if p_l is not None:
                                                temp_policy_losses_for_game.append(p_l)
                                                temp_value_losses_for_game.append(v_l)
                                                temp_entropy_losses_for_game.append(e_l)
        
                                        if temp_policy_losses_for_game:  # Logg ufullstendig spill
                                            TRAINING_STATS[agent_id_str]['game_rewards_raw'].append(
                                                sum(current_game_step_rewards))
                                            TRAINING_STATS[agent_id_str]['policy_loss_per_game'].append(
                                                np.mean(temp_policy_losses_for_game))
                                            TRAINING_STATS[agent_id_str]['value_loss_per_game'].append(
                                                np.mean(temp_value_losses_for_game))
                                            TRAINING_STATS[agent_id_str]['entropy_loss_per_game'].append(
                                                np.mean(temp_entropy_losses_for_game))
                                            TRAINING_STATS[agent_id_str]["games_played_count"] += 1
                                            TRAINING_STATS[agent_id_str]["total_steps_across_games"] += steps_this_game
        
                                    current_game_id = new_game_id
                                    if agent_type == 'a2c': agent.clear_buffers()
                                    # PPO tømmer når batchen er full i learn()
                                    temp_policy_losses_for_game, temp_value_losses_for_game, temp_entropy_losses_for_game, current_game_step_rewards = [], [], [], []
                                    am_i_eliminated_this_game = False
                                    is_my_turn_flag = False
                                    last_known_state_for_learn = msg.get("state")
                                    steps_this_game = 0
        
                                elif msg_type == "TURN_BEGIN":
                                    if current_game_id is None: continue
                                    game_state_for_action = msg.get("state")
                                    last_known_state_for_learn = game_state_for_action
        
                                    if msg.get("player_id") == agent.player_id and not am_i_eliminated_this_game:
                                        is_my_turn_flag = True
                                        steps_this_game += 1
                                        if not game_state_for_action:  # Skulle ikke skje hvis orm er i live
                                            action_to_send_obj = {"action": "stand"}
                                            if agent_type == 'a2c': agent.values_buffer.append(
                                                torch.tensor(0.0, device=config.DEVICE))
                                            # For PPO, håndteres dette i select_action
                                        else:
                                            action_to_send_obj = agent.select_action(game_state_for_action)
        
                                        await websocket.send(json.dumps({
                                            "type": "ACTION", "player_id": agent.player_id,
                                            "action": action_to_send_obj
                                        }))
                                    else:
                                        is_my_turn_flag = False
        
                                elif msg_type == "TURN_RESULT":
                                    if current_game_id is None: continue
                                    last_known_state_for_learn = msg.get("state")
                                    if msg.get("player_id") == agent.player_id and is_my_turn_flag:
                                        reward = msg.get("reward", 0.0)
                                        current_game_step_rewards.append(reward)
                                        # For A2C, done er per episode. For PPO, done er per steg.
                                        # Serveren sender ikke 'done' i TURN_RESULT. Vi antar 'done=False' her for PPO.
                                        # A2C venter på PLAYER_ELIMINATED eller GAME_OVER for 'done'.
                                        if agent_type == 'a2c':
                                            agent.store_reward(reward)
                                        elif agent_type == 'ppo':
                                            # Anta at ormen fortsatt er i live med mindre ELIMINATED eller GAME_OVER kommer
                                            is_my_worm_alive_now = any(
                                                w['id'] == (agent.player_id - 1) and w['health'] > 0 for w in
                                                last_known_state_for_learn.get('worms', []))
                                            step_done_for_ppo = not is_my_worm_alive_now
                                            agent.store_reward_and_done(reward, step_done_for_ppo)
        
                                            # PPO lærer per batch
                                            if agent.batch_count >= (
                                            config.PPO_BATCH_SIZE if hasattr(config, 'PPO_BATCH_SIZE') else 128):
                                                p_l, v_l, e_l = agent.learn(last_known_state_for_learn)
                                                if p_l is not None:
                                                    temp_policy_losses_for_game.append(p_l)
                                                    temp_value_losses_for_game.append(v_l)
                                                    temp_entropy_losses_for_game.append(e_l)
                                    is_my_turn_flag = False
        
                                elif msg_type == "PLAYER_ELIMINATED":
                                    if current_game_id is None: continue
                                    elim_player_id = msg.get("player_id")
                                    if elim_player_id == agent.player_id and not am_i_eliminated_this_game:
                                        am_i_eliminated_this_game = True
                                        if agent_type == 'a2c':
                                            if agent.rewards_buffer:
                                                p_l, v_l, e_l = agent.learn(last_known_state_for_learn, True)  # done=True
                                                if p_l is not None:
                                                    temp_policy_losses_for_game.append(p_l)
                                                    temp_value_losses_for_game.append(v_l)
                                                    temp_entropy_losses_for_game.append(e_l)
                                            else:
                                                agent.clear_buffers()
                                        elif agent_type == 'ppo':
                                            # Siste 'done' for PPO ble satt i TURN_RESULT (hvis helse <=0).
                                            # Hvis det er data igjen i batchen, prøv å lære.
                                            if agent.batch_count > 0:
                                                p_l, v_l, e_l = agent.learn(last_known_state_for_learn)
                                                if p_l is not None:
                                                    temp_policy_losses_for_game.append(p_l)
                                                    temp_value_losses_for_game.append(v_l)
                                                    temp_entropy_losses_for_game.append(e_l)
                                            agent.clear_buffers_and_count()  # PPO tømmer etter learn
        
                                elif msg_type == "GAME_OVER":
                                    if current_game_id is None: continue
                                    final_state_for_learn = msg.get("final_state")
        
                                    if not am_i_eliminated_this_game:  # Hvis jeg fortsatt var i live
                                        if agent_type == 'a2c':
                                            if agent.rewards_buffer:
                                                p_l, v_l, e_l = agent.learn(final_state_for_learn, True)
                                                if p_l is not None:
                                                    temp_policy_losses_for_game.append(p_l)
                                                    temp_value_losses_for_game.append(v_l)
                                                    temp_entropy_losses_for_game.append(e_l)
                                            else:
                                                agent.clear_buffers()
                                        elif agent_type == 'ppo':
                                            # PPO: Siste 'done' ble satt i siste TURN_RESULT. Lær hvis batch er full.
                                            if agent.batch_count > 0:
                                                p_l, v_l, e_l = agent.learn(final_state_for_learn)
                                                if p_l is not None:
                                                    temp_policy_losses_for_game.append(p_l)
                                                    temp_value_losses_for_game.append(v_l)
                                                    temp_entropy_losses_for_game.append(e_l)
                                            agent.clear_buffers_and_count()
        
                                    # Logg statistikk for det fullførte spillet
                                    TRAINING_STATS[agent_id_str]['game_rewards_raw'].append(sum(current_game_step_rewards))
                                    if temp_policy_losses_for_game: TRAINING_STATS[agent_id_str]['policy_loss_per_game'].append(
                                        np.mean(temp_policy_losses_for_game))
                                    if temp_value_losses_for_game: TRAINING_STATS[agent_id_str]['value_loss_per_game'].append(
                                        np.mean(temp_value_losses_for_game))
                                    if temp_entropy_losses_for_game: TRAINING_STATS[agent_id_str][
                                        'entropy_loss_per_game'].append(np.mean(temp_entropy_losses_for_game))
        
                                    TRAINING_STATS[agent_id_str]["games_played_count"] += 1
                                    TRAINING_STATS[agent_id_str]["total_steps_across_games"] += steps_this_game
        
                                    games_count_this_agent = TRAINING_STATS[agent_id_str]["games_played_count"]
                                    print(
                                        f"[{agent_name}] Spill {current_game_id} ferdig. Belønning: {sum(current_game_step_rewards):.2f}. (Agent totalt {games_count_this_agent} spill, {steps_this_game} steg)")
        
                                    if games_count_this_agent > 0 and games_count_this_agent % config.SAVE_MODEL_EVERY_N_GAMES == 0:
                                        agent.save_model(checkpoint_path)
        
                                    if games_count_this_agent > 0 and games_count_this_agent % config.PLOT_STATS_EVERY_N_GAMES == 0:
                                        plot_aggregated_training_results()
        
                                    current_game_id = None  # Klar for neste NEW_GAME
        
                                elif msg_type == "TURN_END":
                                    pass
                                elif msg_type == "ERROR":
                                    print(f"[{agent_name}] FEIL fra server: {msg.get('msg')}")
        
                            except json.JSONDecodeError:
                                print(f"[{agent_name}] FEIL: Kunne ikke dekode JSON: {message_str}")
                            except websockets.exceptions.ConnectionClosed as e_ws_msg:
                                print(f"[{agent_name}] Tilkobling lukket (under melding): {e_ws_msg}.")
                                break
                            except Exception as e_inner:
                                print(f"[{agent_name}] FEIL i meldingsløkke: {type(e_inner).__name__} - {e_inner}")
                                # import traceback; traceback.print_exc() # For dypere debug
                                break
        
                    # Håndter disconnect utenom GAME_OVER (hvis data finnes og spillet var i gang)
                    if current_game_id is not None and not am_i_eliminated_this_game:
                        if agent_type == 'a2c' and agent.rewards_buffer:
                            print(f"[{agent_name}] Lærer fra ufullstendig spill {current_game_id} (A2C) pga. disconnect.")
                            p_l, v_l, e_l = agent.learn(last_known_state_for_learn, True)
                            if p_l is not None: temp_policy_losses_for_game.append(p_l); temp_value_losses_for_game.append(
                                v_l); temp_entropy_losses_for_game.append(e_l)
                        elif agent_type == 'ppo' and agent.batch_count > 0:
                            print(f"[{agent_name}] Lærer fra ufullstendig spill {current_game_id} (PPO) pga. disconnect.")
                            p_l, v_l, e_l = agent.learn(last_known_state_for_learn)
                            if p_l is not None: temp_policy_losses_for_game.append(p_l); temp_value_losses_for_game.append(
                                v_l); temp_entropy_losses_for_game.append(e_l)
        
                        if temp_policy_losses_for_game:  # Logg dette ufullstendige spillet også
                            TRAINING_STATS[agent_id_str]['game_rewards_raw'].append(sum(current_game_step_rewards))
                            TRAINING_STATS[agent_id_str]['policy_loss_per_game'].append(np.mean(temp_policy_losses_for_game))
                            TRAINING_STATS[agent_id_str]['value_loss_per_game'].append(np.mean(temp_value_losses_for_game))
                            TRAINING_STATS[agent_id_str]['entropy_loss_per_game'].append(np.mean(temp_entropy_losses_for_game))
                            TRAINING_STATS[agent_id_str]["games_played_count"] += 1
                            TRAINING_STATS[agent_id_str]["total_steps_across_games"] += steps_this_game
        
                    if SHUTDOWN_FLAG.is_set():
                        print(f"[{agent_name}] Avslutter økt pga shutdown flagg.")
                        break
        
                    # Websocket-tilkobling brutt, vent og prøv igjen
                    connection_attempts += 1
                    wait_time = min(2 ** connection_attempts, 60)  # Exponential backoff
                    print(
                        f"[{agent_name}] Tilkoblingsfeil. Prøver igjen om {wait_time}s. (Forsøk {connection_attempts}/{max_connection_attempts})")
                    if connection_attempts >= max_connection_attempts and not SHUTDOWN_FLAG.is_set():
                        print(f"[{agent_name}] Maks antall tilkoblingsforsøk nådd. Avslutter denne agent-økten.")
                        SHUTDOWN_FLAG.set();
                        break
                    try:
                        await asyncio.sleep(wait_time)
                    except asyncio.CancelledError:
                        if SHUTDOWN_FLAG.is_set(): break
        
                except Exception as e_outer:
                    print(
                        f"[{agent_name}] Alvorlig FEIL (ytre løkke): {type(e_outer).__name__} - {e_outer}. Avslutter agent-økt.")
                    # import traceback; traceback.print_exc()
                    SHUTDOWN_FLAG.set();
                    break
        
            print(f"[{agent_name}] Økt ferdig. Totalt {TRAINING_STATS[agent_id_str]['games_played_count']} spill behandlet.")
            agent.save_model(checkpoint_path)
        
        
        def plot_aggregated_training_results():
            try:
                num_agents_with_data = sum(1 for agent_id in TRAINING_STATS if TRAINING_STATS[agent_id]["game_rewards_raw"])
                if num_agents_with_data == 0:
                    print("Ingen data å plotte for noen agenter.")
                    return
        
                num_plot_rows = num_agents_with_data
                fig, axs = plt.subplots(num_plot_rows, 4, figsize=(24, 6 * num_plot_rows), squeeze=False)  # Lagt til entropy
                current_plot_row = 0
        
                for agent_id_str, stats in TRAINING_STATS.items():
                    if not stats["game_rewards_raw"]:
                        continue
        
                    agent_type_label = stats.get("agent_type", "Ukjent").upper()
        
                    # Rewards
                    rewards_raw = stats["game_rewards_raw"]
                    axs[current_plot_row, 0].cla()
                    axs[current_plot_row, 0].plot(rewards_raw, label='Belønning per spill', alpha=0.7, linestyle='-',
                                                  marker='.', markersize=3)
                    if len(rewards_raw) >= 10:
                        window = min(max(10, len(rewards_raw) // 10), 100)
                        avg_rewards = np.convolve(rewards_raw, np.ones(window) / window, mode='valid')
                        axs[current_plot_row, 0].plot(np.arange(window - 1, len(rewards_raw)), avg_rewards,
                                                      label=f'Gj.snitt ({window} spill)', color='red', lw=2)
                    axs[current_plot_row, 0].set_title(f"Agent {agent_id_str} ({agent_type_label}) - Belønning")
                    axs[current_plot_row, 0].set_xlabel("Spill #");
                    axs[current_plot_row, 0].set_ylabel("Total Belønning")
                    axs[current_plot_row, 0].legend();
                    axs[current_plot_row, 0].grid(True)
        
                    # Policy Loss
                    policy_losses_pg = stats.get('policy_loss_per_game', [])
                    axs[current_plot_row, 1].cla()
                    if policy_losses_pg: axs[current_plot_row, 1].plot(policy_losses_pg, label='Policy Loss', marker='.',
                                                                       markersize=3)
                    axs[current_plot_row, 1].set_title(f"Agent {agent_id_str} ({agent_type_label}) - Policy Tap")
                    axs[current_plot_row, 1].set_xlabel("Spill #");
                    axs[current_plot_row, 1].set_ylabel("Gj.snitt Policy Tap")
                    axs[current_plot_row, 1].legend();
                    axs[current_plot_row, 1].grid(True)
        
                    # Value Loss
                    value_losses_pg = stats.get('value_loss_per_game', [])
                    axs[current_plot_row, 2].cla()
                    if value_losses_pg: axs[current_plot_row, 2].plot(value_losses_pg, label='Value Loss', color='green',
                                                                      marker='.', markersize=3)
                    axs[current_plot_row, 2].set_title(f"Agent {agent_id_str} ({agent_type_label}) - Value Tap")
                    axs[current_plot_row, 2].set_xlabel("Spill #");
                    axs[current_plot_row, 2].set_ylabel("Gj.snitt Value Tap")
                    axs[current_plot_row, 2].legend();
                    axs[current_plot_row, 2].grid(True)
        
                    # Entropy Loss
                    entropy_losses_pg = stats.get('entropy_loss_per_game', [])
                    axs[current_plot_row, 3].cla()
                    if entropy_losses_pg: axs[current_plot_row, 3].plot(entropy_losses_pg, label='Entropy Loss', color='purple',
                                                                        marker='.', markersize=3)
                    axs[current_plot_row, 3].set_title(f"Agent {agent_id_str} ({agent_type_label}) - Entropy Tap")
                    axs[current_plot_row, 3].set_xlabel("Spill #");
                    axs[current_plot_row, 3].set_ylabel("Gj.snitt Entropy Tap")
                    axs[current_plot_row, 3].legend();
                    axs[current_plot_row, 3].grid(True)
        
                    current_plot_row += 1
        
                plt.tight_layout(pad=2.5)
                plot_filename = PLOT_DIR / f"aggregated_training_plots_multialgo.png"
                plt.savefig(plot_filename)
                print(f"Lagret/oppdatert aggregert treningsplot: {plot_filename}")
                plt.close(fig)
        
            except ImportError:
                print("Matplotlib ikke installert.")
            except Exception as plot_e:
                print(f"Kunne ikke plotte: {type(plot_e).__name__} - {plot_e}")
        
        
        async def main():
            parser = argparse.ArgumentParser(description="Kjør RL agenter for W.O.R.M.S.")
            parser.add_argument("--num_agents", type=int, default=1, choices=[1, 2],
                                help="Antall agenter å kjøre (1 eller 2).")
            parser.add_argument("--agent1_type", type=str, default="a2c", choices=["a2c", "ppo"],
                                help="Type for agent 1 (a2c eller ppo).")
            parser.add_argument("--agent2_type", type=str, default="ppo", choices=["a2c", "ppo"],
                                help="Type for agent 2 (a2c eller ppo), hvis num_agents=2.")
        
            args = parser.parse_args()
        
            signal.signal(signal.SIGINT, signal_handler_main)
            signal.signal(signal.SIGTERM, signal_handler_main)
        
            tasks = []
            if args.num_agents >= 1:
                print(f"(Koordinator) Forbereder Agent 1 ({args.agent1_type.upper()})...")
                tasks.append(asyncio.create_task(run_single_agent_session(
                    agent_id_str="1",
                    agent_type=args.agent1_type,
                    checkpoint_filename=f"{args.agent1_type}_worms_agent1_checkpoint.pth"
                )))
        
            if args.num_agents == 2:
                print(f"(Koordinator) Forbereder Agent 2 ({args.agent2_type.upper()})...")
                tasks.append(asyncio.create_task(run_single_agent_session(
                    agent_id_str="2",
                    agent_type=args.agent2_type,
                    checkpoint_filename=f"{args.agent2_type}_worms_agent2_checkpoint.pth"
                )))
        
            if not tasks:
                print("Ingen agenter spesifisert.")
                return
        
            print(f"(Koordinator) Starter {len(tasks)} agent-økt(er)...")
            try:
                await asyncio.gather(*tasks)
            except asyncio.CancelledError:
                print("(Koordinator) Hovedoppgave (gather) kansellert.")
        
            print("\n(Koordinator) Alle agent-økter er avsluttet eller avbrutt.")
            print("(Koordinator) Genererer endelige plott...")
            plot_aggregated_training_results()
            print("(Koordinator) Programmet avsluttes.")
        
        
        if __name__ == "__main__":
            try:
                asyncio.run(main())
            except KeyboardInterrupt:
                print("\n(Hovedprogram) KeyboardInterrupt. Avslutter.")
                SHUTDOWN_FLAG.set()
        --- SLUTT INNHOLD (main_coordinator.py) ---

    a2c_manual/
        |-- __init__.py
            [--- INNHOLD: Tom fil ---]

        |-- agent.py
            --- START INNHOLD (agent.py) ---
            # agents/a2c_manual/agent.py
            import torch
            import torch.optim as optim
            import torch.nn.functional as F
            from torch.distributions import Categorical, Normal
            import numpy as np
            from pathlib import Path
            
            from agents.common import config
            from .model import ActorCriticNetwork # model.py forblir foreløpig lokal for A2C
            from agents.common.utils import preprocess_state, format_action
            
            
            class A2CAgent:
                def __init__(self, agent_name="A2C_Agent_Default"):
                    self.network = ActorCriticNetwork().to(config.DEVICE)
                    self.optimizer = optim.Adam(self.network.parameters(), lr=config.LEARNING_RATE)
                    self.player_id = None  # Settes av main_coordinator.py ved ASSIGN_ID
                    self.agent_name = agent_name  # For logging og unike sjekkpunktfiler
            
                    # Buffere for ett læringssteg (tømmes etter hver .learn())
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.entropies_buffer = []
            
                def set_player_id(self, player_id: int):
                    self.player_id = player_id
                    # print(f"[{self.agent_name}] Player ID satt til: {self.player_id}")
            
                def select_action(self, current_game_state_json: dict):
                    if self.player_id is None:
                        # print(f"[{self.agent_name}] FEIL select_action: Player ID ikke satt. Sender 'stand'.")
                        return {"action": "stand"}
            
                    # Sjekk om vår orm er i live før vi gjør noe
                    my_worm_json_id = self.player_id - 1
                    is_my_worm_alive = any(
                        w['id'] == my_worm_json_id and w['health'] > 0
                        for w in current_game_state_json.get('worms', [])
                    )
                    if not is_my_worm_alive:
                        # print(f"[{self.agent_name}] Info select_action: Min orm (P{self.player_id}) er ikke lenger i live. Sender 'stand'.")
                        # Det er viktig å lagre noe i bufferne selv om vi sender stand, slik at learn() ikke krasjer
                        # pga. ulik bufferlengde. En dummy-verdi (f.eks. for V(s)) er nok.
                        # Siden vi ikke tar en reell handling, lagrer vi ikke log_prob eller entropi.
                        # Value for en "død" state kan anses som 0.
                        self.values_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        return {"action": "stand"}
            
                    map_tensor, worm_vector_tensor = preprocess_state(current_game_state_json, self.player_id)
            
                    try:
                        actor_outputs, state_value_tensor = self.network(map_tensor, worm_vector_tensor)
                    except Exception as e:
                        print(f"[{self.agent_name}] FEIL under network forward pass i select_action: {e}")
                        # Fallback for å unngå krasj, og lagre dummy value
                        self.values_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        return {"action": "stand"}
            
                    self.values_buffer.append(state_value_tensor.squeeze())  # Skalar tensor
            
                    action_type_probs = actor_outputs['action_type_probs']
                    action_type_dist = Categorical(action_type_probs)
                    network_action_idx_tensor = action_type_dist.sample()  # 0-dim tensor
                    network_action_idx_item = network_action_idx_tensor.item()
            
                    log_prob_action_type = action_type_dist.log_prob(network_action_idx_tensor)  # Skalar
                    entropy_action_type = action_type_dist.entropy()  # Skalar
            
                    step_log_probs = [log_prob_action_type]
                    step_entropies = [entropy_action_type]
                    params_for_formatting = {}
                    chosen_network_action_name = config.NETWORK_ACTION_ORDER[network_action_idx_item]
            
                    if chosen_network_action_name == 'walk':
                        walk_dx_probs = actor_outputs['walk_dx_probs']
                        walk_dx_dist = Categorical(walk_dx_probs)
                        walk_dx_bin_idx_tensor = walk_dx_dist.sample()
                        params_for_formatting['walk_dx_bin_idx'] = walk_dx_bin_idx_tensor.item()
                        step_log_probs.append(walk_dx_dist.log_prob(walk_dx_bin_idx_tensor))
                        step_entropies.append(walk_dx_dist.entropy())
            
                    elif chosen_network_action_name == 'attack_kick':
                        # Ingen parametere å sample for kick iht. nye specs
                        pass
            
                    elif chosen_network_action_name == 'attack_bazooka':
                        angle_mean, angle_std = actor_outputs['bazooka_params']
                        dist = Normal(angle_mean.squeeze(), angle_std.squeeze())
                        angle_val_tensor = dist.sample()
                        params_for_formatting['bazooka_angle_val'] = angle_val_tensor.item()
                        step_log_probs.append(dist.log_prob(angle_val_tensor))
                        step_entropies.append(dist.entropy())
            
                    elif chosen_network_action_name == 'attack_grenade':
                        grenade_dx_probs = actor_outputs['grenade_dx_probs']
                        grenade_dx_dist = Categorical(grenade_dx_probs)
                        grenade_dx_bin_idx_tensor = grenade_dx_dist.sample()
                        params_for_formatting['grenade_dx_bin_idx'] = grenade_dx_bin_idx_tensor.item()
                        step_log_probs.append(grenade_dx_dist.log_prob(grenade_dx_bin_idx_tensor))
                        step_entropies.append(grenade_dx_dist.entropy())
            
                    try:
                        # Sikre at alle elementer er skalar-tensorer før stack
                        step_log_probs = [lp.squeeze() for lp in step_log_probs]
                        step_entropies = [e.squeeze() for e in step_entropies]
            
                        stacked_log_probs = torch.stack(step_log_probs)
                        summed_log_probs = stacked_log_probs.sum()
                        self.log_probs_buffer.append(summed_log_probs)
            
                        stacked_entropies = torch.stack(step_entropies)
                        summed_entropies = stacked_entropies.sum()
                        self.entropies_buffer.append(summed_entropies)
                    except Exception as e_stack:
                        print(f"[{self.agent_name}] FEIL select_action stacking/summing: {e_stack}")
                        # Fallback for å opprettholde buffer-integritet
                        self.log_probs_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        self.entropies_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
            
                    action_json_to_send = format_action(network_action_idx_item, params_for_formatting)
                    return action_json_to_send
            
                def store_reward(self, reward: float):
                    self.rewards_buffer.append(reward)
            
                def learn(self, next_game_state_json: dict | None, done: bool):
                    # Kritisk sjekk: Antall lagrede verdier må matche antall lagrede rewards.
                    # Hver 'select_action' legger til i log_probs, values, entropies.
                    # Hver 'store_reward' (som kalles etter en handling) legger til i rewards.
                    # Så lengden på rewards_buffer skal være lik lengden på de andre bufferne
                    # *før* vi legger til V(s_next) eller gjør noe med returns.
            
                    num_rewards = len(self.rewards_buffer)
                    consistent_buffers = (
                            len(self.log_probs_buffer) == num_rewards and
                            len(self.values_buffer) == num_rewards and  # values_buffer har V(s_0) ... V(s_T-1)
                            len(self.entropies_buffer) == num_rewards
                    )
            
                    if not consistent_buffers or num_rewards == 0:
                        # print(f"[{self.agent_name}] DEBUG learn: Ujevne eller tomme buffere. Tømmer og returnerer.")
                        # print(f"  LP:{len(self.log_probs_buffer)} V:{len(self.values_buffer)} R:{len(self.rewards_buffer)} E:{len(self.entropies_buffer)}")
                        self.clear_buffers()
                        return None, None, None
            
                    R_bootstrap = torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32)
                    if not done:
                        if next_game_state_json and self.player_id is not None:
                            my_worm_json_id = self.player_id - 1
                            is_my_worm_alive_in_next_state = any(
                                w['id'] == my_worm_json_id and w['health'] > 0
                                for w in next_game_state_json.get('worms', [])
                            )
                            if is_my_worm_alive_in_next_state:
                                if isinstance(next_game_state_json, dict) and \
                                        next_game_state_json.get("map") and next_game_state_json.get("worms"):
                                    next_map, next_worm = preprocess_state(next_game_state_json, self.player_id)
                                    with torch.no_grad():
                                        _, next_value_tensor = self.network(next_map, next_worm)
                                        R_bootstrap = next_value_tensor.squeeze()
            
                    returns = []
                    R_discounted = R_bootstrap
                    for r_idx in range(num_rewards - 1, -1, -1):
                        reward_val = torch.tensor(self.rewards_buffer[r_idx], device=config.DEVICE, dtype=torch.float32)
                        R_discounted = reward_val + config.GAMMA * R_discounted
                        returns.insert(0, R_discounted)
            
                    if not returns:  # Bør ikke skje hvis num_rewards > 0
                        self.clear_buffers()
                        return None, None, None
            
                    try:
                        returns_tensor = torch.stack(returns).detach()
                        values_tensor = torch.stack(self.values_buffer)  # values_buffer skal ha num_rewards elementer
                        log_probs_tensor = torch.stack(self.log_probs_buffer)
                        entropies_tensor = torch.stack(self.entropies_buffer)
                    except RuntimeError as e_stack:
                        print(f"[{self.agent_name}] FEIL learn (stack): {e_stack}")
                        print(
                            f"  R:{len(returns)}, V:{len(self.values_buffer)}, LP:{len(self.log_probs_buffer)}, E:{len(self.entropies_buffer)}")
                        self.clear_buffers()
                        return None, None, None
            
                    # Nå SKAL alle tensorer ha samme lengde (num_rewards)
                    if not (returns_tensor.shape[0] == values_tensor.shape[0] == \
                            log_probs_tensor.shape[0] == entropies_tensor.shape[0] == num_rewards):
                        print(f"[{self.agent_name}] KRITISK FEIL learn: Tensorstørrelser stemmer ikke etter stack!")
                        print(
                            f"  Shape R:{returns_tensor.shape}, V:{values_tensor.shape}, LP:{log_probs_tensor.shape}, E:{entropies_tensor.shape}, Expected:{num_rewards}")
                        self.clear_buffers()
                        return None, None, None
            
                    advantages = returns_tensor - values_tensor
                    if advantages.numel() > 1:
                        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
                    policy_loss = (-log_probs_tensor * advantages.detach()).mean()
                    value_loss = F.mse_loss(values_tensor, returns_tensor)
                    entropy_loss = -entropies_tensor.mean()
            
                    total_loss = policy_loss + \
                                 config.VALUE_LOSS_COEF * value_loss + \
                                 config.ENTROPY_COEF * entropy_loss
            
                    self.optimizer.zero_grad()
                    total_loss.backward()
                    if hasattr(config, 'MAX_GRAD_NORM') and config.MAX_GRAD_NORM is not None:
                        torch.nn.utils.clip_grad_norm_(self.network.parameters(), config.MAX_GRAD_NORM)
                    self.optimizer.step()
            
                    # print(f"[{self.agent_name}] Lært. Losses - P:{policy_loss.item():.3f} V:{value_loss.item():.3f} E:{entropy_loss.item():.3f}")
                    self.clear_buffers()
                    return policy_loss.item(), value_loss.item(), entropy_loss.item()
            
                def clear_buffers(self):
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.entropies_buffer = []
            
                def save_model(self, path_str: str):
                    path = Path(path_str)
                    path.parent.mkdir(parents=True, exist_ok=True)  # Sikre at mappen finnes
                    try:
                        torch.save(self.network.state_dict(), path)
                        # print(f"[{self.agent_name}] Modell lagret til {path}")
                    except Exception as e:
                        print(f"[{self.agent_name}] Kunne ikke lagre modell til {path}: {e}")
            
                def load_model(self, path_str: str):
                    path = Path(path_str)
                    if not path.exists():
                        print(f"[{self.agent_name}] Ingen modell funnet på {path}, starter med ny/tilfeldig initialisert modell.")
                        return
            
                    try:
                        self.network.load_state_dict(torch.load(path, map_location=torch.device(config.DEVICE)))
                        self.network.eval()  # Sett til evaluation mode etter lasting
                        print(f"[{self.agent_name}] Modell lastet fra {path}")
                    except Exception as e:
                        print(f"[{self.agent_name}] Kunne ikke laste modell fra {path}: {e}. Bruker ny modell.")
            --- SLUTT INNHOLD (agent.py) ---

        |-- model.py
            --- START INNHOLD (model.py) ---
            # agents/a2c_manual/model.py
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            # Denne skal være:
            from agents.common import config
            # Hvis du hadde "..common", og det virket, er det ok, men "agents.common" er mer robust
            # når du kjører fra en høyere mappe med -m.
            
            
            class ActorCriticNetwork(nn.Module):
                def __init__(self):
                    super(ActorCriticNetwork, self).__init__()
            
                    # --- CNN for Map Processing ---
                    self.conv1 = nn.Conv2d(config.CNN_INPUT_CHANNELS, 16, kernel_size=8, stride=4)
                    self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)
                    self.pool = nn.AdaptiveMaxPool2d((6, 6))
                    # CNN_FEATURE_DIM forblir 32*6*6 = 1152
            
                    # --- Felles Fullt Tilkoblede Lag ---
                    self.fc_shared1 = nn.Linear(config.CNN_FEATURE_DIM + config.WORM_VECTOR_DIM, 256)
                    self.fc_shared2 = nn.Linear(256, 128)
            
                    # --- Actor Hoder ---
                    # 1. Hode for diskret action type
                    self.action_type_head = nn.Linear(128, config.ACTION_DIM)
            
                    # 2. Parameterhoder
                    # Walk 'dx' (logits for diskrete bins)
                    self.walk_dx_head = nn.Linear(128, config.WALK_DX_BINS)
            
                    # Kick: Ingen parameterhoder, da 'force' ikke lenger sendes fra klienten.
            
                    # Bazooka 'angle_deg' (mean og log_std for Normalfordeling)
                    self.bazooka_angle_mean_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
                    self.bazooka_angle_log_std_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
            
                    # Grenade 'dx' (logits for diskrete bins)
                    self.grenade_dx_head = nn.Linear(128, config.GRENADE_DX_BINS)
            
                    # --- Critic Hode ---
                    self.value_head = nn.Linear(128, 1)
            
                def forward(self, map_tensor, worm_vector_tensor):
                    # CNN
                    x_map = F.relu(self.conv1(map_tensor))
                    x_map = F.relu(self.conv2(x_map))
                    x_map = self.pool(x_map)
                    x_map = x_map.view(x_map.size(0), -1)  # Flatten
            
                    # Kombiner med worm data
                    if worm_vector_tensor.dim() == 1:
                        worm_vector_tensor = worm_vector_tensor.unsqueeze(0)
            
                    try:
                        x_combined = torch.cat((x_map, worm_vector_tensor), dim=1)
                    except RuntimeError as e:
                        print(f"FEIL ved torch.cat: map_shape={x_map.shape}, worm_shape={worm_vector_tensor.shape}")
                        print(f"Forventet map flat dim: {config.CNN_FEATURE_DIM}, worm_vector_dim: {config.WORM_VECTOR_DIM}")
                        raise e
            
                    # Felles lag
                    x_shared = F.relu(self.fc_shared1(x_combined))
                    x_shared = F.relu(self.fc_shared2(x_shared))
            
                    # --- Actor Outputs ---
                    action_type_logits = self.action_type_head(x_shared)
                    action_type_probs = F.softmax(action_type_logits, dim=-1)
            
                    # Walk dx (logits for bins)
                    walk_dx_logits = self.walk_dx_head(x_shared)
                    walk_dx_probs = F.softmax(walk_dx_logits, dim=-1)
            
                    # Bazooka angle (mean, std for Normal dist)
                    bazooka_angle_mean = self.bazooka_angle_mean_head(x_shared)
                    bazooka_angle_log_std = self.bazooka_angle_log_std_head(x_shared)
                    bazooka_angle_std = torch.exp(bazooka_angle_log_std.clamp(-20, 2)) # Klem for stabilitet
            
                    # Grenade dx (logits for bins)
                    grenade_dx_logits = self.grenade_dx_head(x_shared)
                    grenade_dx_probs = F.softmax(grenade_dx_logits, dim=-1)
            
                    actor_outputs = {
                        'action_type_probs': action_type_probs,
                        'walk_dx_probs': walk_dx_probs,
                        # Kick har ingen parametere som predikeres av nettverket
                        'bazooka_params': (bazooka_angle_mean, bazooka_angle_std),
                        'grenade_dx_probs': grenade_dx_probs,
                    }
            
                    # --- Critic Output ---
                    state_value = self.value_head(x_shared)
            
                    return actor_outputs, state_value
            --- SLUTT INNHOLD (model.py) ---

    common/
        |-- __init__.py
            [--- INNHOLD: Tom fil ---]

        |-- config.py
            --- START INNHOLD (config.py) ---
            # agents/a2c_manual/config.py
            import torch
            
            # ---- Kartdimensjoner og Normalisering ----
            MAP_WIDTH = 250  # Forventet maks bredde for CNN-padding/cropping
            MAP_HEIGHT = 250 # Forventet maks høyde for CNN-padding/cropping
            # utils.py vil håndtere faktiske kartstørrelser for normalisering.
            
            MAX_WORM_HEALTH = 100.0
            
            # ---- Modell Dimensjoner ----
            CNN_INPUT_CHANNELS = 1
            # Gitt AdaptiveMaxPool2d((6, 6)) og 32 output kanaler fra conv2, blir dette 32*6*6 = 1152
            CNN_FEATURE_DIM = 1152
            
            ACTIVE_WORM_FEATURE_DIM = 3  # Egen orms: health, x, y (normalisert)
            # TODO: Utvid senere til å inkludere info om andre ormer hvis ønskelig
            WORM_VECTOR_DIM = ACTIVE_WORM_FEATURE_DIM
            COMBINED_FEATURE_DIM = CNN_FEATURE_DIM + WORM_VECTOR_DIM
            
            # ---- Action Space Definisjoner (basert på ny json-docs.md) ----
            # Rekkefølgen nettverket ser handlingene i:
            NETWORK_ACTION_ORDER = ['stand', 'walk', 'attack_kick', 'attack_bazooka', 'attack_grenade']
            ACTION_DIM = len(NETWORK_ACTION_ORDER)
            
            # Mapping fra nettverkets actionnavn til serverens JSON-format
            SERVER_ACTION_MAPPING = {
                'stand': {'action': 'stand'},
                'walk': {'action': 'walk'},  # 'dx' parameter legges til dynamisk
                'attack_kick': {'action': 'attack', 'weapon': 'kick'}, # Ingen 'force' fra klient lenger
                'attack_bazooka': {'action': 'attack', 'weapon': 'bazooka'},  # 'angle_deg' parameter
                'attack_grenade': {'action': 'attack', 'weapon': 'grenade'}  # 'dx' parameter (erstatter angle/force)
            }
            
            # ---- Parameter Space for Nettverket ----
            # Walk 'dx'. Server maks er +/-2.0.
            WALK_DX_BINS = 5  # Gir f.eks. bins for [-2.0, -1.0, 0.0, 1.0, 2.0]
            WALK_DX_MIN = -2.0
            WALK_DX_MAX = 2.0
            
            # Kick: Ingen parametere som predikeres av nettverket (ingen 'force' fra klient).
            
            # Bazooka 'angle_deg' (kontinuerlig: nettverket outputer mean og std for Normalfordeling)
            BAZOOKA_ANGLE_PARAMS = 1 # 1 for mean, 1 for std (totalt 2 noder i modellen for dette)
            
            # Grenade 'dx'. Server maks er +/-5.0.
            GRENADE_DX_BINS = 11 # Gir f.eks. bins for [-5.0, -4.0, ..., 0.0, ..., 4.0, 5.0]
            GRENADE_DX_MIN = -5.0
            GRENADE_DX_MAX = 5.0
            
            # ---- A2C Hyperparametre ----
            LEARNING_RATE = 0.0003 # Ofte en god startverdi for A2C
            GAMMA = 0.99  # Discount factor
            ENTROPY_COEF = 0.01
            VALUE_LOSS_COEF = 0.5
            MAX_GRAD_NORM = 0.5 # For gradient clipping
            
            # ---- Trening & Lagring ----
            # Serveren kjører nå kontinuerlig spill. Klienten bør lagre periodisk.
            # NUM_EPISODES i main_coordinator.py styrer hvor mange *spill* hver agent prøver å fullføre
            # før den potensielt avslutter sin egen økt (men serveren fortsetter).
            # For kontinuerlig trening kan man sette NUM_EPISODES veldig høyt eller la den kjøre evig.
            NUM_GAMES_PER_AGENT_SESSION = 10000 # Hvor mange spill en agent-instans skal sikte mot
            
            SAVE_MODEL_EVERY_N_GAMES = 50   # Lagre modell etter X antall *fullførte spill* for denne agenten
            PLOT_STATS_EVERY_N_GAMES = 10   # Hvor ofte generere/oppdatere plot
            
            # ---- Websocket ----
            SERVER_HOST = '127.0.0.1'
            SERVER_PORT = 8765
            
            # ---- Annet ----
            DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
            # DEVICE = "cpu" # Kan overstyres for debugging
            print(f"Bruker enhet: {DEVICE}")
            --- SLUTT INNHOLD (config.py) ---

        |-- utils.py
            --- START INNHOLD (utils.py) ---
            # agents/a2c_manual/utils.py
            import torch
            import numpy as np
            from agents.common import config
            
            
            def preprocess_state(current_game_state_json, agent_player_id):
                """
                Konverterer game_state JSON (fra msg['state']) til tensorer klar for nettverket.
                Returnerer en tuple: (map_tensor, worm_vector_tensor)
                """
                try:
                    map_data = current_game_state_json['map']
                    worms_data = current_game_state_json['worms']
            
                    # --- Kart Preprocessing ---
                    map_array = np.array(map_data, dtype=np.float32)
                    actual_map_h, actual_map_w = map_array.shape
            
                    # Senter-justert padding/cropping til config.MAP_HEIGHT, config.MAP_WIDTH
                    processed_map_array = np.zeros((config.MAP_HEIGHT, config.MAP_WIDTH), dtype=np.float32)
            
                    copy_h_len = min(actual_map_h, config.MAP_HEIGHT)
                    src_start_h = max(0, (actual_map_h - copy_h_len) // 2)
                    dst_start_h = max(0, (config.MAP_HEIGHT - copy_h_len) // 2)
            
                    copy_w_len = min(actual_map_w, config.MAP_WIDTH)
                    src_start_w = max(0, (actual_map_w - copy_w_len) // 2)
                    dst_start_w = max(0, (config.MAP_WIDTH - copy_w_len) // 2)
            
                    processed_map_array[dst_start_h: dst_start_h + copy_h_len,
                    dst_start_w: dst_start_w + copy_w_len] = \
                        map_array[src_start_h: src_start_h + copy_h_len,
                        src_start_w: src_start_w + copy_w_len]
            
                    map_tensor = torch.from_numpy(processed_map_array).unsqueeze(0).unsqueeze(0).to(config.DEVICE)
            
                    # --- Worm Data Preprocessing ---
                    active_worm_features = [0.0] * config.ACTIVE_WORM_FEATURE_DIM
                    my_worm_json_id = agent_player_id - 1  # Server player_id er 1-basert, JSON orm id er 0-basert
            
                    my_worm_alive = False
                    for worm_info in worms_data:
                        if worm_info['id'] == my_worm_json_id:
                            if worm_info['health'] > 0:
                                norm_health = worm_info['health'] / config.MAX_WORM_HEALTH
                                # Normaliser x og y basert på *faktiske* kartdimensjoner for korrekthet
                                norm_x = np.clip(worm_info['x'] / float(actual_map_w - 1 if actual_map_w > 1 else 1.0), 0.0, 1.0)
                                norm_y = np.clip(worm_info['y'] / float(actual_map_h - 1 if actual_map_h > 1 else 1.0), 0.0, 1.0)
            
                                active_worm_features = [np.clip(norm_health, 0, 1), norm_x, norm_y]
                                my_worm_alive = True
                            break  # Funnet vår orm (død eller levende), ikke nødvendig å lete mer
            
                    # if not my_worm_alive:
                    #     print(f"Advarsel preprocess_state: Fant ikke levende orm data for P{agent_player_id} (JSON id {my_worm_json_id}). Bruker null-vektor.")
            
                    worm_vector_tensor = torch.FloatTensor([active_worm_features]).to(config.DEVICE)
                    return map_tensor, worm_vector_tensor
            
                except KeyError as e:
                    print(
                        f"FEIL preprocess_state: Manglende nøkkel '{e}' i game_state_json: {str(current_game_state_json)[:200]}...")
                    # Returner dummy-tensorer for å unngå krasj, men dette indikerer et problem.
                    dummy_map = torch.zeros(1, config.CNN_INPUT_CHANNELS, config.MAP_HEIGHT, config.MAP_WIDTH).to(config.DEVICE)
                    dummy_worm = torch.zeros(1, config.WORM_VECTOR_DIM).to(config.DEVICE)
                    return dummy_map, dummy_worm
                except Exception as e:
                    print(f"Uventet FEIL preprocess_state: {type(e).__name__} - {e} data: {str(current_game_state_json)[:200]}...")
                    dummy_map = torch.zeros(1, config.CNN_INPUT_CHANNELS, config.MAP_HEIGHT, config.MAP_WIDTH).to(config.DEVICE)
                    dummy_worm = torch.zeros(1, config.WORM_VECTOR_DIM).to(config.DEVICE)
                    return dummy_map, dummy_worm
            
            
            def _convert_bin_to_value(bin_idx, num_bins, min_val, max_val):
                """ Hjelpefunksjon for å konvertere en bin-indeks til en faktisk verdi. """
                if num_bins <= 1:  # Unngå divisjon med null hvis num_bins er 0 eller 1
                    return (min_val + max_val) / 2.0
                value = min_val + bin_idx * ((max_val - min_val) / float(num_bins - 1))
                return np.clip(value, min_val, max_val)
            
            
            def format_action(network_action_idx, params_from_network):
                """
                Formaterer valgt handling og parametere til JSON-formatet serveren forventer.
                """
                network_action_name = config.NETWORK_ACTION_ORDER[network_action_idx]
            
                if network_action_name not in config.SERVER_ACTION_MAPPING:
                    print(f"FEIL format_action: Ukjent nettverkshandling '{network_action_name}'. Sender 'stand'.")
                    return {"action": "stand"}
            
                action_json = config.SERVER_ACTION_MAPPING[network_action_name].copy()
            
                if network_action_name == 'walk':
                    dx_bin_idx = params_from_network.get('walk_dx_bin_idx', config.WALK_DX_BINS // 2)
                    dx_value = _convert_bin_to_value(dx_bin_idx, config.WALK_DX_BINS, config.WALK_DX_MIN, config.WALK_DX_MAX)
                    action_json['dx'] = float(np.clip(dx_value, -2.0, 2.0))  # Server klipper til +/-2.0
            
                elif network_action_name == 'attack_kick':
                    # Ingen 'force' parameter fra klienten lenger
                    pass
            
                elif network_action_name == 'attack_bazooka':
                    angle_val = params_from_network.get('bazooka_angle_val', 0.0)
                    action_json['angle_deg'] = float(angle_val)
                    # Ingen 'force' for bazooka
            
                elif network_action_name == 'attack_grenade':
                    # Bruker nå 'dx' for grenade
                    dx_bin_idx = params_from_network.get('grenade_dx_bin_idx', config.GRENADE_DX_BINS // 2)
                    dx_value = _convert_bin_to_value(dx_bin_idx, config.GRENADE_DX_BINS, config.GRENADE_DX_MIN,
                                                     config.GRENADE_DX_MAX)
                    action_json['dx'] = float(np.clip(dx_value, -5.0, 5.0))  # Server klipper til +/-5.0
            
                return action_json
            --- SLUTT INNHOLD (utils.py) ---

    ppo_manual/
        |-- agent.py
            --- START INNHOLD (agent.py) ---
            # agents/ppo_manual/agent.py
            import torch
            import torch.optim as optim
            import torch.nn.functional as F
            from torch.distributions import Categorical, Normal
            import numpy as np
            from pathlib import Path
            
            from agents.common import config  # Bruker felles config
            # Endre denne linjen:
            # from .model import ActorCriticNetwork
            # til:
            from agents.ppo_manual.model import ActorCriticNetwork  # Bruker PPO sin modellfil
            from agents.common.utils import preprocess_state, format_action  # Bruker felles utils
            
            
            class PPOAgent:
                def __init__(self, agent_name="PPO_Agent_Default"):
                    self.network = ActorCriticNetwork().to(config.DEVICE)
                    self.optimizer = optim.Adam(self.network.parameters(), lr=config.LEARNING_RATE_PPO if hasattr(config,
                                                                                                                  'LEARNING_RATE_PPO') else config.LEARNING_RATE)
                    self.player_id = None
                    self.agent_name = agent_name
            
                    # Buffere for PPO - samler en hel batch før oppdatering
                    self.map_pixel_buffer = []
                    self.worm_vector_buffer = []
            
                    self.action_indices_buffer = []
                    self.action_params_raw_buffer = []
            
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.dones_buffer = []
                    self.entropies_buffer = []
            
                    self.batch_count = 0
            
                def set_player_id(self, player_id: int):
                    self.player_id = player_id
            
                def select_action(self, current_game_state_json: dict):
                    if self.player_id is None:
                        return {"action": "stand"}
            
                    my_worm_json_id = self.player_id - 1
                    is_my_worm_alive = any(
                        w['id'] == my_worm_json_id and w['health'] > 0
                        for w in current_game_state_json.get('worms', [])
                    )
                    if not is_my_worm_alive:
                        self.map_pixel_buffer.append(
                            torch.zeros(1, config.CNN_INPUT_CHANNELS, config.MAP_HEIGHT, config.MAP_WIDTH).to(config.DEVICE))
                        self.worm_vector_buffer.append(torch.zeros(1, config.WORM_VECTOR_DIM).to(config.DEVICE))
                        self.values_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        self.log_probs_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        self.entropies_buffer.append(torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32))
                        self.action_indices_buffer.append(-1)
                        self.action_params_raw_buffer.append({})
                        return {"action": "stand"}
            
                    map_tensor, worm_vector_tensor = preprocess_state(current_game_state_json, self.player_id)
                    self.map_pixel_buffer.append(map_tensor)
                    self.worm_vector_buffer.append(worm_vector_tensor)
            
                    with torch.no_grad():
                        actor_outputs, state_value_tensor = self.network(map_tensor, worm_vector_tensor)
            
                    self.values_buffer.append(state_value_tensor.squeeze())
            
                    action_type_probs = actor_outputs['action_type_probs']
                    action_type_dist = Categorical(action_type_probs)
                    network_action_idx_tensor = action_type_dist.sample()
                    network_action_idx_item = network_action_idx_tensor.item()
                    self.action_indices_buffer.append(network_action_idx_item)
            
                    log_prob_action_type = action_type_dist.log_prob(network_action_idx_tensor)
                    entropy_action_type = action_type_dist.entropy()
            
                    step_log_probs = [log_prob_action_type]
                    step_entropies = [entropy_action_type]
                    params_for_formatting = {}
                    raw_params_for_buffer = {}
                    chosen_network_action_name = config.NETWORK_ACTION_ORDER[network_action_idx_item]
            
                    if chosen_network_action_name == 'walk':
                        walk_dx_probs = actor_outputs['walk_dx_probs']
                        walk_dx_dist = Categorical(walk_dx_probs)
                        walk_dx_bin_idx_tensor = walk_dx_dist.sample()
                        params_for_formatting['walk_dx_bin_idx'] = walk_dx_bin_idx_tensor.item()
                        raw_params_for_buffer['walk_dx_bin_idx'] = walk_dx_bin_idx_tensor
                        step_log_probs.append(walk_dx_dist.log_prob(walk_dx_bin_idx_tensor))
                        step_entropies.append(walk_dx_dist.entropy())
            
                    elif chosen_network_action_name == 'attack_kick':
                        pass
            
                    elif chosen_network_action_name == 'attack_bazooka':
                        angle_mean, angle_std = actor_outputs['bazooka_params']
                        # Sørg for at mean og std er skalarer for Normal-distribusjonen hvis de kommer ut med batch-dim
                        current_angle_mean = angle_mean.squeeze() if angle_mean.ndim > 0 else angle_mean
                        current_angle_std = angle_std.squeeze() if angle_std.ndim > 0 else angle_std
            
                        dist = Normal(current_angle_mean, current_angle_std)
                        angle_val_tensor = dist.sample()
                        params_for_formatting['bazooka_angle_val'] = angle_val_tensor.item()
                        raw_params_for_buffer['bazooka_angle_val'] = angle_val_tensor
                        step_log_probs.append(dist.log_prob(angle_val_tensor))
                        step_entropies.append(dist.entropy())
            
                    elif chosen_network_action_name == 'attack_grenade':
                        grenade_dx_probs = actor_outputs['grenade_dx_probs']
                        grenade_dx_dist = Categorical(grenade_dx_probs)
                        grenade_dx_bin_idx_tensor = grenade_dx_dist.sample()
                        params_for_formatting['grenade_dx_bin_idx'] = grenade_dx_bin_idx_tensor.item()
                        raw_params_for_buffer['grenade_dx_bin_idx'] = grenade_dx_bin_idx_tensor
                        step_log_probs.append(grenade_dx_dist.log_prob(grenade_dx_bin_idx_tensor))
                        step_entropies.append(grenade_dx_dist.entropy())
            
                    self.action_params_raw_buffer.append(raw_params_for_buffer)
            
                    # Sikre at alle er skalar-tensorer før stack/sum
                    squeezed_log_probs = [lp.squeeze() for lp in step_log_probs]
                    squeezed_entropies = [e.squeeze() for e in step_entropies]
            
                    stacked_log_probs = torch.stack(squeezed_log_probs)
                    self.log_probs_buffer.append(stacked_log_probs.sum())
            
                    stacked_entropies = torch.stack(squeezed_entropies)
                    self.entropies_buffer.append(stacked_entropies.sum())
            
                    action_json_to_send = format_action(network_action_idx_item, params_for_formatting)
                    # self.actions_buffer.append(action_json_to_send) # Fjernet, da den ikke brukes i PPO loss
                    return action_json_to_send
            
                def store_reward_and_done(self, reward: float, done: bool):
                    self.rewards_buffer.append(reward)
                    self.dones_buffer.append(done)
                    self.batch_count += 1
            
                def _compute_gae_and_returns(self, next_value_tensor_no_grad):
                    rewards_np = np.array(self.rewards_buffer, dtype=np.float32)
                    values_np = torch.stack(self.values_buffer).cpu().numpy()
                    dones_np = np.array(self.dones_buffer, dtype=np.float32)
            
                    advantages = np.zeros_like(rewards_np)
                    last_gae_lam = 0
                    num_steps = len(rewards_np)
            
                    for t in reversed(range(num_steps)):
                        if t == num_steps - 1:
                            next_non_terminal = 1.0 - dones_np[t]
                            next_val = next_value_tensor_no_grad.cpu().item()
                        else:
                            next_non_terminal = 1.0 - dones_np[t]  # Endret fra t+1 til t for dones_np
                            next_val = values_np[t + 1]
            
                        delta = rewards_np[t] + config.GAMMA_PPO * next_val * next_non_terminal - values_np[t]
                        advantages[
                            t] = last_gae_lam = delta + config.GAMMA_PPO * config.GAE_LAMBDA_PPO * next_non_terminal * last_gae_lam
            
                    returns_np = advantages + values_np
                    return torch.tensor(advantages, device=config.DEVICE, dtype=torch.float32), torch.tensor(returns_np,
                                                                                                             device=config.DEVICE,
                                                                                                             dtype=torch.float32)
            
                def learn(self, next_game_state_json: dict | None):
                    if self.batch_count < (config.PPO_BATCH_SIZE if hasattr(config, 'PPO_BATCH_SIZE') else 128):
                        return None, None, None
            
                    R_bootstrap_next_state = torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32)
                    if not self.dones_buffer[-1]:  # Bare bootstrap hvis siste steg IKKE var terminalt
                        if next_game_state_json and self.player_id is not None:
                            my_worm_json_id = self.player_id - 1
                            is_my_worm_alive_in_next_state = any(
                                w['id'] == my_worm_json_id and w['health'] > 0
                                for w in next_game_state_json.get('worms', [])
                            )
                            if is_my_worm_alive_in_next_state:
                                if isinstance(next_game_state_json, dict) and \
                                        next_game_state_json.get("map") and next_game_state_json.get("worms"):
                                    next_map, next_worm = preprocess_state(next_game_state_json, self.player_id)
                                    with torch.no_grad():
                                        _, R_bootstrap_next_state_tensor = self.network(next_map, next_worm)
                                        R_bootstrap_next_state = R_bootstrap_next_state_tensor.squeeze()
            
                    advantages, returns = self._compute_gae_and_returns(R_bootstrap_next_state)
                    old_log_probs_tensor = torch.stack(self.log_probs_buffer).detach()
            
                    map_pixel_batch = torch.cat(self.map_pixel_buffer, dim=0)
                    worm_vector_batch = torch.cat(self.worm_vector_buffer, dim=0)
            
                    total_policy_loss_epoch_avg = 0
                    total_value_loss_epoch_avg = 0
                    total_entropy_loss_epoch_avg = 0
            
                    num_samples_in_batch = len(self.rewards_buffer)
                    batch_indices = np.arange(num_samples_in_batch)
            
                    for _ in range(config.PPO_EPOCHS if hasattr(config, 'PPO_EPOCHS') else 4):
                        np.random.shuffle(batch_indices)
            
                        # For PPO, vi itererer over hele batchen (eller minibatches av den)
                        # Her tar vi hele batchen for enkelhets skyld
                        shuffled_maps = map_pixel_batch[batch_indices]
                        shuffled_worms = worm_vector_batch[batch_indices]
            
                        actor_outputs_new, values_new_tensor = self.network(shuffled_maps, shuffled_worms)
                        shuffled_values_new = values_new_tensor.squeeze()  # Blir (batch_size)
            
                        new_log_probs_list_for_epoch = []
                        new_entropies_list_for_epoch = []
            
                        # Gå gjennom de shufflet indeksene for å hente de tilsvarende lagrede handlingene/parametrene
                        for original_idx in batch_indices:  # original_idx referer til posisjonen i de ushufflede bufferne
                            action_type_idx_orig = self.action_indices_buffer[original_idx]
            
                            if action_type_idx_orig == -1:  # Ormen var død
                                new_log_probs_list_for_epoch.append(torch.tensor(0.0, device=config.DEVICE))
                                new_entropies_list_for_epoch.append(torch.tensor(0.0, device=config.DEVICE))
                                continue
            
                            # Finn korresponderende output fra nettverket for dette samplet
                            # batch_indices er shufflet, så vi må finne hvor original_idx havnet i den shufflet rekkefølgen
                            current_sample_shuffled_idx = np.where(batch_indices == original_idx)[0][0]
            
                            current_actor_output_for_sample = {
                                'action_type_probs': actor_outputs_new['action_type_probs'][current_sample_shuffled_idx],
                                'walk_dx_probs': actor_outputs_new['walk_dx_probs'][
                                    current_sample_shuffled_idx] if 'walk_dx_probs' in actor_outputs_new else None,
                                'bazooka_params': (actor_outputs_new['bazooka_params'][0][current_sample_shuffled_idx],
                                                   actor_outputs_new['bazooka_params'][1][current_sample_shuffled_idx]) if
                                actor_outputs_new['bazooka_params'] and actor_outputs_new['bazooka_params'][0] is not None else (
                                None, None),
                                'grenade_dx_probs': actor_outputs_new['grenade_dx_probs'][
                                    current_sample_shuffled_idx] if 'grenade_dx_probs' in actor_outputs_new else None,
                            }
            
                            dist_action_type = Categorical(current_actor_output_for_sample['action_type_probs'])
                            log_p_action_type = dist_action_type.log_prob(torch.tensor(action_type_idx_orig, device=config.DEVICE))
                            entropy_action_type = dist_action_type.entropy()
            
                            current_log_probs_parts = [log_p_action_type]
                            current_entropies_parts = [entropy_action_type]
            
                            action_name = config.NETWORK_ACTION_ORDER[action_type_idx_orig]
                            raw_params_this_step_orig = self.action_params_raw_buffer[original_idx]
            
                            if action_name == 'walk':
                                dist_walk = Categorical(current_actor_output_for_sample['walk_dx_probs'])
                                log_p_walk = dist_walk.log_prob(raw_params_this_step_orig['walk_dx_bin_idx'])
                                entropy_walk = dist_walk.entropy()
                                current_log_probs_parts.append(log_p_walk)
                                current_entropies_parts.append(entropy_walk)
                            elif action_name == 'attack_bazooka':
                                mean, std = current_actor_output_for_sample['bazooka_params']
                                dist_bzk = Normal(mean, std)  # Sørg for at mean/std er skalarer her
                                log_p_bzk = dist_bzk.log_prob(raw_params_this_step_orig['bazooka_angle_val'])
                                entropy_bzk = dist_bzk.entropy()
                                current_log_probs_parts.append(log_p_bzk)
                                current_entropies_parts.append(entropy_bzk)
                            elif action_name == 'attack_grenade':
                                dist_grenade = Categorical(current_actor_output_for_sample['grenade_dx_probs'])
                                log_p_grenade = dist_grenade.log_prob(raw_params_this_step_orig['grenade_dx_bin_idx'])
                                entropy_grenade = dist_grenade.entropy()
                                current_log_probs_parts.append(log_p_grenade)
                                current_entropies_parts.append(entropy_grenade)
            
                            new_log_probs_list_for_epoch.append(torch.stack(current_log_probs_parts).sum())
                            new_entropies_list_for_epoch.append(torch.stack(current_entropies_parts).sum())
            
                        # active_mask er basert på *opprinnelig* rekkefølge
                        active_mask_orig = torch.tensor([idx != -1 for idx in self.action_indices_buffer], device=config.DEVICE,
                                                        dtype=torch.bool)
                        # shufflet_active_mask er basert på *shufflet* rekkefølge
                        shuffled_active_mask = active_mask_orig[batch_indices]
            
                        if not torch.any(shuffled_active_mask):
                            # Ingen aktive samples i denne (teoretisk mulig hvis alle døde i batchen)
                            continue
            
                            # Tensorer for aktive, shufflet samples
                        final_new_log_probs = torch.stack(new_log_probs_list_for_epoch)[shuffled_active_mask]
                        final_new_entropies = torch.stack(new_entropies_list_for_epoch)[shuffled_active_mask]
                        final_shuffled_values_new = shuffled_values_new[shuffled_active_mask]
            
                        final_shuffled_old_log_probs = old_log_probs_tensor[batch_indices][shuffled_active_mask]
                        final_shuffled_advantages = advantages[batch_indices][shuffled_active_mask]
                        final_shuffled_returns = returns[batch_indices][shuffled_active_mask]
            
                        if final_shuffled_old_log_probs.nelement() == 0:  # Dobbeltsjekk
                            continue
            
                        ratios = torch.exp(final_new_log_probs - final_shuffled_old_log_probs)
            
                        surr1 = ratios * final_shuffled_advantages
                        surr2 = torch.clamp(ratios, 1 - config.PPO_CLIP_EPSILON,
                                            1 + config.PPO_CLIP_EPSILON) * final_shuffled_advantages
            
                        policy_loss = -torch.min(surr1, surr2).mean()
                        value_loss = F.mse_loss(final_shuffled_values_new, final_shuffled_returns)
                        entropy_loss = -final_new_entropies.mean()
            
                        current_total_loss = policy_loss + \
                                             (config.VALUE_LOSS_COEF_PPO if hasattr(config,
                                                                                    'VALUE_LOSS_COEF_PPO') else config.VALUE_LOSS_COEF) * value_loss + \
                                             (config.ENTROPY_COEF_PPO if hasattr(config,
                                                                                 'ENTROPY_COEF_PPO') else config.ENTROPY_COEF) * entropy_loss
            
                        self.optimizer.zero_grad()
                        current_total_loss.backward()
                        grad_norm_key = 'MAX_GRAD_NORM_PPO' if hasattr(config, 'MAX_GRAD_NORM_PPO') else 'MAX_GRAD_NORM'
                        if hasattr(config, grad_norm_key) and getattr(config, grad_norm_key) is not None:
                            torch.nn.utils.clip_grad_norm_(self.network.parameters(), getattr(config, grad_norm_key))
                        self.optimizer.step()
            
                        total_policy_loss_epoch_avg += policy_loss.item()
                        total_value_loss_epoch_avg += value_loss.item()
                        total_entropy_loss_epoch_avg += entropy_loss.item()
            
                    ppo_epochs_val = config.PPO_EPOCHS if hasattr(config, 'PPO_EPOCHS') else 4
                    avg_policy_loss = total_policy_loss_epoch_avg / ppo_epochs_val
                    avg_value_loss = total_value_loss_epoch_avg / ppo_epochs_val
                    avg_entropy_loss = total_entropy_loss_epoch_avg / ppo_epochs_val
            
                    self.clear_buffers_and_count()
                    return avg_policy_loss, avg_value_loss, avg_entropy_loss
            
                def clear_buffers_and_count(self):
                    self.map_pixel_buffer = []
                    self.worm_vector_buffer = []
                    self.action_indices_buffer = []
                    self.action_params_raw_buffer = []
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.dones_buffer = []
                    self.entropies_buffer = []
                    self.batch_count = 0
            
                def save_model(self, path_str: str):
                    path = Path(path_str)
                    path.parent.mkdir(parents=True, exist_ok=True)
                    try:
                        torch.save(self.network.state_dict(), path)
                    except Exception as e:
                        print(f"[{self.agent_name}] Kunne ikke lagre PPO-modell til {path}: {e}")
            
                def load_model(self, path_str: str):
                    path = Path(path_str)
                    if not path.exists():
                        print(f"[{self.agent_name}] Ingen PPO-modell funnet på {path}, starter med ny.")
                        return
                    try:
                        self.network.load_state_dict(torch.load(path, map_location=torch.device(config.DEVICE)))
                        self.network.eval()
                        print(f"[{self.agent_name}] PPO-Modell lastet fra {path}")
                    except Exception as e:
                        print(f"[{self.agent_name}] Kunne ikke laste PPO-modell fra {path}: {e}. Bruker ny.")
            --- SLUTT INNHOLD (agent.py) ---

        |-- config.py
            --- START INNHOLD (config.py) ---
            # agents/common/config.py
            import torch
            
            # ---- Kartdimensjoner og Normalisering (Felles) ----
            MAP_WIDTH = 250
            MAP_HEIGHT = 250
            MAX_WORM_HEALTH = 100.0
            
            # ---- Modell Dimensjoner (Felles) ----
            CNN_INPUT_CHANNELS = 1
            CNN_FEATURE_DIM = 1152 # 32*6*6
            ACTIVE_WORM_FEATURE_DIM = 3
            WORM_VECTOR_DIM = ACTIVE_WORM_FEATURE_DIM
            COMBINED_FEATURE_DIM = CNN_FEATURE_DIM + WORM_VECTOR_DIM
            
            # ---- Action Space Definisjoner (Felles) ----
            NETWORK_ACTION_ORDER = ['stand', 'walk', 'attack_kick', 'attack_bazooka', 'attack_grenade']
            ACTION_DIM = len(NETWORK_ACTION_ORDER)
            SERVER_ACTION_MAPPING = {
                'stand': {'action': 'stand'},
                'walk': {'action': 'walk'},
                'attack_kick': {'action': 'attack', 'weapon': 'kick'},
                'attack_bazooka': {'action': 'attack', 'weapon': 'bazooka'},
                'attack_grenade': {'action': 'attack', 'weapon': 'grenade'}
            }
            
            # ---- Parameter Space for Nettverket (Felles) ----
            WALK_DX_BINS = 5
            WALK_DX_MIN = -2.0
            WALK_DX_MAX = 2.0
            BAZOOKA_ANGLE_PARAMS = 1 # For mean (std beregnes også, så 2 output noder per)
            GRENADE_DX_BINS = 11
            GRENADE_DX_MIN = -5.0
            GRENADE_DX_MAX = 5.0
            
            # ---- A2C Hyperparametre ----
            LEARNING_RATE = 0.0003 # For A2C
            GAMMA = 0.99  # Felles discount factor
            ENTROPY_COEF = 0.01 # Felles
            VALUE_LOSS_COEF = 0.5 # Felles
            MAX_GRAD_NORM = 0.5 # Felles for A2C
            
            # ---- PPO Hyperparametre ----
            LEARNING_RATE_PPO = 0.0003 # Kan være lik A2C, eller justeres
            GAMMA_PPO = 0.99 # Ofte lik vanlig gamma
            ENTROPY_COEF_PPO = 0.01
            VALUE_LOSS_COEF_PPO = 0.5
            MAX_GRAD_NORM_PPO = 0.5
            PPO_EPOCHS = 4             # Antall ganger å iterere over batchen
            PPO_BATCH_SIZE = 128       # Antall steg å samle før PPO-oppdatering
            PPO_CLIP_EPSILON = 0.2     # PPO clipping parameter
            GAE_LAMBDA_PPO = 0.95      # Lambda for Generalized Advantage Estimation
            
            # ---- Trening & Lagring (Felles) ----
            NUM_GAMES_PER_AGENT_SESSION = 10000
            SAVE_MODEL_EVERY_N_GAMES = 50
            PLOT_STATS_EVERY_N_GAMES = 10
            
            # ---- Websocket (Felles) ----
            SERVER_HOST = '127.0.0.1'
            SERVER_PORT = 8765
            
            # ---- Annet (Felles) ----
            DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Bruker enhet: {DEVICE}")
            --- SLUTT INNHOLD (config.py) ---

        |-- model.py
            --- START INNHOLD (model.py) ---
            # agents/ppo_manual/model.py
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            # Endre denne linjen:
            # from . import config
            # til:
            from agents.common import config
            
            
            class ActorCriticNetwork(nn.Module):
                def __init__(self):
                    super(ActorCriticNetwork, self).__init__()
            
                    # --- CNN for Map Processing ---
                    self.conv1 = nn.Conv2d(config.CNN_INPUT_CHANNELS, 16, kernel_size=8, stride=4)
                    self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)
                    self.pool = nn.AdaptiveMaxPool2d((6, 6))
                    # CNN_FEATURE_DIM forblir 32*6*6 = 1152
            
                    # --- Felles Fullt Tilkoblede Lag ---
                    self.fc_shared1 = nn.Linear(config.CNN_FEATURE_DIM + config.WORM_VECTOR_DIM, 256)
                    self.fc_shared2 = nn.Linear(256, 128)
            
                    # --- Actor Hoder ---
                    # 1. Hode for diskret action type
                    self.action_type_head = nn.Linear(128, config.ACTION_DIM)
            
                    # 2. Parameterhoder
                    # Walk 'dx' (logits for diskrete bins)
                    self.walk_dx_head = nn.Linear(128, config.WALK_DX_BINS)
            
                    # Kick: Ingen parameterhoder, da 'force' ikke lenger sendes fra klienten.
            
                    # Bazooka 'angle_deg' (mean og log_std for Normalfordeling)
                    self.bazooka_angle_mean_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
                    self.bazooka_angle_log_std_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
            
                    # Grenade 'dx' (logits for diskrete bins)
                    self.grenade_dx_head = nn.Linear(128, config.GRENADE_DX_BINS)
            
                    # --- Critic Hode ---
                    self.value_head = nn.Linear(128, 1)
            
                def forward(self, map_tensor, worm_vector_tensor):
                    # CNN
                    x_map = F.relu(self.conv1(map_tensor))
                    x_map = F.relu(self.conv2(x_map))
                    x_map = self.pool(x_map)
                    x_map = x_map.view(x_map.size(0), -1)  # Flatten
            
                    # Kombiner med worm data
                    if worm_vector_tensor.dim() == 1:
                        worm_vector_tensor = worm_vector_tensor.unsqueeze(0)
            
                    try:
                        x_combined = torch.cat((x_map, worm_vector_tensor), dim=1)
                    except RuntimeError as e:
                        print(f"FEIL ved torch.cat: map_shape={x_map.shape}, worm_shape={worm_vector_tensor.shape}")
                        print(f"Forventet map flat dim: {config.CNN_FEATURE_DIM}, worm_vector_dim: {config.WORM_VECTOR_DIM}")
                        raise e
            
                    # Felles lag
                    x_shared = F.relu(self.fc_shared1(x_combined))
                    x_shared = F.relu(self.fc_shared2(x_shared))
            
                    # --- Actor Outputs ---
                    action_type_logits = self.action_type_head(x_shared)
                    action_type_probs = F.softmax(action_type_logits, dim=-1)
            
                    # Walk dx (logits for bins)
                    walk_dx_logits = self.walk_dx_head(x_shared)
                    walk_dx_probs = F.softmax(walk_dx_logits, dim=-1)
            
                    # Bazooka angle (mean, std for Normal dist)
                    bazooka_angle_mean = self.bazooka_angle_mean_head(x_shared)
                    bazooka_angle_log_std = self.bazooka_angle_log_std_head(x_shared)
                    bazooka_angle_std = torch.exp(bazooka_angle_log_std.clamp(-20, 2)) # Klem for stabilitet
            
                    # Grenade dx (logits for bins)
                    grenade_dx_logits = self.grenade_dx_head(x_shared)
                    grenade_dx_probs = F.softmax(grenade_dx_logits, dim=-1)
            
                    actor_outputs = {
                        'action_type_probs': action_type_probs,
                        'walk_dx_probs': walk_dx_probs,
                        # Kick har ingen parametere som predikeres av nettverket
                        'bazooka_params': (bazooka_angle_mean, bazooka_angle_std),
                        'grenade_dx_probs': grenade_dx_probs,
                    }
            
                    # --- Critic Output ---
                    state_value = self.value_head(x_shared)
            
                    return actor_outputs, state_value
            --- SLUTT INNHOLD (model.py) ---

    training_plots_output/
environment/
    |-- game_core.py
        --- START INNHOLD (game_core.py) ---
        # File: environment/game_core.py
        
        import copy
        import json
        import logging
        import math
        import random
        from pathlib import Path
        from typing import Any, Dict, List, Tuple
        
        logger = logging.getLogger(__name__)
        
        # tweakable constants
        WALK_MAX_SPEED = 2.0
        
        KICK_RANGE = 1.0
        KICK_DAMAGE = 80.0
        
        BAZOOKA_STEP_SIZE = 0.1
        BAZOOKA_MAX_RANGE = 10.0
        BAZOOKA_DAMAGE = 60.0
        BAZOOKA_HIT_RADIUS = 0.5
        
        GRENADE_MAX_THROW = 5.0
        GRENADE_STEP_SIZE = 0.1
        GRENADE_DAMAGE = 30.0
        GRENADE_HIT_RADIUS = 0.5
        
        class GameCore:
            def __init__(self, expected_players: int = 2) -> None:
                self.expected = expected_players
                self.map = self._load_random_map()
                self.state: Dict[str, Any] = self.initial_state()
        
            def _load_random_map(self) -> List[List[int]]:
                maps_dir = Path(__file__).resolve().parent / "maps"
                files = list(maps_dir.glob("*.json"))
                if not files:
                    raise FileNotFoundError(f"No map files found in {maps_dir!r}")
                choice = random.choice(files)
                with open(choice, "r") as f:
                    data = json.load(f)
                width = len(data[0])
                if any(len(row) != width for row in data):
                    raise ValueError(f"Map {choice.name} is not rectangular")
                return data  # type: ignore
        
            def expected_players(self) -> int:
                return self.expected
        
            def initial_state(self) -> Dict[str, Any]:
                floor = []
                rows = len(self.map)
                cols = len(self.map[0])
                for r in range(1, rows):
                    for c in range(cols):
                        if self.map[r][c] == 1 and self.map[r - 1][c] == 0:
                            floor.append((r, c))
        
                if len(floor) < self.expected:
                    raise ValueError(f"Not enough floor tiles ({len(floor)}) for {self.expected} players")
        
                chosen = random.sample(floor, self.expected)
                worms = []
                for pid, (r, c) in enumerate(chosen):
                    x = c + 0.5
                    y = float(r) - 0.25
                    worms.append({"id": pid, "health": 100, "x": x, "y": y})
        
                return {"worms": worms, "map": copy.deepcopy(self.map)}
        
            def step(
                self, player_id: int, action: Dict[str, Any]
            ) -> Tuple[Dict[str, Any], float, Dict[str, Any]]:
                state = self.state
                worms = state["worms"]
                idx = player_id - 1
                worm = worms[idx]
                effects: Dict[str, Any] = {}
        
                if action.get("action") == "walk":
                    dx = float(action.get("dx", 0.0))
                    dx = max(-WALK_MAX_SPEED, min(dx, WALK_MAX_SPEED))
                    new_x = worm["x"] + dx
                    worm["x"] = max(0.0, min(new_x, len(self.map[0]) - 0.01))
                    col = int(math.floor(worm["x"]))
                    height = len(self.map)
                    OFFSET = 0.25
                    row_here = int(math.floor(worm["y"]))
                    if 0 <= row_here < height and self.map[row_here][col] == 1:
                        worm["y"] = float(row_here) - OFFSET
                    else:
                        for row in range(row_here + 1, height):
                            if self.map[row][col] == 1:
                                worm["y"] = float(row) - OFFSET
                                break
                        else:
                            worm["y"] = float(height)
                            worm["health"] = 0
                    return copy.deepcopy(state), 0.0, effects
        
                if action.get("action") == "attack":
                    damage_total = 0.0
                    kill_bonus = 0.0
                    weapon = action.get("weapon")
                    effects = {"weapon": weapon, "trajectory": []}
        
                    # ─── Kick ──────────────────────────────────────────────────────────
                    if weapon == "kick":
                        for other in worms:
                            if other["id"] == worm["id"] or other["health"] <= 0:
                                continue
                            dist = math.hypot(other["x"] - worm["x"], other["y"] - worm["y"])
                            if dist <= KICK_RANGE:
                                old_hp = other["health"]
                                new_hp = max(0.0, old_hp - KICK_DAMAGE)
                                actual_damage = old_hp - new_hp
                                other["health"] = new_hp
                                damage_total += actual_damage
                                if new_hp == 0.0:
                                    kill_bonus += 100.0
                                effects["impact"] = {"x": other["x"], "y": other["y"]}
                                break
                        return copy.deepcopy(state), damage_total + kill_bonus, effects
        
                    # ─── Bazooka ───────────────────────────────────────────────────────
                    elif weapon == "bazooka":
                        angle = math.radians(float(action.get("angle_deg", 0.0)))
                        dx_unit, dy_unit = math.cos(angle), -math.sin(angle)
                        x0, y0 = worm["x"], worm["y"]
                        t = BAZOOKA_STEP_SIZE
                        effects["impact"] = {}
                        while t <= BAZOOKA_MAX_RANGE:
                            x_proj = x0 + dx_unit * t
                            y_proj = y0 + dy_unit * t
                            effects["trajectory"].append({"x": x_proj, "y": y_proj})
                            tx, ty = int(math.floor(x_proj)), int(math.floor(y_proj))
                            if (
                                tx < 0
                                or tx >= len(self.map[0])
                                or ty < 0
                                or ty >= len(self.map)
                                or self.map[ty][tx] == 1
                            ):
                                effects["impact"] = {"x": x_proj, "y": y_proj}
                                break
                            for other in worms:
                                if other["id"] == worm["id"] or other["health"] <= 0:
                                    continue
                                if math.hypot(other["x"] - x_proj, other["y"] - y_proj) <= BAZOOKA_HIT_RADIUS:
                                    old_hp = other["health"]
                                    new_hp = max(0.0, old_hp - BAZOOKA_DAMAGE)
                                    actual_damage = old_hp - new_hp
                                    other["health"] = new_hp
                                    damage_total += actual_damage
                                    if new_hp == 0.0:
                                        kill_bonus += 100.0
                                    effects["impact"] = {"x": x_proj, "y": y_proj}
                                    t = BAZOOKA_MAX_RANGE + BAZOOKA_STEP_SIZE
                                    break
                            t += BAZOOKA_STEP_SIZE
                        return copy.deepcopy(state), damage_total + kill_bonus, effects
        
                    # ─── Grenade ──────────────────────────────────────────────────────
                    elif weapon == "grenade":
                        dx_total = float(action.get("dx", 0.0))
                        sign = 1.0 if dx_total >= 0 else -1.0
                        width = max(-GRENADE_MAX_THROW, min(dx_total, GRENADE_MAX_THROW))
                        height = abs(width) / 2.0
                        x0, y0 = worm["x"], worm["y"]
                        t = GRENADE_STEP_SIZE
                        effects["impact"] = {}
        
                        while True:
                            x_proj = x0 + sign * t
                            u = (t / abs(width)) if width != 0 else 0.0
                            y_proj = y0 - 4 * height * u * (1 - u)
        
                            effects["trajectory"].append({"x": x_proj, "y": y_proj})
                            tx, ty = int(math.floor(x_proj)), int(math.floor(y_proj))
        
                            if (
                                tx < 0
                                or tx >= len(self.map[0])
                                or ty < 0
                                or ty >= len(self.map)
                                or self.map[ty][tx] == 1
                            ):
                                effects["impact"] = {"x": x_proj, "y": y_proj}
                                break
        
                            hit = False
                            for other in worms:
                                if other["id"] == worm["id"] or other["health"] <= 0:
                                    continue
                                if math.hypot(other["x"] - x_proj, other["y"] - y_proj) <= GRENADE_HIT_RADIUS:
                                    old_hp = other["health"]
                                    new_hp = max(0.0, old_hp - GRENADE_DAMAGE)
                                    actual_damage = old_hp - new_hp
                                    other["health"] = new_hp
                                    damage_total += actual_damage
                                    if new_hp == 0.0:
                                        kill_bonus += 100.0
                                    effects["impact"] = {"x": x_proj, "y": y_proj}
                                    hit = True
                                    break
                            if hit:
                                break
        
                            t += GRENADE_STEP_SIZE
        
                        return copy.deepcopy(state), damage_total + kill_bonus, effects
        
                    else:
                        logger.warning("Unknown weapon: %s", weapon)
                        return copy.deepcopy(state), 0.0, {}
        
                # Default: no reward
                return copy.deepcopy(state), 0.0, effects
        
            def get_state_with_nicks(self, clients: Dict[Any, Dict[str, Any]]) -> Dict[str, Any]:
                state_copy = copy.deepcopy(self.state)
                for ws, info in clients.items():
                    pid = info["id"] - 1
                    if 0 <= pid < len(state_copy["worms"]):
                        state_copy["worms"][pid]["nick"] = info.get("nick", f"Player {pid + 1}")
                return state_copy
        --- SLUTT INNHOLD (game_core.py) ---

    |-- server.py
        --- START INNHOLD (server.py) ---
        #!/usr/bin/env python3
        """
        Continuous W.O.R.M.S. match server.
        
        Keeps the same WebSocket connections alive across many games so that RL clients
        can train without reconnecting.
        
        ‼ 2025‑05‑14: Added MAX_TURNS_PER_GAME so a game can never run forever.
        """
        from __future__ import annotations
        
        import argparse
        import asyncio
        import json
        import logging
        import random
        import sys
        from enum import IntEnum
        from pathlib import Path
        from typing import Any, Dict
        
        import websockets
        from websockets.exceptions import ConnectionClosed
        
        # ────────────────────────────────────────────────────────────────────────────────
        HOST, PORT = "127.0.0.1", 8765
        TIME_LIMIT_MS = 15_000
        MAX_TURNS_PER_GAME = 1_000          # ← NEW ── safe upper bound for one match
        # ────────────────────────────────────────────────────────────────────────────────
        
        # make game_core importable when run as script
        sys.path.append(str(Path(__file__).resolve().parent))
        from game_core import GameCore
        
        logger = logging.getLogger("server")
        
        
        class WSState(IntEnum):
            CONNECTING = 0
            OPEN = 1
            CLOSING = 2
            CLOSED = 3
        
        
        class WormsServer:
            def __init__(self, expected_players: int, max_turns: int = MAX_TURNS_PER_GAME) -> None:
                self.expected = expected_players
                self.max_turns = max_turns
                self.core = GameCore(expected_players=self.expected)
                self.clients: dict[Any, dict[str, Any]] = {}
                self.turn_order: list[Any] = []
                self.idx = 0
                self.turn_counter = 0
                self.game_id = 0
        
            # ───────────────────────────── connection handling ─────────────────────────
        
            async def accept(self, ws: Any) -> None:
                """Handle a brand‑new WebSocket until it closes."""
                try:
                    raw = await asyncio.wait_for(ws.recv(), timeout=10)
                    msg = json.loads(raw)
                except (asyncio.TimeoutError, json.JSONDecodeError):
                    await ws.close(code=4000, reason="Expected CONNECT")
                    return
        
                if msg.get("type") != "CONNECT":
                    await ws.close(code=4002, reason="First message must be CONNECT")
                    return
        
                pid = len(self.clients) + 1
                nick = msg.get("nick", f"Player {pid}")
                self.clients[ws] = {"id": pid, "nick": nick}
                self.turn_order.append(ws)
                await ws.send(json.dumps({"type": "ASSIGN_ID", "player_id": pid}))
                logger.info("player %d (%s) connected", pid, nick)
        
                try:
                    await ws.wait_closed()
                finally:
                    self._remove(ws)
        
            def _remove(self, ws: Any) -> None:
                """Forget a websocket that closed or errored."""
                if ws in self.turn_order:
                    idx = self.turn_order.index(ws)
                    self.turn_order.remove(ws)
                    # adjust current index if we removed an earlier entry
                    if idx <= self.idx and self.idx > 0:
                        self.idx -= 1
                if ws in self.clients:
                    pid = self.clients.pop(ws)["id"]
                    logger.info("player %d disconnected", pid)
        
            # ───────────────────────────── WebSocket helpers ───────────────────────────
        
            async def _safe_send(self, ws: Any, msg: Dict[str, Any]) -> bool:
                try:
                    await ws.send(json.dumps(msg))
                    return True
                except ConnectionClosed:
                    self._remove(ws)
                    return False
        
            async def _broadcast(self, msg: Dict[str, Any]) -> None:
                for ws in list(self.clients):
                    await self._safe_send(ws, msg)
        
            # ────────────────────────── single‑game loop ───────────────────────────────
        
            async def _play_single_game(self) -> None:
                # fresh random turn order for this game
                self.turn_order = [ws for ws in self.clients if ws.state == WSState.OPEN]
                random.shuffle(self.turn_order)
        
                # reset core & counters
                self.core = GameCore(expected_players=self.expected)
                self.turn_counter = 0
                self.idx = 0
                self.game_id += 1
        
                initial = self.core.get_state_with_nicks(self.clients)
                await self._broadcast(
                    {"type": "NEW_GAME", "game_id": self.game_id, "state": initial}
                )
                logger.info("=== GAME %d started with %d players ===",
                            self.game_id, len(self.turn_order))
        
                while True:
                    # 1. sudden death by turn limit ─────────────────────────────────────
                    if self.turn_counter >= self.max_turns:
                        # pick winner: most HP strictly > others, else draw
                        worms = self.core.state["worms"]
                        healthiest = max(worms, key=lambda w: w["health"], default=None)
                        tied = [
                            w for w in worms
                            if w["health"] == healthiest["health"]  # type: ignore[index]
                        ] if healthiest else []
                        winner_id = None
                        if healthiest and healthiest["health"] > 0 and len(tied) == 1:
                            winner_id = healthiest["id"] + 1  # store as 1‑based
                        await self._broadcast(
                            {
                                "type": "GAME_OVER",
                                "game_id": self.game_id,
                                "winner_id": winner_id,
                                "final_state": self.core.state,
                                "reason": "turn_limit",
                            }
                        )
                        logger.info("game %d ended by turn limit (%d turns)",
                                    self.game_id, self.turn_counter)
                        return
                    # ───────────────────────────────────────────────────────────────────
        
                    # 2. ordinary win condition (only one worm alive)
                    alive = [w for w in self.core.state["worms"] if w["health"] > 0]
                    if len(alive) <= 1:
                        winner = alive[0]["id"] + 1 if alive else None
                        await self._broadcast(
                            {
                                "type": "GAME_OVER",
                                "game_id": self.game_id,
                                "winner_id": winner,
                                "final_state": self.core.state,
                            }
                        )
                        logger.info("game %d finished naturally, winner=%s",
                                    self.game_id, winner)
                        return
        
                    # 3. if no one left in turn_order (should not happen) abort game
                    if not self.turn_order:
                        logger.warning("no active players left, aborting game %d", self.game_id)
                        return
        
                    # 4. choose the current websocket in round‑robin fashion
                    if self.idx >= len(self.turn_order):
                        self.idx = 0
                    ws = self.turn_order[self.idx]
                    if ws.state != WSState.OPEN:
                        self._remove(ws)
                        continue
        
                    pid = self.clients[ws]["id"]
                    worm = self.core.state["worms"][pid - 1]
        
                    # eliminated worm? announce once and skip its turn
                    if worm["health"] <= 0:
                        await self._broadcast({"type": "PLAYER_ELIMINATED", "player_id": pid})
                        self.turn_order.pop(self.idx)
                        continue
        
                    # 5. TURN_BEGIN ─ send state snapshot only to current player
                    begin_msg = {
                        "type": "TURN_BEGIN",
                        "turn_index": self.turn_counter,
                        "player_id": pid,
                        "state": self.core.get_state_with_nicks(self.clients),
                        "time_limit_ms": TIME_LIMIT_MS,
                    }
                    if not await self._safe_send(ws, begin_msg):
                        continue  # disconnected during send
        
                    # 6. wait for ACTION
                    try:
                        raw = await asyncio.wait_for(ws.recv(), timeout=15)
                        msg = json.loads(raw)
                        if msg.get("type") != "ACTION" or msg.get("player_id") != pid:
                            raise ValueError
                    except (asyncio.TimeoutError, ValueError):
                        # treat illegal / missing action as "stand"
                        msg = {"action": {"action": "stand"}}
                    except ConnectionClosed:
                        self._remove(ws)
                        continue
        
                    # 7. apply action in core
                    action = msg.get("action", {})
                    new_state, reward, effects = self.core.step(pid, action)
        
                    await self._broadcast(
                        {
                            "type": "TURN_RESULT",
                            "turn_index": self.turn_counter,
                            "player_id": pid,
                            "state": new_state,
                            "reward": reward,
                            "effects": effects,
                        }
                    )
        
                    # 8. advance turn
                    next_idx = (self.idx + 1) % len(self.turn_order)
                    next_pid = self.clients[self.turn_order[next_idx]]["id"]
                    await self._broadcast({"type": "TURN_END", "next_player_id": next_pid})
        
                    self.idx += 1
                    self.turn_counter += 1
        
            # ───────────────────────── orchestrator: endless loop ─────────────────────
        
            async def orchestrator(self) -> None:
                while True:
                    while len(self.clients) < self.expected:
                        await asyncio.sleep(0.1)
                    await self._play_single_game()
        
        
        # ──────────────────────────────── main entry ──────────────────────────────────
        async def main() -> None:
            parser = argparse.ArgumentParser(description="W.O.R.M.S. continuous server")
            parser.add_argument(
                "--log-level",
                choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                default="INFO",
                help="set logging level",
            )
            parser.add_argument("--max-players", type=int, default=2,
                                help="number of worms per game")
            parser.add_argument("--max-turns", type=int, default=MAX_TURNS_PER_GAME,
                                help="hard cap on turns before declaring a draw")
            args = parser.parse_args()
        
            logging.basicConfig(
                level=getattr(logging, args.log_level),
                format="%(asctime)s %(levelname)-8s %(name)s: %(message)s",
                datefmt="%H:%M:%S",
            )
        
            server = WormsServer(expected_players=args.max_players, max_turns=args.max_turns)
            async with websockets.serve(server.accept, HOST, PORT):
                asyncio.create_task(server.orchestrator())
                await asyncio.Future()  # run forever
        
        
        if __name__ == "__main__":
            asyncio.run(main())
        --- SLUTT INNHOLD (server.py) ---

    maps/
        |-- 001.json
            --- START INNHOLD (001.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0],
              [0,1,0,0,0,0,0,0,1,1,1,0],
              [0,1,1,1,1,0,0,0,1,1,1,0],
              [0,1,1,1,1,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (001.json) ---

        |-- 002.json
            --- START INNHOLD (002.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0],
              [0,0,0,1,1,1,1,0,0,0],
              [0,0,0,1,1,1,1,1,0,0],
              [0,1,1,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (002.json) ---

        |-- 003.json
            --- START INNHOLD (003.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,1,1,0,0,0,0,0,0,0,0,0,0],
              [0,1,1,1,1,0,0,0,0,1,1,1,0],
              [0,1,1,1,1,0,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (003.json) ---

        |-- 004.json
            --- START INNHOLD (004.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,1,1,0,0,0,0,0,0,0,0,1,1,0],
              [0,1,1,0,0,0,1,1,0,0,1,1,1,0],
              [0,1,1,1,0,0,1,1,1,0,1,1,1,0]
            ]
            --- SLUTT INNHOLD (004.json) ---

        |-- 005.json
            --- START INNHOLD (005.json) ---
            [
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,1,1,1,1,0,0,0,0,0,0],
              [0,1,1,1,1,1,1,1,1,1,1,1,1,0]
            ]
            --- SLUTT INNHOLD (005.json) ---

frontend/
    |-- package-lock.json
        --- START INNHOLD (package-lock.json) ---
        {
          "name": "relearning",
          "version": "0.0.1",
          "lockfileVersion": 3,
          "requires": true,
          "packages": {
            "": {
              "name": "relearning",
              "version": "0.0.1",
              "dependencies": {
                "esbuild": "^0.25.4",
                "pixi.js": "^8.9.2"
              }
            },
            "node_modules/@esbuild/aix-ppc64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.4.tgz",
              "integrity": "sha512-1VCICWypeQKhVbE9oW/sJaAmjLxhVqacdkvPLEjwlttjfwENRSClS8EjBz0KzRyFSCPDIkuXW34Je/vk7zdB7Q==",
              "cpu": [
                "ppc64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "aix"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-arm": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.4.tgz",
              "integrity": "sha512-QNdQEps7DfFwE3hXiU4BZeOV68HHzYwGd0Nthhd3uCkkEKK7/R6MTgM0P7H7FAs5pU/DIWsviMmEGxEoxIZ+ZQ==",
              "cpu": [
                "arm"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.4.tgz",
              "integrity": "sha512-bBy69pgfhMGtCnwpC/x5QhfxAz/cBgQ9enbtwjf6V9lnPI/hMyT9iWpR1arm0l3kttTr4L0KSLpKmLp/ilKS9A==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.4.tgz",
              "integrity": "sha512-TVhdVtQIFuVpIIR282btcGC2oGQoSfZfmBdTip2anCaVYcqWlZXGcdcKIUklfX2wj0JklNYgz39OBqh2cqXvcQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/darwin-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.4.tgz",
              "integrity": "sha512-Y1giCfM4nlHDWEfSckMzeWNdQS31BQGs9/rouw6Ub91tkK79aIMTH3q9xHvzH8d0wDru5Ci0kWB8b3up/nl16g==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "darwin"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/darwin-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.4.tgz",
              "integrity": "sha512-CJsry8ZGM5VFVeyUYB3cdKpd/H69PYez4eJh1W/t38vzutdjEjtP7hB6eLKBoOdxcAlCtEYHzQ/PJ/oU9I4u0A==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "darwin"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/freebsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.4.tgz",
              "integrity": "sha512-yYq+39NlTRzU2XmoPW4l5Ifpl9fqSk0nAJYM/V/WUGPEFfek1epLHJIkTQM6bBs1swApjO5nWgvr843g6TjxuQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "freebsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/freebsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.4.tgz",
              "integrity": "sha512-0FgvOJ6UUMflsHSPLzdfDnnBBVoCDtBTVyn/MrWloUNvq/5SFmh13l3dvgRPkDihRxb77Y17MbqbCAa2strMQQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "freebsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-arm": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.4.tgz",
              "integrity": "sha512-kro4c0P85GMfFYqW4TWOpvmF8rFShbWGnrLqlzp4X1TNWjRY3JMYUfDCtOxPKOIY8B0WC8HN51hGP4I4hz4AaQ==",
              "cpu": [
                "arm"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.4.tgz",
              "integrity": "sha512-+89UsQTfXdmjIvZS6nUnOOLoXnkUTB9hR5QAeLrQdzOSWZvNSAXAtcRDHWtqAUtAmv7ZM1WPOOeSxDzzzMogiQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-ia32": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.4.tgz",
              "integrity": "sha512-yTEjoapy8UP3rv8dB0ip3AfMpRbyhSN3+hY8mo/i4QXFeDxmiYbEKp3ZRjBKcOP862Ua4b1PDfwlvbuwY7hIGQ==",
              "cpu": [
                "ia32"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-loong64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.4.tgz",
              "integrity": "sha512-NeqqYkrcGzFwi6CGRGNMOjWGGSYOpqwCjS9fvaUlX5s3zwOtn1qwg1s2iE2svBe4Q/YOG1q6875lcAoQK/F4VA==",
              "cpu": [
                "loong64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-mips64el": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.4.tgz",
              "integrity": "sha512-IcvTlF9dtLrfL/M8WgNI/qJYBENP3ekgsHbYUIzEzq5XJzzVEV/fXY9WFPfEEXmu3ck2qJP8LG/p3Q8f7Zc2Xg==",
              "cpu": [
                "mips64el"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-ppc64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.4.tgz",
              "integrity": "sha512-HOy0aLTJTVtoTeGZh4HSXaO6M95qu4k5lJcH4gxv56iaycfz1S8GO/5Jh6X4Y1YiI0h7cRyLi+HixMR+88swag==",
              "cpu": [
                "ppc64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-riscv64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.4.tgz",
              "integrity": "sha512-i8JUDAufpz9jOzo4yIShCTcXzS07vEgWzyX3NH2G7LEFVgrLEhjwL3ajFE4fZI3I4ZgiM7JH3GQ7ReObROvSUA==",
              "cpu": [
                "riscv64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-s390x": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.4.tgz",
              "integrity": "sha512-jFnu+6UbLlzIjPQpWCNh5QtrcNfMLjgIavnwPQAfoGx4q17ocOU9MsQ2QVvFxwQoWpZT8DvTLooTvmOQXkO51g==",
              "cpu": [
                "s390x"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.4.tgz",
              "integrity": "sha512-6e0cvXwzOnVWJHq+mskP8DNSrKBr1bULBvnFLpc1KY+d+irZSgZ02TGse5FsafKS5jg2e4pbvK6TPXaF/A6+CA==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/netbsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.4.tgz",
              "integrity": "sha512-vUnkBYxZW4hL/ie91hSqaSNjulOnYXE1VSLusnvHg2u3jewJBz3YzB9+oCw8DABeVqZGg94t9tyZFoHma8gWZQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "netbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/netbsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.4.tgz",
              "integrity": "sha512-XAg8pIQn5CzhOB8odIcAm42QsOfa98SBeKUdo4xa8OvX8LbMZqEtgeWE9P/Wxt7MlG2QqvjGths+nq48TrUiKw==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "netbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/openbsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.4.tgz",
              "integrity": "sha512-Ct2WcFEANlFDtp1nVAXSNBPDxyU+j7+tId//iHXU2f/lN5AmO4zLyhDcpR5Cz1r08mVxzt3Jpyt4PmXQ1O6+7A==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "openbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/openbsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.4.tgz",
              "integrity": "sha512-xAGGhyOQ9Otm1Xu8NT1ifGLnA6M3sJxZ6ixylb+vIUVzvvd6GOALpwQrYrtlPouMqd/vSbgehz6HaVk4+7Afhw==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "openbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/sunos-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.4.tgz",
              "integrity": "sha512-Mw+tzy4pp6wZEK0+Lwr76pWLjrtjmJyUB23tHKqEDP74R3q95luY/bXqXZeYl4NYlvwOqoRKlInQialgCKy67Q==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "sunos"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.4.tgz",
              "integrity": "sha512-AVUP428VQTSddguz9dO9ngb+E5aScyg7nOeJDrF1HPYu555gmza3bDGMPhmVXL8svDSoqPCsCPjb265yG/kLKQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-ia32": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.4.tgz",
              "integrity": "sha512-i1sW+1i+oWvQzSgfRcxxG2k4I9n3O9NRqy8U+uugaT2Dy7kLO9Y7wI72haOahxceMX8hZAzgGou1FhndRldxRg==",
              "cpu": [
                "ia32"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.4.tgz",
              "integrity": "sha512-nOT2vZNw6hJ+z43oP1SPea/G/6AbN6X+bGNhNuq8NtRHy4wsMhw765IKLNmnjek7GvjWBYQ8Q5VBoYTFg9y1UQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@pixi/colord": {
              "version": "2.9.6",
              "resolved": "https://registry.npmjs.org/@pixi/colord/-/colord-2.9.6.tgz",
              "integrity": "sha512-nezytU2pw587fQstUu1AsJZDVEynjskwOL+kibwcdxsMBFqPsFFNA7xl0ii/gXuDi6M0xj3mfRJj8pBSc2jCfA==",
              "license": "MIT"
            },
            "node_modules/@types/css-font-loading-module": {
              "version": "0.0.12",
              "resolved": "https://registry.npmjs.org/@types/css-font-loading-module/-/css-font-loading-module-0.0.12.tgz",
              "integrity": "sha512-x2tZZYkSxXqWvTDgveSynfjq/T2HyiZHXb00j/+gy19yp70PHCizM48XFdjBCWH7eHBD0R5i/pw9yMBP/BH5uA==",
              "license": "MIT"
            },
            "node_modules/@types/earcut": {
              "version": "2.1.4",
              "resolved": "https://registry.npmjs.org/@types/earcut/-/earcut-2.1.4.tgz",
              "integrity": "sha512-qp3m9PPz4gULB9MhjGID7wpo3gJ4bTGXm7ltNDsmOvsPduTeHp8wSW9YckBj3mljeOh4F0m2z/0JKAALRKbmLQ==",
              "license": "MIT"
            },
            "node_modules/@webgpu/types": {
              "version": "0.1.60",
              "resolved": "https://registry.npmjs.org/@webgpu/types/-/types-0.1.60.tgz",
              "integrity": "sha512-8B/tdfRFKdrnejqmvq95ogp8tf52oZ51p3f4QD5m5Paey/qlX4Rhhy5Y8tgFMi7Ms70HzcMMw3EQjH/jdhTwlA==",
              "license": "BSD-3-Clause"
            },
            "node_modules/@xmldom/xmldom": {
              "version": "0.8.10",
              "resolved": "https://registry.npmjs.org/@xmldom/xmldom/-/xmldom-0.8.10.tgz",
              "integrity": "sha512-2WALfTl4xo2SkGCYRt6rDTFfk9R1czmBvUQy12gK2KuRKIpWEhcbbzy8EZXtz/jkRqHX8bFEc6FC1HjX4TUWYw==",
              "license": "MIT",
              "engines": {
                "node": ">=10.0.0"
              }
            },
            "node_modules/earcut": {
              "version": "2.2.4",
              "resolved": "https://registry.npmjs.org/earcut/-/earcut-2.2.4.tgz",
              "integrity": "sha512-/pjZsA1b4RPHbeWZQn66SWS8nZZWLQQ23oE3Eam7aroEFGEvwKAsJfZ9ytiEMycfzXWpca4FA9QIOehf7PocBQ==",
              "license": "ISC"
            },
            "node_modules/esbuild": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.4.tgz",
              "integrity": "sha512-8pgjLUcUjcgDg+2Q4NYXnPbo/vncAY4UmyaCm0jZevERqCHZIaWwdJHkf8XQtu4AxSKCdvrUbT0XUr1IdZzI8Q==",
              "hasInstallScript": true,
              "license": "MIT",
              "bin": {
                "esbuild": "bin/esbuild"
              },
              "engines": {
                "node": ">=18"
              },
              "optionalDependencies": {
                "@esbuild/aix-ppc64": "0.25.4",
                "@esbuild/android-arm": "0.25.4",
                "@esbuild/android-arm64": "0.25.4",
                "@esbuild/android-x64": "0.25.4",
                "@esbuild/darwin-arm64": "0.25.4",
                "@esbuild/darwin-x64": "0.25.4",
                "@esbuild/freebsd-arm64": "0.25.4",
                "@esbuild/freebsd-x64": "0.25.4",
                "@esbuild/linux-arm": "0.25.4",
                "@esbuild/linux-arm64": "0.25.4",
                "@esbuild/linux-ia32": "0.25.4",
                "@esbuild/linux-loong64": "0.25.4",
                "@esbuild/linux-mips64el": "0.25.4",
                "@esbuild/linux-ppc64": "0.25.4",
                "@esbuild/linux-riscv64": "0.25.4",
                "@esbuild/linux-s390x": "0.25.4",
                "@esbuild/linux-x64": "0.25.4",
                "@esbuild/netbsd-arm64": "0.25.4",
                "@esbuild/netbsd-x64": "0.25.4",
                "@esbuild/openbsd-arm64": "0.25.4",
                "@esbuild/openbsd-x64": "0.25.4",
                "@esbuild/sunos-x64": "0.25.4",
                "@esbuild/win32-arm64": "0.25.4",
                "@esbuild/win32-ia32": "0.25.4",
                "@esbuild/win32-x64": "0.25.4"
              }
            },
            "node_modules/eventemitter3": {
              "version": "5.0.1",
              "resolved": "https://registry.npmjs.org/eventemitter3/-/eventemitter3-5.0.1.tgz",
              "integrity": "sha512-GWkBvjiSZK87ELrYOSESUYeVIc9mvLLf/nXalMOS5dYrgZq9o5OVkbZAVM06CVxYsCwH9BDZFPlQTlPA1j4ahA==",
              "license": "MIT"
            },
            "node_modules/gifuct-js": {
              "version": "2.1.2",
              "resolved": "https://registry.npmjs.org/gifuct-js/-/gifuct-js-2.1.2.tgz",
              "integrity": "sha512-rI2asw77u0mGgwhV3qA+OEgYqaDn5UNqgs+Bx0FGwSpuqfYn+Ir6RQY5ENNQ8SbIiG/m5gVa7CD5RriO4f4Lsg==",
              "license": "MIT",
              "dependencies": {
                "js-binary-schema-parser": "^2.0.3"
              }
            },
            "node_modules/ismobilejs": {
              "version": "1.1.1",
              "resolved": "https://registry.npmjs.org/ismobilejs/-/ismobilejs-1.1.1.tgz",
              "integrity": "sha512-VaFW53yt8QO61k2WJui0dHf4SlL8lxBofUuUmwBo0ljPk0Drz2TiuDW4jo3wDcv41qy/SxrJ+VAzJ/qYqsmzRw==",
              "license": "MIT"
            },
            "node_modules/js-binary-schema-parser": {
              "version": "2.0.3",
              "resolved": "https://registry.npmjs.org/js-binary-schema-parser/-/js-binary-schema-parser-2.0.3.tgz",
              "integrity": "sha512-xezGJmOb4lk/M1ZZLTR/jaBHQ4gG/lqQnJqdIv4721DMggsa1bDVlHXNeHYogaIEHD9vCRv0fcL4hMA+Coarkg==",
              "license": "MIT"
            },
            "node_modules/parse-svg-path": {
              "version": "0.1.2",
              "resolved": "https://registry.npmjs.org/parse-svg-path/-/parse-svg-path-0.1.2.tgz",
              "integrity": "sha512-JyPSBnkTJ0AI8GGJLfMXvKq42cj5c006fnLz6fXy6zfoVjJizi8BNTpu8on8ziI1cKy9d9DGNuY17Ce7wuejpQ==",
              "license": "MIT"
            },
            "node_modules/pixi.js": {
              "version": "8.9.2",
              "resolved": "https://registry.npmjs.org/pixi.js/-/pixi.js-8.9.2.tgz",
              "integrity": "sha512-oLFBkOOA/O6OpT5T8o05AxgZB9x9yWNzEQ+WTNZZFoCvfU2GdT4sFTjpVFuHQzgZPmAm/1IFhKdNiXVnlL8PRw==",
              "license": "MIT",
              "dependencies": {
                "@pixi/colord": "^2.9.6",
                "@types/css-font-loading-module": "^0.0.12",
                "@types/earcut": "^2.1.4",
                "@webgpu/types": "^0.1.40",
                "@xmldom/xmldom": "^0.8.10",
                "earcut": "^2.2.4",
                "eventemitter3": "^5.0.1",
                "gifuct-js": "^2.1.2",
                "ismobilejs": "^1.1.1",
                "parse-svg-path": "^0.1.2"
              }
            }
          }
        }
        --- SLUTT INNHOLD (package-lock.json) ---

    |-- package.json
        --- START INNHOLD (package.json) ---
        {
          "name": "relearning",
          "version": "0.0.1",
          "author": "Ask Sødal <asksodal@gmail.com>",
          "scripts": {
            "watch": "esbuild --bundle src/script.js --outfile=public/bundle.js --watch",
            "serve": "esbuild --bundle src/script.js --outfile=public/bundle.js --servedir=public"
          },
          "dependencies": {
            "pixi.js": "^8.9.2",
            "esbuild": "^0.25.4"
          }
        }
        --- SLUTT INNHOLD (package.json) ---

    public/
    src/
training_plots/