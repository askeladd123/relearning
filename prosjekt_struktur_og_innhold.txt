Prosjektstruktur og filinnhold (fra mappen: C:\Users\kriny\andre_fag\pattern_recognition\Directory\relearning)

Skriptet 'all.py' og outputfilen 'prosjekt_struktur_og_innhold.txt' er ekskludert.
Følgende mappenavn blir også fullstendig ignorert (i tillegg til andre skjulte mapper som starter med '.'): .git, __pycache__, .vscode, .idea, venv, env, .venv, node_modules

================================================================================

    |-- description.md
        --- START INNHOLD (description.md) ---
        *Turn‑based Reinforcement‑Learning playground*
        
        The goal is to train an AI to play W.O.R.M.S.. I also want to be able to test the game against the agents. That is why I chose to modularize into components, that communicate through websockets:
        - server / environment: manages communication with clients and simulates the environment
        - cli client / ai agent: choses actions based on the gives environment, and learns
        - web client: user can take the role as a player and manually chose actions against the AI. This is visualize as a simple game interface in the browser
        
        ---
        
        ## 1  High‑level Picture
        
        ```
        
        ┌──────────┐      WebSocket/JSON       ┌─────────────┐
        │  Browser │◀─────────────────────────▶│  Server     │
        │ (Pixi UI)│                           │ (env + core)│
        └────┬─────┘                           └────┬────────┘
             │ WebSocket/JSON                        │
             │                                       │
             ▼                                       ▼
        ┌──────────┐      WebSocket/JSON       ┌─────────────┐
        │  RL Bot  │◀─────────────────────────▶│  GameCore   │
        │ (Python) │                           │  (pure logic)
        └──────────┘                           └─────────────┘
        
        ````
        
        * **Server / Environment** (`environment/`)  
          Runs a match, owns the authoritative **`GameCore`** logic, and speaks the
          JSON protocol over **WebSockets**.  
        * **Agents** (`agents/`)  
          CLI clients—human or reinforcement‑learning models—that connect, read
          `TURN_BEGIN`, decide an `ACTION`, and learn from `TURN_RESULT`.  
        * **Web Client** (`frontend/`)  
          Pixi.js interface for humans: shows the map full‑width, lets you pick
          actions via dropdown + dynamic parameter inputs, and renders worms in
          sub‑tile positions.
        
        Everything communicates through **one common surface**: the JSON messages
        documented in `json-docs.md`.
        
        ---
        
        ## 2  Runtime Flow
        
        1. **CONNECT** Clients open `ws://127.0.0.1:8765` and send
           ```json
           { "type": "CONNECT", "nick": "<name>" }
        ````
        
        The server replies `ASSIGN_ID`.
        
        2. **Turn loop**
        
           ```
           TURN_BEGIN → ACTION → TURN_RESULT → TURN_END
           ```
        
           repeats until `GAME_OVER`.
        
        3. Clients render or learn from the **Game State** snapshot inside each
           `TURN_BEGIN` / `TURN_RESULT`.  The state contains:
        
           * `worms` with **world‑unit** `(x,y)` floats
           * `map` grid with `1 = terrain`, `0 = empty`
        
        4. **Physics & rules** (to‑be‑implemented) live entirely in `GameCore.step()`.
        
        ---
        
        ## 3  Coordinate System
        
        * **Tile grid** Index `(0,0)` is top‑left.
        * **World units** 1 world‑unit ≙ 1 tile.  Worms can be at `3.2, 1.75` etc.
        * **Screen mapping** (client side)
        
          ```
          tile   = min(canvasW / cols, canvasH / rows)
          offset = ((canvasW-tile*cols)/2, (canvasH-tile*rows)/2)
          pixelX = offset.x + xWorld * tile
          pixelY = offset.y + yWorld * tile
          ```
        
        This keeps the grid centred and lets agents move smoothly within a tile.
        
        ---
        
        ## 4  Components in Detail
        
        | Folder         | Key files                            | Role                                  |
        | -------------- | ------------------------------------ | ------------------------------------- |
        | `environment/` | `server.py`, `game_core.py`          | Match orchestration + pure game logic |
        | `agents/`      | `client.py` (baseline random bot)    | Example RL agent; train & act here    |
        | `frontend/`    | `src/script.js`, `public/index.html` | Pixi.js UI + dynamic action form      |
        | root           | `json-docs.md`                       | Authoritative protocol spec           |
        
        ### Build / Run Quick‑start
        
        ```bash
        # Python side
        cd environment && python server.py
        
        # Web UI
        cd frontend
        npm i            # (once)
        npx esbuild src/script.js --bundle --outfile=public/bundle.js --format=esm
        npx serve public # or any static server
        
        # Random bot
        cd agents && python client.py
        ```
        
        ---
        
        ## 5  Extensibility Guidelines
        
        * **Physics upgrades** Add terrain slopes or water by extending the `map`
          encoding; clients ignore unknown keys.
        * **New actions** Introduce additional `action` variants in `json-docs.md`
          (clients will ignore ones they don’t understand).
        * **More players** `GameCore.expected_players()` controls how many `CONNECT`s
          start a match.
        * **Rewards** Fill the `reward` field in `TURN_RESULT`; RL agents train on it.
        
        ---
        --- SLUTT INNHOLD (description.md) ---

    |-- json-docs.md
        --- START INNHOLD (json-docs.md) ---
        # W.O.R.M.S. WebSocket JSON Protocol
        
        All messages are JSON objects sent over a WebSocket.  
        Each object **must** include a `"type"` field that selects one of the structures below.  
        Clients **must ignore** unknown keys so the protocol can evolve.
        
        Numbers follow JS/Python IEEE‑754 semantics unless noted.
        
        ---
        
        ## Overall flow
        
        ```text
                    Client                                  Server
                     │                                         │
                     ├─▶  CONNECT                              │
                     │     (nick)                              │
                     │                                         │
                     │   ASSIGN_ID  ◀──────────────────────────┤
                     │                                         │
              repeat │                                         │
              until  │  TURN_BEGIN ───────────────────────────▶│
           GAME_OVER │  ACTION       ◀──────────────────────── │
                     │  TURN_RESULT ─────────────────────────▶ │
                     │  TURN_END    ─────────────────────────▶ │
                     │                                         │
                     └─▶ GAME_OVER   ◀─────────────────────────┤
        ````
        
        ---
        
        ## 1  Messages
        
        ### 1.1 CONNECT  (client → server)
        
        | Field       | Type    | Required | Description                       |
        | ----------- | ------- | -------- | --------------------------------- |
        | `type`      | string  | ✓        | `"CONNECT"`                       |
        | `nick`      | string  | ✗        | Human nickname (for logs / UI)    |
        | `spectator` | boolean | ✗        | `true` to observe without playing |
        
        ```json
        { "type": "CONNECT", "nick": "bot‑42" }
        ```
        
        ---
        
        ### 1.2 ASSIGN\_ID  (server → client)
        
        | Field       | Type    | Required | Description                                     |
        | ----------- | ------- | -------- | ----------------------------------------------- |
        | `type`      | string  | ✓        | `"ASSIGN_ID"`                                   |
        | `player_id` | integer | ✗        | Omitted for spectators; starts at 1 for players |
        
        ```json
        { "type": "ASSIGN_ID", "player_id": 2 }
        ```
        
        ---
        
        ### 1.3 TURN\_BEGIN  (server → all)
        
        | Field           | Type       | Required | Description                        |
        | --------------- | ---------- | -------- | ---------------------------------- |
        | `type`          | string     | ✓        | `"TURN_BEGIN"`                     |
        | `turn_index`    | integer    | ✓        | 0‑based global turn counter        |
        | `player_id`     | integer    | ✓        | ID whose turn it is                |
        | `state`         | Game State | ✓        | Full snapshot                      |
        | `time_limit_ms` | integer    | ✓        | How long that player has to answer |
        
        ---
        
        ### 1.4 ACTION  (client → server)
        
        Sent **only** by the `player_id` that just received `TURN_BEGIN`.
        
        | Field       | Type   | Required | Description               |
        | ----------- | ------ | -------- | ------------------------- |
        | `type`      | string | ✓        | `"ACTION"`                |
        | `player_id` | int    | ✓        | Must match sender’s ID    |
        | `action`    | object | ✓        | One of the variants below |
        
        #### `action` variants
        
        | Variant         | Required keys                                                                     | Notes                           |
        | --------------- | --------------------------------------------------------------------------------- | ------------------------------- |
        | stand           | `{ "action": "stand" }`                                                           | –                               |
        | walk            | `{ "action": "walk", "dx": float }`                                               | `dx` in world units (+ → right) |
        | attack\:kick    | `{ "action": "attack", "weapon": "kick", "force": 0‑100 }`                        | –                               |
        | attack\:bazooka | `{ "action": "attack", "weapon": "bazooka", "angle_deg": float }`                 | 0 ° = right, CCW positive       |
        | attack\:grenade | `{ "action": "attack", "weapon": "grenade", "angle_deg": float, "force": 0‑100 }` | –                               |
        
        ---
        
        ### 1.5 TURN\_RESULT  (server → all)
        
        | Field        | Type       | Required | Description                       |
        | ------------ | ---------- | -------- | --------------------------------- |
        | `type`       | string     | ✓        | `"TURN_RESULT"`                   |
        | `turn_index` | integer    | ✓        | Same index as the triggering turn |
        | `player_id`  | integer    | ✓        | The acting player                 |
        | `state`      | Game State | ✓        | Resulting state                   |
        | `reward`     | number     | ✗        | Optional per‑turn reward          |
        
        ---
        
        ### 1.6 TURN\_END  (server → all)
        
        | Field            | Type    | Required | Description       |
        | ---------------- | ------- | -------- | ----------------- |
        | `type`           | string  | ✓        | `"TURN_END"`      |
        | `next_player_id` | integer | ✓        | Who will act next |
        
        ---
        
        ### 1.7 GAME\_OVER  (server → all)
        
        | Field         | Type       | Required | Description        |
        | ------------- | ---------- | -------- | ------------------ |
        | `type`        | string     | ✓        | `"GAME_OVER"`      |
        | `winner_id`   | integer    | ✗        | Winner’s ID if any |
        | `final_state` | Game State | ✓        | Final snapshot     |
        
        ---
        
        ### 1.8 ERROR  (server → client)
        
        | Field  | Type   | Required | Description         |
        | ------ | ------ | -------- | ------------------- |
        | `type` | string | ✓        | `"ERROR"`           |
        | `msg`  | string | ✓        | Human‑readable text |
        
        ---
        
        ## 2  Game State
        
        ```jsonc
        {
          "worms": [
            { "id": 0, "health": 100, "x": 3.20, "y": 1.75 },
            { "id": 1, "health":  90, "x": 6.00, "y": 2.00 }
          ],
          "map": [
            [1,0,0,0,0,0,0,0],
            [1,0,0,0,0,0,0,0],
            [1,1,1,0,0,0,1,0],
            [1,1,1,0,0,1,1,1]
          ]
        }
        ```
        
        | Key     | Type          | Description                                                                                   |
        | ------- | ------------- | --------------------------------------------------------------------------------------------- |
        | `worms` | array         | Each worm’s `id`, `health` (0‑100), and **position** in world units (floats; 1 unit = 1 tile) |
        | `map`   | 2‑D int array | `1` = solid terrain, `0` = empty/water. Index (0,0) is top‑left                               |
        
        Clients **must ignore** any extra keys they do not understand.
        
        ---
        
        ## 3  Rendering rule (client hint)
        
        To draw the map centred in the canvas:
        
        ```text
        tile = min(canvasW / cols, canvasH / rows)
        xMargin = (canvasW − tile*cols)/2
        yMargin = (canvasH − tile*rows)/2
        tile(i,j) at (xMargin + i*tile , yMargin + j*tile)
        ```
        
        This leaves a minimal margin on one axis while fully filling the other.
        --- SLUTT INNHOLD (json-docs.md) ---

    |-- plan.md
        --- START INNHOLD (plan.md) ---
        - **learn**: runs and saves state of environment
        - **data**: saved here
        - **frontend**: reads data and plays it in a browser
        --- SLUTT INNHOLD (plan.md) ---

agents/
    |-- __init__.py
        [--- INNHOLD: Tom fil ---]

    |-- client.py
        --- START INNHOLD (client.py) ---
        #!/usr/bin/env python3
        """Simple random bot for W.O.R.M.S."""
        import argparse
        import asyncio
        import json
        import logging
        import random
        
        import websockets
        
        # ─── Define custom TRACE level ─────────────────────────────────────
        TRACE_LEVEL_NUM = 5
        logging.addLevelName(TRACE_LEVEL_NUM, "TRACE")
        logging.TRACE = TRACE_LEVEL_NUM               # expose logging.TRACE
        def trace(self, message, *args, **kwargs):
            if self.isEnabledFor(TRACE_LEVEL_NUM):
                self._log(TRACE_LEVEL_NUM, message, args, **kwargs)
        logging.Logger.trace = trace
        
        # ─── Parse --log-level & configure ────────────────────────────────
        def setup_logging() -> logging.Logger:
            parser = argparse.ArgumentParser(description="W.O.R.M.S. bot client")
            parser.add_argument(
                "--log-level",
                choices=["TRACE", "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                default="INFO",
                help="set logging level"
            )
            args = parser.parse_args()
            level = getattr(logging, args.log_level)
            logging.basicConfig(
                level=level,
                format="%(asctime)s %(levelname)-8s %(name)s: %(message)s",
                datefmt="%H:%M:%S"
            )
            return logging.getLogger("client")
        
        logger = setup_logging()
        
        HOST, PORT = "127.0.0.1", 8765
        
        ACTIONS = [
            {"action": "stand"},
            {"action": "walk", "dx": 1.0},
            {"action": "attack", "weapon": "kick", "force": 70.0},
            {"action": "attack", "weapon": "bazooka", "angle_deg": 90.0},
            {"action": "attack", "weapon": "grenade", "angle_deg": 30.0, "force": 50.0},
        ]
        
        async def start_client() -> None:
            uri = f"ws://{HOST}:{PORT}"
            logger.info("connecting to %s", uri)
            async with websockets.connect(uri) as ws:
                await ws.send(json.dumps({"type": "CONNECT", "nick": "bot"}))
                logger.info("sent CONNECT")
                player_id: int | None = None
                async for message in ws:
                    msg = json.loads(message)
                    t = msg.get("type")
                    if t == "ASSIGN_ID":
                        player_id = msg["player_id"]
                        logger.info("assigned player_id=%d", player_id)
                        continue
                    if t == "TURN_BEGIN" and msg["player_id"] == player_id:
                        action = random.choice(ACTIONS)
                        payload = {"type": "ACTION", "player_id": player_id, "action": action}
                        await ws.send(json.dumps(payload))
                        logger.debug("did action: %r", payload)
        
        if __name__ == "__main__":
            asyncio.run(start_client())
        --- SLUTT INNHOLD (client.py) ---

    a2c_manual/
        |-- __init__.py
            [--- INNHOLD: Tom fil ---]

        |-- agent.py
            --- START INNHOLD (agent.py) ---
            # agents/a2c_manual/agent.py
            import torch
            import torch.optim as optim
            import torch.nn.functional as F
            from torch.distributions import Categorical, Normal
            import numpy as np
            
            from . import config
            from .model import ActorCriticNetwork
            from .utils import preprocess_state, format_action
            
            
            class A2CAgent:
                def __init__(self):
                    self.network = ActorCriticNetwork().to(config.DEVICE)
                    self.optimizer = optim.Adam(self.network.parameters(), lr=config.LEARNING_RATE)
                    self.player_id = None
            
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.entropies_buffer = []
            
                def set_player_id(self, player_id: int):
                    self.player_id = player_id
                    print(f"Agentens player_id er satt til: {self.player_id}")
            
                def select_action(self, current_game_state_json: dict):
                    if self.player_id is None:
                        print("FEIL select_action: Agent player_id er ikke satt. Returnerer 'stand'.")
                        return {"action": "stand"}
            
                    map_tensor, worm_vector_tensor = preprocess_state(current_game_state_json, self.player_id)
            
                    actor_outputs, state_value = self.network(map_tensor, worm_vector_tensor)
            
                    squeezed_state_value = state_value.squeeze()
                    self.values_buffer.append(squeezed_state_value)
            
                    action_type_probs = actor_outputs['action_type_probs']
                    action_type_dist = Categorical(action_type_probs)
                    network_action_idx_tensor = action_type_dist.sample()  # Dette er en tensor, f.eks. tensor([2])
                    network_action_idx_item = network_action_idx_tensor.item()  # Få Python-int for indeksering
            
                    # Gjør om til skalar tensor ved å bruke .squeeze() hvis den har batch-dim,
                    # eller ved å hente ut elementet hvis det er en 1-element tensor.
                    # For Categorical.log_prob(sample), hvis sample er en 0-dim tensor, er output 0-dim.
                    # Hvis sample er 1-dim (som network_action_idx_tensor), er output 1-dim.
                    log_prob_action = action_type_dist.log_prob(network_action_idx_tensor).squeeze()
                    entropy_action = action_type_dist.entropy().squeeze()
            
                    # print(f"DEBUG select_action: log_prob_action={log_prob_action}, shape={log_prob_action.shape}")
                    # print(f"DEBUG select_action: entropy_action={entropy_action}, shape={entropy_action.shape}")
            
                    step_log_probs = [log_prob_action]
                    step_entropies = [entropy_action]
            
                    chosen_network_action_name = config.NETWORK_ACTION_ORDER[network_action_idx_item]
                    # print(f"DEBUG select_action: Chosen action_name: {chosen_network_action_name}")
                    params_for_formatting = {}
            
                    if chosen_network_action_name == 'walk':
                        walk_dx_probs = actor_outputs['walk_dx_probs']
                        walk_dx_dist = Categorical(walk_dx_probs)
                        walk_dx_bin_idx_tensor = walk_dx_dist.sample()
                        params_for_formatting['walk_dx_bin_idx'] = walk_dx_bin_idx_tensor.item()
                        log_p = walk_dx_dist.log_prob(walk_dx_bin_idx_tensor).squeeze()
                        ent_p = walk_dx_dist.entropy().squeeze()
                        # print(f"DEBUG select_action (walk): log_p_param={log_p}, shape={log_p.shape}")
                        step_log_probs.append(log_p)
                        step_entropies.append(ent_p)
            
                    elif chosen_network_action_name == 'attack_kick':
                        mean, std = actor_outputs['kick_params']
                        # Normal tar skalar mean/std hvis output fra FC er [batch, 1] og vi squeezer
                        dist = Normal(mean.squeeze(), std.squeeze())
                        force_val = dist.sample()  # force_val vil være skalar
                        params_for_formatting['kick_force_val'] = force_val.item()
                        log_p = dist.log_prob(force_val)  # log_prob av skalar er skalar
                        ent_p = dist.entropy()  # entropi av dist med skalar params er skalar
                        # print(f"DEBUG select_action (kick): log_p_param={log_p}, shape={log_p.shape}")
                        step_log_probs.append(log_p)
                        step_entropies.append(ent_p)
            
                    elif chosen_network_action_name == 'attack_bazooka':
                        angle_mean, angle_std = actor_outputs['bazooka_params']
                        dist = Normal(angle_mean.squeeze(), angle_std.squeeze())
                        angle_val = dist.sample()
                        params_for_formatting['bazooka_angle_val'] = angle_val.item()
                        log_p = dist.log_prob(angle_val)
                        ent_p = dist.entropy()
                        # print(f"DEBUG select_action (bazooka): log_p_param_angle={log_p}, shape={log_p.shape}")
                        step_log_probs.append(log_p)
                        step_entropies.append(ent_p)
            
                    elif chosen_network_action_name == 'attack_grenade':
                        angle_mean, angle_std, force_mean, force_std = actor_outputs['grenade_params']
                        # Angle
                        angle_dist = Normal(angle_mean.squeeze(), angle_std.squeeze())
                        angle_val = angle_dist.sample()
                        params_for_formatting['grenade_angle_val'] = angle_val.item()
                        log_p_angle = angle_dist.log_prob(angle_val)
                        ent_p_angle = angle_dist.entropy()
                        # print(f"DEBUG select_action (grenade): log_p_param_angle={log_p_angle}, shape={log_p_angle.shape}")
                        step_log_probs.append(log_p_angle)
                        step_entropies.append(ent_p_angle)
            
                        # Force
                        force_dist = Normal(force_mean.squeeze(), force_std.squeeze())
                        force_val = force_dist.sample()
                        params_for_formatting['grenade_force_val'] = force_val.item()
                        log_p_force = force_dist.log_prob(force_val)
                        ent_p_force = force_dist.entropy()
                        # print(f"DEBUG select_action (grenade): log_p_param_force={log_p_force}, shape={log_p_force.shape}")
                        step_log_probs.append(log_p_force)
                        step_entropies.append(ent_p_force)
            
                    # Nå skal alle elementer i step_log_probs og step_entropies være skalar-tensorer (0-dim)
                    # print(f"DEBUG select_action: step_log_probs before stack: {[lp.shape for lp in step_log_probs]}")
                    try:
                        # torch.stack lager en 1D tensor fra listen av skalarer
                        stacked_log_probs = torch.stack(step_log_probs)
                        summed_log_probs = stacked_log_probs.sum()  # Summerer 1D tensoren til en skalar
                    except Exception as e:
                        print(f"FEIL i select_action under stacking/summing av log_probs: {e}")
                        print(f"  step_log_probs var: {step_log_probs}")
                        summed_log_probs = torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32)
            
                    self.log_probs_buffer.append(summed_log_probs)
            
                    # print(f"DEBUG select_action: step_entropies before stack: {[e.shape for e in step_entropies]}")
                    try:
                        stacked_entropies = torch.stack(step_entropies)
                        summed_entropies = stacked_entropies.sum()
                    except Exception as e:
                        print(f"FEIL i select_action under stacking/summing av entropies: {e}")
                        print(f"  step_entropies var: {step_entropies}")
                        summed_entropies = torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32)
            
                    self.entropies_buffer.append(summed_entropies)
            
                    action_json_to_send = format_action(network_action_idx_item, params_for_formatting)
            
                    return action_json_to_send
            
                def store_reward(self, reward: float):
                    self.rewards_buffer.append(reward)
            
                def learn(self, next_game_state_json: dict | None, done: bool):
                    if not self.log_probs_buffer or not self.rewards_buffer or not self.values_buffer:
                        # print(f"DEBUG learn: Buffers tomme. log_probs_buffer: {len(self.log_probs_buffer)}, rewards_buffer: {len(self.rewards_buffer)}, values_buffer: {len(self.values_buffer)}")
                        self.clear_buffers()
                        return None, None, None
            
                        # print(f"DEBUG learn: Antall steg i buffer: log_probs={len(self.log_probs_buffer)}, values={len(self.values_buffer)}, rewards={len(self.rewards_buffer)}, entropies={len(self.entropies_buffer)}")
            
                    R_bootstrap = torch.tensor(0.0, device=config.DEVICE, dtype=torch.float32)
                    if not done:
                        if next_game_state_json and self.player_id is not None:
                            if isinstance(next_game_state_json, dict) and next_game_state_json.get(
                                    "map") and next_game_state_json.get("worms"):
                                next_map, next_worm = preprocess_state(next_game_state_json, self.player_id)
                                with torch.no_grad():
                                    _, next_value = self.network(next_map, next_worm)
                                    R_bootstrap = next_value.squeeze()  # Blir skalar
                            else:
                                # print(f"WARN learn: next_game_state_json er ugyldig for bootstrapping ved done=False: {str(next_game_state_json)[:200]}. Bruker V(s_next)=0.")
                                pass  # R_bootstrap forblir 0.0
            
                    returns = []
                    R_discounted = R_bootstrap
                    for r_idx in range(len(self.rewards_buffer) - 1, -1, -1):
                        reward_val = torch.tensor(self.rewards_buffer[r_idx], device=config.DEVICE, dtype=torch.float32)
                        R_discounted = reward_val + config.GAMMA * R_discounted
                        returns.insert(0, R_discounted)
            
                    if not returns:
                        # print("FEIL learn: `returns` listen er tom etter beregning.")
                        self.clear_buffers()
                        return None, None, None
            
                    try:
                        # Alle buffere skal nå inneholde skalar-tensorer.
                        # torch.stack vil lage 1D tensorer.
                        returns_tensor = torch.stack(returns).detach()
                        values_tensor = torch.stack(self.values_buffer)
                        log_probs_tensor = torch.stack(self.log_probs_buffer)
                        entropies_tensor = torch.stack(self.entropies_buffer)
                    except RuntimeError as e_stack:
                        print(f"FEIL learn: RuntimeError under torch.stack: {e_stack}")
                        print(
                            f"  Antall elementer: returns={len(returns)}, values_buffer={len(self.values_buffer)}, log_probs_buffer={len(self.log_probs_buffer)}, entropies_buffer={len(self.entropies_buffer)}")
                        # Detaljert sjekk av formen på elementene i bufferne hvis feilen vedvarer
                        if len(self.values_buffer) > 0: print(f"  values_buffer[0] shape: {self.values_buffer[0].shape}")
                        if len(self.log_probs_buffer) > 0: print(f"  log_probs_buffer[0] shape: {self.log_probs_buffer[0].shape}")
                        if len(self.entropies_buffer) > 0: print(f"  entropies_buffer[0] shape: {self.entropies_buffer[0].shape}")
                        self.clear_buffers()
                        return None, None, None
            
                    returns_tensor = returns_tensor.to(config.DEVICE)
                    values_tensor = values_tensor.to(config.DEVICE)
                    log_probs_tensor = log_probs_tensor.to(config.DEVICE)
                    entropies_tensor = entropies_tensor.to(config.DEVICE)
            
                    # print(f"DEBUG learn: shapes etter stack: R={returns_tensor.shape}, V={values_tensor.shape}, LP={log_probs_tensor.shape}, E={entropies_tensor.shape}")
            
                    if values_tensor.numel() == 0 or returns_tensor.numel() == 0 or log_probs_tensor.numel() == 0:
                        self.clear_buffers()
                        return None, None, None
            
                    # Sikre at tensorer er minst 1D for .mean() og F.mse_loss
                    # Dette er normalt håndtert av torch.stack hvis inputlistene ikke er tomme.
                    # Men en ekstra sjekk hvis en tensor ble 0-dim av en eller annen grunn (f.eks. stack av én 0-dim tensor).
                    if values_tensor.ndim == 0: values_tensor = values_tensor.unsqueeze(0)
                    if returns_tensor.ndim == 0: returns_tensor = returns_tensor.unsqueeze(0)
                    if log_probs_tensor.ndim == 0: log_probs_tensor = log_probs_tensor.unsqueeze(0)
                    if entropies_tensor.ndim == 0: entropies_tensor = entropies_tensor.unsqueeze(0)
            
                    advantages = returns_tensor - values_tensor
                    if advantages.numel() > 1:
                        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
                    elif advantages.numel() == 0:
                        self.clear_buffers()
                        return None, None, None
            
                    policy_loss = (-log_probs_tensor * advantages.detach()).mean()
                    value_loss = F.mse_loss(values_tensor, returns_tensor)
                    entropy_loss = -entropies_tensor.mean()
            
                    total_loss = policy_loss + \
                                 config.VALUE_LOSS_COEF * value_loss + \
                                 config.ENTROPY_COEF * entropy_loss
            
                    # print(f"DEBUG learn: policy_loss={policy_loss.item():.4f}, value_loss={value_loss.item():.4f}, entropy_loss={entropy_loss.item():.4f}, total_loss={total_loss.item():.4f}")
            
                    self.optimizer.zero_grad()
                    total_loss.backward()
                    if hasattr(config, 'MAX_GRAD_NORM') and config.MAX_GRAD_NORM is not None:
                        torch.nn.utils.clip_grad_norm_(self.network.parameters(), config.MAX_GRAD_NORM)
                    self.optimizer.step()
            
                    self.clear_buffers()
            
                    return policy_loss.item(), value_loss.item(), entropy_loss.item()
            
                def clear_buffers(self):
                    self.log_probs_buffer = []
                    self.values_buffer = []
                    self.rewards_buffer = []
                    self.entropies_buffer = []
            --- SLUTT INNHOLD (agent.py) ---

        |-- config.py
            --- START INNHOLD (config.py) ---
            # agents/a2c_manual/config.py
            import torch
            
            # ---- Kartdimensjoner og Normalisering ----
            # Disse bør ideelt sett være dynamiske eller store nok for padding.
            # game_core.py har et lite 8x4 kart. Vi setter en MAKS forventet størrelse
            # for CNN input, men utils.py må håndtere mindre kart.
            MAP_WIDTH = 250
            MAP_HEIGHT = 250
            
            MAX_WORM_HEALTH = 100.0
            # MAX_X_POS og MAX_Y_POS brukes for normalisering i utils.py og bør
            # reflektere faktiske kartdimensjoner mottatt fra serveren, ikke padding-størrelsen.
            
            # ---- Modell Dimensjoner ----
            CNN_INPUT_CHANNELS = 1
            # Gitt AdaptiveMaxPool2d((6, 6)) og 32 output kanaler fra conv2, blir dette 32*6*6 = 1152
            CNN_FEATURE_DIM = 1152 # Dette er output etter CNN og pooling, før flattening
            
            # WORM_VECTOR_DIM: Hvilke features skal vi ha for ormene?
            # For aktiv orm: normalisert health, x, y
            # For andre ormer (potensielt): health, relativ x, relativ y, er fiende?
            # Enkel start: Kun egen orms health, x, y (normalisert)
            ACTIVE_WORM_FEATURE_DIM = 3 # health, x, y
            # La oss for nå kun fokusere på den aktive ormen.
            # Hvis vi vil inkludere andre ormer, må WORM_VECTOR_DIM økes.
            WORM_VECTOR_DIM = ACTIVE_WORM_FEATURE_DIM
            COMBINED_FEATURE_DIM = CNN_FEATURE_DIM + WORM_VECTOR_DIM
            
            # ---- Action Space Definisjoner (basert på json-docs.md og internt for nettverket) ----
            # Dette er rekkefølgen handlingene presenteres for nettverkets output-hode (policy).
            # Må matche output-laget i model.py og logikken i agent.py.
            # 'attack_...' er interne navn for nettverket for å skille våpentyper.
            NETWORK_ACTION_ORDER = ['stand', 'walk', 'attack_kick', 'attack_bazooka', 'attack_grenade']
            ACTION_DIM = len(NETWORK_ACTION_ORDER) # Antall diskrete hovedhandlinger for nettverket
            
            # Mapping fra nettverkets action navn til serverens action format
            # Brukes i utils.format_action
            SERVER_ACTION_MAPPING = {
                'stand': {'action': 'stand'},
                'walk': {'action': 'walk'}, # 'dx' parameter legges til dynamisk
                'attack_kick': {'action': 'attack', 'weapon': 'kick'}, # 'force' parameter legges til
                'attack_bazooka': {'action': 'attack', 'weapon': 'bazooka'}, # 'angle_deg' parameter
                'attack_grenade': {'action': 'attack', 'weapon': 'grenade'} # 'angle_deg', 'force' params
            }
            
            # ---- Parameter Space for Nettverket ----
            # 'walk' -> 'dx' (diskretiserte bins for nettverket)
            WALK_DX_BINS = 11 # Gir verdier fra -5 til +5 hvis sentrert rundt 0
            WALK_DX_MIN = -5.0 # Minste faktiske dx verdi
            WALK_DX_MAX = 5.0  # Største faktiske dx verdi
            
            # Kontinuerlige parametere (nettverket outputer mean og std for Normalfordeling)
            # Force (0-100)
            KICK_FORCE_PARAMS = 1 # Nettverket outputer 1 verdi for mean, 1 for std (totalt 2 nodes)
            BAZOOKA_ANGLE_PARAMS = 1
            GRENADE_ANGLE_PARAMS = 1
            GRENADE_FORCE_PARAMS = 1
            
            # ---- A2C Hyperparametre ----
            LEARNING_RATE = 0.0007
            GAMMA = 0.99  # Discount factor
            ENTROPY_COEF = 0.01
            VALUE_LOSS_COEF = 0.5
            MAX_GRAD_NORM = 0.5 # For gradient clipping (valgfritt, men ofte lurt)
            
            # ---- Trening ----
            NUM_EPISODES = 10000
            # STEPS_PER_UPDATE = 20 # For N-step A2C, hvis ikke episode-basert
            
            # ---- Websocket ----
            SERVER_HOST = '127.0.0.1'
            SERVER_PORT = 8765
            
            # ---- Agent ----
            PLAYER_ID = None # Vil bli satt av serveren ved 'ASSIGN_ID'
            
            # ---- Annet ----
            DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {DEVICE}")
            --- SLUTT INNHOLD (config.py) ---

        |-- main_a2c.py
            --- START INNHOLD (main_a2c.py) ---
            # agents/a2c_manual/main_a2c.py
            import asyncio
            import websockets
            import json
            import time
            import torch
            import numpy as np
            from .agent import A2CAgent
            from . import config  # Bruk relativ import
            
            
            # (Setup logging her hvis du vil, likt som i client.py eller server.py. For enkelhets skyld, print brukes her)
            
            async def run_agent():
                uri = f"ws://{config.SERVER_HOST}:{config.SERVER_PORT}"
                a2c_agent = A2CAgent()
                episode_count = 0
                all_episode_rewards_log = []  # For plotting
                losses_log = {'policy': [], 'value': [], 'entropy': []}  # For plotting
            
                checkpoint_path = "a2c_worms_checkpoint.pth"
                try:
                    a2c_agent.network.load_state_dict(torch.load(checkpoint_path, map_location=torch.device(config.DEVICE)))
                    print(f"Lastet sjekkpunkt for modellen fra {checkpoint_path}.")
                except FileNotFoundError:
                    print(f"Ingen sjekkpunkt funnet på {checkpoint_path}, starter med ny modell.")
                except Exception as e:
                    print(f"Kunne ikke laste sjekkpunkt: {e}")
            
                print(f"Starter {config.NUM_EPISODES} episoder med A2C agent...")
            
                for episode_idx in range(config.NUM_EPISODES):
                    episode_reward_sum = 0
                    # Agentens buffere tømmes i agent.learn() eller agent.clear_buffers()
                    # a2c_agent.clear_buffers() # Kan kalles her for ekstra sikkerhet
            
                    game_over_flag = False
                    last_received_state = None  # For å ha state tilgjengelig for learn() hvis tilkobling brytes
            
                    print(f"\n--- Starter Episode {episode_idx + 1}/{config.NUM_EPISODES} ---")
                    try:
                        async with websockets.connect(uri, ping_interval=20, ping_timeout=20) as websocket:
                            # 1. CONNECT og ASSIGN_ID
                            await websocket.send(json.dumps({"type": "CONNECT", "nick": "A2C-JetBot"}))
                            assign_id_msg_str = await websocket.recv()
                            assign_id_msg = json.loads(assign_id_msg_str)
            
                            if assign_id_msg.get("type") == "ASSIGN_ID":
                                player_id = assign_id_msg.get("player_id")
                                if player_id is None:
                                    print("FEIL: Fikk ikke player_id. Avslutter episoden.")
                                    await asyncio.sleep(1)  # Vent litt før neste forsøk
                                    continue  # Neste episode
                                a2c_agent.set_player_id(player_id)
                                print(f"Tilkoblet server. Tildelt Player ID: {a2c_agent.player_id}.")
                            else:
                                print(f"FEIL: Forventet ASSIGN_ID, fikk: {assign_id_msg}. Avslutter episoden.")
                                await asyncio.sleep(1)
                                continue
            
                            # Hoved meldingsløkke for episoden
                            async for message_str in websocket:
                                try:
                                    msg = json.loads(message_str)
                                    msg_type = msg.get("type")
                                    # print(f"DEBUG: Mottok: {msg_type} - {str(msg)[:150]}") # Kort debug
            
                                    if msg_type == "TURN_BEGIN":
                                        last_received_state = msg.get("state")  # Lagre for learn() ved evt. disconnect
                                        if msg.get("player_id") == a2c_agent.player_id:
                                            turn_idx = msg.get('turn_index', -1)
                                            print(f"  Min tur (P{a2c_agent.player_id}, Turn {turn_idx}). State mottatt.")
                                            if not last_received_state:
                                                print("  FEIL: TURN_BEGIN mangler 'state'. Hopper over handling.")
                                                continue
            
                                            action_to_send_obj = a2c_agent.select_action(last_received_state)
                                            action_payload = {
                                                "type": "ACTION",
                                                "player_id": a2c_agent.player_id,
                                                "action": action_to_send_obj
                                            }
                                            print(f"    Sender handling: {action_payload['action']}")
                                            await websocket.send(json.dumps(action_payload))
                                        # else: Ikke min tur, ignorer
            
                                    elif msg_type == "TURN_RESULT":
                                        # Kun lagre reward hvis det var vår handling
                                        if msg.get("player_id") == a2c_agent.player_id:
                                            reward = msg.get("reward", 0.0)
                                            a2c_agent.store_reward(reward)
                                            episode_reward_sum += reward
                                            print(f"  Mottok reward: {reward}. Total ep. reward: {episode_reward_sum:.2f}")
                                        last_received_state = msg.get("state")  # Oppdater uansett
            
                                    elif msg_type == "TURN_END":
                                        pass  # Vent på neste TURN_BEGIN
            
                                    elif msg_type == "GAME_OVER":
                                        print(f"GAME OVER mottatt. Vinner: {msg.get('winner_id')}. ")
                                        game_over_flag = True
                                        final_state = msg.get("final_state")
                                        # Hvis siste handling ikke var vår, har vi ikke en reward for final_state.
                                        # A2C lærer vanligvis fra (s, a, r, s_next).
                                        # Hvis det ikke var vår tur sist, er siste reward i buffer fra forrige handling.
                                        # V(final_state) vil være 0.
            
                                        # Sørg for at vi har en state å sende til learn, selv om det var en annen spillers tur sist
                                        current_state_for_learn = final_state if final_state else last_received_state
            
                                        if not a2c_agent.rewards_buffer:  # Hvis ingen handlinger ble tatt av agenten
                                            print("  Ingen handlinger/rewards lagret denne episoden. Ingen læring.")
                                        else:
                                            pol_l, val_l, ent_l = a2c_agent.learn(current_state_for_learn, True)  # True for done
                                            if pol_l is not None:
                                                losses_log['policy'].append(pol_l)
                                                losses_log['value'].append(val_l)
                                                losses_log['entropy'].append(ent_l)
                                                print(
                                                    f"  Lært fra episode. Losses - P: {pol_l:.4f}, V: {val_l:.4f}, E: {ent_l:.4f}")
                                        break  # Bryt meldingsløkken, episoden er ferdig
            
                                    elif msg_type == "ERROR":
                                        print(f"FEIL fra server: {msg.get('msg')}")
                                        # Vurder å tømme buffere hvis feilen gjør episoden ugyldig
                                        # a2c_agent.clear_buffers()
            
                                except json.JSONDecodeError:
                                    print(f"FEIL: Kunne ikke dekode JSON fra server: {message_str}")
                                except Exception as e_inner:
                                    print(f"FEIL i meldingsløkke: {type(e_inner).__name__} - {e_inner}")
                                    game_over_flag = True  # Anta episoden er korrupt
                                    break  # Bryt meldingsløkken
            
                            # Etter meldingsløkken (enten GAME_OVER eller disconnect)
                            if not game_over_flag and a2c_agent.rewards_buffer:  # Disconnect før GAME_OVER
                                print("Advarsel: Tilkobling lukket før GAME_OVER, men data samlet. Lærer (antar 'done').")
                                # Bruk sist kjente state hvis mulig
                                state_for_learn = last_received_state if last_received_state else {}
                                pol_l, val_l, ent_l = a2c_agent.learn(state_for_learn, True)
                                if pol_l is not None:
                                    losses_log['policy'].append(pol_l);
                                    losses_log['value'].append(val_l);
                                    losses_log['entropy'].append(ent_l)
            
                    except websockets.exceptions.ConnectionClosed as e:
                        print(f"Tilkobling lukket: {e}. Prøver neste episode.")
                        if not game_over_flag and a2c_agent.rewards_buffer:  # Lær hvis data finnes
                            print("  Lærer fra ufullstendig episode (ConnectionClosed).")
                            state_for_learn = last_received_state if last_received_state else {}
                            pol_l, val_l, ent_l = a2c_agent.learn(state_for_learn, True)
                            if pol_l: losses_log['policy'].append(pol_l); losses_log['value'].append(val_l); losses_log[
                                'entropy'].append(ent_l)
                    except ConnectionRefusedError:
                        print(f"FEIL: Kunne ikke koble til server {uri}. Er den startet? Venter 10s.")
                        await asyncio.sleep(10)
                        continue  # Prøv å starte en ny episodeforbindelse
                    except Exception as e_outer:
                        print(f"Alvorlig FEIL i episode {episode_idx + 1}: {type(e_outer).__name__} - {e_outer}")
                        if not game_over_flag and a2c_agent.rewards_buffer:
                            print("  Lærer fra ufullstendig episode (Alvorlig feil).")
                            state_for_learn = last_received_state if last_received_state else {}
                            pol_l, val_l, ent_l = a2c_agent.learn(state_for_learn, True)
                            if pol_l: losses_log['policy'].append(pol_l); losses_log['value'].append(val_l); losses_log[
                                'entropy'].append(ent_l)
                        await asyncio.sleep(5)  # Pause før neste forsøk
            
                    all_episode_rewards_log.append(episode_reward_sum)
                    print(f"--- Episode {episode_idx + 1} avsluttet. Total belønning: {episode_reward_sum:.2f} ---")
            
                    # Lagre sjekkpunkt periodisk
                    if (episode_idx + 1) % 50 == 0:
                        try:
                            torch.save(a2c_agent.network.state_dict(), checkpoint_path)
                            print(f"Lagret modell sjekkpunkt til {checkpoint_path} etter episode {episode_idx + 1}")
                        except Exception as e_save:
                            print(f"Kunne ikke lagre modell: {e_save}")
            
                    # Liten pause mellom episoder for å unngå å hamre serveren
                    await asyncio.sleep(0.1)
            
                print("\nTrening ferdig etter alle episoder.")
                # Plotting (samme som før)
                try:
                    import matplotlib.pyplot as plt
                    avg_rewards = []
                    if all_episode_rewards_log:
                        window_size = min(100, len(all_episode_rewards_log))
                        if window_size > 0:
                            avg_rewards = [np.mean(all_episode_rewards_log[max(0, i - window_size + 1):i + 1]) for i in
                                           range(len(all_episode_rewards_log))]
            
                    plt.figure(figsize=(18, 6))
                    plt.subplot(1, 3, 1)
                    plt.plot(all_episode_rewards_log, label='Rå belønning', alpha=0.6)
                    if avg_rewards: plt.plot(avg_rewards, label=f'Glidende gj.snitt ({window_size} ep)', color='red', linewidth=2)
                    plt.xlabel("Episode");
                    plt.ylabel("Total belønning");
                    plt.title("Belønning per episode")
                    plt.legend();
                    plt.grid(True)
            
                    if losses_log['policy']:  # Sjekk om det er noe å plotte
                        plt.subplot(1, 3, 2)
                        plt.plot(losses_log['policy'], label='Policy Loss')
                        plt.xlabel("Læringssteg");
                        plt.ylabel("Policy Loss");
                        plt.title("Policy (Actor) Tap")
                        plt.legend();
                        plt.grid(True)
            
                        plt.subplot(1, 3, 3)
                        plt.plot(losses_log['value'], label='Value Loss', color='green')
                        plt.xlabel("Læringssteg");
                        plt.ylabel("Value Loss");
                        plt.title("Value (Critic) Tap")
                        plt.legend();
                        plt.grid(True)
            
                    # Du kan også plotte entropi hvis du vil:
                    # if losses_log['entropy']:
                    #     plt.figure() # Ny figur for entropi
                    #     plt.plot(losses_log['entropy'], label='Entropy (neg)')
                    #     plt.xlabel("Læringssteg"); plt.ylabel("Entropy (neg)"); plt.title("Entropi")
                    #     plt.legend(); plt.grid(True)
            
                    plt.tight_layout()
                    plt.savefig("training_plots_worms_a2c_new_protocol.png")
                    print("Lagret treningsplot som training_plots_worms_a2c_new_protocol.png")
                    # plt.show() # Kommenter ut hvis du kjører uten GUI
                except ImportError:
                    print("Matplotlib ikke installert. Kan ikke plotte resultater.")
                except Exception as plot_e:
                    print(f"Kunne ikke plotte resultater: {type(plot_e).__name__} - {plot_e}")
            
            
            if __name__ == "__main__":
                try:
                    asyncio.run(run_agent())
                except KeyboardInterrupt:
                    print("\nTrening avbrutt av bruker.")
            --- SLUTT INNHOLD (main_a2c.py) ---

        |-- model.py
            --- START INNHOLD (model.py) ---
            # agents/a2c_manual/model.py
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            from . import config  # Bruk relativ import
            
            
            class ActorCriticNetwork(nn.Module):
                def __init__(self):
                    super(ActorCriticNetwork, self).__init__()
            
                    # --- CNN for Map Processing ---
                    self.conv1 = nn.Conv2d(config.CNN_INPUT_CHANNELS, 16, kernel_size=8, stride=4)
                    self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)
                    self.pool = nn.AdaptiveMaxPool2d((6, 6))  # Sikrer fast output-størrelse: [B, 32, 6, 6]
                    # config.CNN_FEATURE_DIM er satt til 1152 (32*6*6)
            
                    # --- Felles Fullt Tilkoblede Lag ---
                    self.fc_shared1 = nn.Linear(config.CNN_FEATURE_DIM + config.WORM_VECTOR_DIM, 256)
                    self.fc_shared2 = nn.Linear(256, 128)
            
                    # --- Actor Hoder ---
                    # 1. Hode for diskret action type (basert på config.NETWORK_ACTION_ORDER)
                    self.action_type_head = nn.Linear(128, config.ACTION_DIM)  # ACTION_DIM = len(NETWORK_ACTION_ORDER)
            
                    # 2. Parameterhoder
                    # Walk 'dx' (logits for diskrete bins)
                    self.walk_dx_head = nn.Linear(128, config.WALK_DX_BINS)
            
                    # Kick 'force' (mean og log_std for Normalfordeling)
                    self.kick_force_mean_head = nn.Linear(128, config.KICK_FORCE_PARAMS)
                    self.kick_force_log_std_head = nn.Linear(128, config.KICK_FORCE_PARAMS)
            
                    # Bazooka 'angle_deg' (mean og log_std)
                    self.bazooka_angle_mean_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
                    self.bazooka_angle_log_std_head = nn.Linear(128, config.BAZOOKA_ANGLE_PARAMS)
                    # Ingen force for bazooka ifølge json-docs.md
            
                    # Grenade 'angle_deg' (mean og log_std)
                    self.grenade_angle_mean_head = nn.Linear(128, config.GRENADE_ANGLE_PARAMS)
                    self.grenade_angle_log_std_head = nn.Linear(128, config.GRENADE_ANGLE_PARAMS)
                    # Grenade 'force' (mean og log_std)
                    self.grenade_force_mean_head = nn.Linear(128, config.GRENADE_FORCE_PARAMS)
                    self.grenade_force_log_std_head = nn.Linear(128, config.GRENADE_FORCE_PARAMS)
            
                    # --- Critic Hode ---
                    self.value_head = nn.Linear(128, 1)  # Outputter state value V(s)
            
                def forward(self, map_tensor, worm_vector_tensor):
                    # CNN
                    x_map = F.relu(self.conv1(map_tensor))
                    x_map = F.relu(self.conv2(x_map))
                    x_map = self.pool(x_map)
                    x_map = x_map.view(x_map.size(0), -1)  # Flatten til [BatchSize, CNN_FEATURE_DIM]
            
                    # Kombiner med worm data
                    if worm_vector_tensor.dim() == 1:  # Hvis batch size = 1 og worm_vector ikke har batch dim
                        worm_vector_tensor = worm_vector_tensor.unsqueeze(0)
            
                    try:
                        x_combined = torch.cat((x_map, worm_vector_tensor), dim=1)
                    except RuntimeError as e:
                        print(f"FEIL ved torch.cat: map_shape={x_map.shape}, worm_shape={worm_vector_tensor.shape}")
                        print(f"Forventet worm_vector_dim: {config.WORM_VECTOR_DIM}, CNN_feature_dim: {config.CNN_FEATURE_DIM}")
                        raise e
            
                    # Felles lag
                    x_shared = F.relu(self.fc_shared1(x_combined))
                    x_shared = F.relu(self.fc_shared2(x_shared))
            
                    # --- Actor Outputs ---
                    action_type_logits = self.action_type_head(x_shared)
                    action_type_probs = F.softmax(action_type_logits, dim=-1)
            
                    # Walk
                    walk_dx_logits = self.walk_dx_head(x_shared)
                    walk_dx_probs = F.softmax(walk_dx_logits, dim=-1)
            
                    # Kick
                    kick_force_mean = self.kick_force_mean_head(x_shared)
                    kick_force_log_std = self.kick_force_log_std_head(x_shared)
                    kick_force_std = torch.exp(kick_force_log_std.clamp(-20, 2))  # Stabilitet
            
                    # Bazooka
                    bazooka_angle_mean = self.bazooka_angle_mean_head(x_shared)
                    bazooka_angle_log_std = self.bazooka_angle_log_std_head(x_shared)
                    bazooka_angle_std = torch.exp(bazooka_angle_log_std.clamp(-20, 2))
            
                    # Grenade
                    grenade_angle_mean = self.grenade_angle_mean_head(x_shared)
                    grenade_angle_log_std = self.grenade_angle_log_std_head(x_shared)
                    grenade_angle_std = torch.exp(grenade_angle_log_std.clamp(-20, 2))
                    grenade_force_mean = self.grenade_force_mean_head(x_shared)
                    grenade_force_log_std = self.grenade_force_log_std_head(x_shared)
                    grenade_force_std = torch.exp(grenade_force_log_std.clamp(-20, 2))
            
                    actor_outputs = {
                        'action_type_probs': action_type_probs,
                        'walk_dx_probs': walk_dx_probs,
                        'kick_params': (kick_force_mean, kick_force_std),
                        'bazooka_params': (bazooka_angle_mean, bazooka_angle_std),  # Kun angle
                        'grenade_params': (grenade_angle_mean, grenade_angle_std, grenade_force_mean, grenade_force_std)
                    }
            
                    # --- Critic Output ---
                    state_value = self.value_head(x_shared)
            
                    return actor_outputs, state_value
            --- SLUTT INNHOLD (model.py) ---

        |-- utils.py
            --- START INNHOLD (utils.py) ---
            # agents/a2c_manual/utils.py
            import torch
            import numpy as np
            from . import config
            
            
            def preprocess_state(current_game_state_json, agent_player_id):
                """
                Konverterer game_state JSON (fra msg['state']) til tensorer klar for nettverket.
                Returnerer en tuple: (map_tensor, worm_vector_tensor)
                """
                try:
                    # `current_game_state_json` er nå det som var `new-environment` før,
                    # dvs. direkte innholdet i `msg['state']`.
                    map_data = current_game_state_json['map']
                    worms_data = current_game_state_json['worms']
            
                    # --- Kart Preprocessing ---
                    map_array = np.array(map_data, dtype=np.float32)
                    actual_map_h, actual_map_w = map_array.shape
            
                    # Padding/Cropping for å matche config.MAP_HEIGHT, config.MAP_WIDTH
                    processed_map_array = np.zeros((config.MAP_HEIGHT, config.MAP_WIDTH), dtype=np.float32)
            
                    copy_h_len = min(actual_map_h, config.MAP_HEIGHT)
                    copy_w_len = min(actual_map_w, config.MAP_WIDTH)
            
                    # Senter-justert padding/cropping
                    src_start_h = (actual_map_h - copy_h_len) // 2
                    src_start_w = (actual_map_w - copy_w_len) // 2
                    dst_start_h = (config.MAP_HEIGHT - copy_h_len) // 2
                    dst_start_w = (config.MAP_WIDTH - copy_w_len) // 2
            
                    processed_map_array[dst_start_h: dst_start_h + copy_h_len,
                    dst_start_w: dst_start_w + copy_w_len] = \
                        map_array[src_start_h: src_start_h + copy_h_len,
                        src_start_w: src_start_w + copy_w_len]
            
                    map_tensor = torch.from_numpy(processed_map_array).unsqueeze(0).unsqueeze(0).to(config.DEVICE)
            
                    # --- Worm Data Preprocessing ---
                    active_worm_features = [0.0] * config.ACTIVE_WORM_FEATURE_DIM  # Default til null-vektor
            
                    # agent_player_id er 1-basert. Orm ID i JSON er 0-basert.
                    my_worm_json_id = agent_player_id - 1
            
                    found_my_worm = False
                    for worm_info in worms_data:
                        if worm_info['id'] == my_worm_json_id:
                            # Normaliser aktiv orm data
                            # Bruk faktiske kartdimensjoner for normalisering av posisjon
                            norm_health = worm_info['health'] / config.MAX_WORM_HEALTH
                            # Klipp x og y for å unngå verdier utenfor [0,1] ved kanten av kartet
                            norm_x = np.clip(worm_info['x'] / float(actual_map_w - 1 if actual_map_w > 1 else 1), 0.0, 1.0)
                            norm_y = np.clip(worm_info['y'] / float(actual_map_h - 1 if actual_map_h > 1 else 1), 0.0, 1.0)
            
                            active_worm_features = [
                                np.clip(norm_health, 0.0, 1.0),
                                norm_x,
                                norm_y
                            ]
                            found_my_worm = True
                            break
            
                    if not found_my_worm:
                        # Dette kan skje hvis ormen er død og fjernet fra listen,
                        # eller hvis det er en feil i player_id matching.
                        # Agenten bør fortsatt få en input.
                        # print(f"Advarsel: Fant ikke orm data for agent player_id {agent_player_id} (intern id {my_worm_json_id})")
                        pass  # Bruker default null-vektor
            
                    worm_vector_tensor = torch.FloatTensor([active_worm_features]).to(config.DEVICE)
            
                    return map_tensor, worm_vector_tensor
            
                except KeyError as e:
                    print(
                        f"FEIL i preprocess_state: Manglende nøkkel '{e}' i game_state_json: {str(current_game_state_json)[:200]}...")
                    dummy_map = torch.zeros(1, config.CNN_INPUT_CHANNELS, config.MAP_HEIGHT, config.MAP_WIDTH).to(config.DEVICE)
                    dummy_worm = torch.zeros(1, config.WORM_VECTOR_DIM).to(config.DEVICE)
                    return dummy_map, dummy_worm
                except Exception as e:
                    print(
                        f"Uventet FEIL i preprocess_state: {type(e).__name__} - {e} med data {str(current_game_state_json)[:200]}...")
                    dummy_map = torch.zeros(1, config.CNN_INPUT_CHANNELS, config.MAP_HEIGHT, config.MAP_WIDTH).to(config.DEVICE)
                    dummy_worm = torch.zeros(1, config.WORM_VECTOR_DIM).to(config.DEVICE)
                    return dummy_map, dummy_worm
            
            
            def format_action(network_action_idx, params_from_network):
                """
                Formaterer valgt handling (fra nettverkets interne representasjon) og parametere
                til JSON-formatet som serveren forventer (basert på json-docs.md).
            
                Args:
                    network_action_idx (int): Indeksen til handlingen i config.NETWORK_ACTION_ORDER.
                    params_from_network (dict): En dictionary med samplede parametere fra nettverket.
                                                Nøklene bør matche det agent.py legger inn,
                                                f.eks. 'walk_dx_bin_idx', 'kick_force_val', etc.
                """
                network_action_name = config.NETWORK_ACTION_ORDER[network_action_idx]
            
                # Start med basis action type fra mapping
                if network_action_name not in config.SERVER_ACTION_MAPPING:
                    print(f"FEIL: Ukjent nettverkshandling '{network_action_name}' i format_action. Sender 'stand'.")
                    return {"action": "stand"}
            
                action_json = config.SERVER_ACTION_MAPPING[network_action_name].copy()  # Viktig med .copy()
            
                # Legg til/modifiser parametere basert på nettverkets output
                if network_action_name == 'walk':
                    # Konverter bin-indeks for dx tilbake til en faktisk dx-verdi
                    # Eks: 11 bins (-5 til +5). Bin 0 -> -5, Bin 5 -> 0, Bin 10 -> +5
                    # Formel: verdi = min_verdi + bin_idx * ( (max_verdi - min_verdi) / (antall_bins - 1) )
                    # Eller enklere: (bin_idx - (antall_bins // 2)) * step_size, hvis step_size er 1.
                    dx_bin_idx = params_from_network.get('walk_dx_bin_idx', config.WALK_DX_BINS // 2)  # Default til midten (0)
                    dx_value = config.WALK_DX_MIN + dx_bin_idx * \
                               ((config.WALK_DX_MAX - config.WALK_DX_MIN) / (config.WALK_DX_BINS - 1))
                    action_json['dx'] = float(dx_value)
            
                elif network_action_name == 'attack_kick':
                    # Force er 0-100
                    force_val = params_from_network.get('kick_force_val', 50.0)  # Default til 50
                    action_json['force'] = float(np.clip(force_val, 0.0, 100.0))
            
                elif network_action_name == 'attack_bazooka':
                    angle_val = params_from_network.get('bazooka_angle_val', 0.0)  # Default til 0
                    action_json['angle_deg'] = float(angle_val)
                    # Merk: Bazooka har ikke 'force' i json-docs.md ACTION, kun i client.py eksempelet.
                    # Hvis serveren forventer det, må det legges til i json-docs og her.
            
                elif network_action_name == 'attack_grenade':
                    angle_val = params_from_network.get('grenade_angle_val', 0.0)
                    action_json['angle_deg'] = float(angle_val)
                    force_val = params_from_network.get('grenade_force_val', 50.0)
                    action_json['force'] = float(np.clip(force_val, 0.0, 100.0))
            
                # 'stand' trenger ingen ekstra parametere utover det som er i SERVER_ACTION_MAPPING
            
                return action_json
            --- SLUTT INNHOLD (utils.py) ---

environment/
    |-- game_core.py
        --- START INNHOLD (game_core.py) ---
        # environment/game_core.py
        
        import copy
        import logging
        from typing import Any, Dict, Tuple
        
        logger = logging.getLogger(__name__)
        
        class GameCore:
            def __init__(self) -> None:
                self.state: Dict[str, Any] = self.initial_state()
        
            def initial_state(self) -> Dict[str, Any]:
                return {
                    "worms": [
                        {"id": 0, "health": 100, "x": 3.20, "y": 1.75},
                        {"id": 1, "health": 100, "x": 6.00, "y": 2.00},
                    ],
                    "map": [
                        [1, 0, 0, 0, 0, 0, 0, 0],
                        [1, 0, 0, 0, 0, 0, 0, 0],
                        [1, 1, 1, 0, 0, 0, 1, 0],
                        [1, 1, 1, 0, 0, 1, 1, 1],
                    ],
                }
        
            def expected_players(self) -> int:
                return 2
        
            def step(self, player_id: int, action: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
                logger.trace("GameCore.step: player_id=%d action=%r", player_id, action)
                state = self.state
                worms = state["worms"]
                grid = state["map"]
                rows = len(grid)
                cols = len(grid[0])
        
                idx = player_id - 1
                worm = worms[idx]
        
                if action.get("action") == "walk":
                    dx = float(action.get("dx", 0.0))
                    new_x = worm["x"] + dx
                    new_x = max(0.0, min(new_x, cols - 1e-6))
                    ty = int(worm["y"])
                    tx = int(new_x)
                    if 0 <= ty < rows and grid[ty][tx] == 0:
                        worm["x"] = new_x
        
                return copy.deepcopy(state), 0.0
        
            def game_over(self) -> bool:
                return False
        
            def final_info(self) -> Dict[str, Any]:
                return {"winner_id": 0, "final_state": self.state}
        --- SLUTT INNHOLD (game_core.py) ---

    |-- server.py
        --- START INNHOLD (server.py) ---
        #!/usr/bin/env python3
        """Turn-based WebSocket server for W.O.R.M.S. (websockets ≥ 12.x)."""
        from __future__ import annotations
        
        import argparse
        import asyncio
        import json
        import logging
        import sys
        from enum import IntEnum
        from pathlib import Path
        from typing import Any
        
        import websockets
        from websockets.exceptions import ConnectionClosed
        
        # ─── Define custom TRACE level ─────────────────────────────────────
        TRACE_LEVEL_NUM = 5
        logging.addLevelName(TRACE_LEVEL_NUM, "TRACE")
        logging.TRACE = TRACE_LEVEL_NUM                      # make logging.TRACE available
        def trace(self, message, *args, **kwargs):
            if self.isEnabledFor(TRACE_LEVEL_NUM):
                self._log(TRACE_LEVEL_NUM, message, args, **kwargs)
        logging.Logger.trace = trace
        
        # ─── Parse --log-level & configure ────────────────────────────────
        def setup_logging() -> logging.Logger:
            parser = argparse.ArgumentParser(description="W.O.R.M.S. server")
            parser.add_argument(
                "--log-level",
                choices=["TRACE", "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                default="INFO",
                help="set logging level"
            )
            args = parser.parse_args()
            level = getattr(logging, args.log_level)
            logging.basicConfig(
                level=level,
                format="%(asctime)s %(levelname)-8s %(name)s: %(message)s",
                datefmt="%H:%M:%S"
            )
            return logging.getLogger("server")
        
        logger = setup_logging()
        
        # ─── Import game logic ─────────────────────────────────────────────
        sys.path.append(str(Path(__file__).resolve().parent))
        from game_core import GameCore  # noqa: E402
        
        HOST, PORT = "127.0.0.1", 8765
        
        class WSState(IntEnum):
            CONNECTING = 0
            OPEN       = 1
            CLOSING    = 2
            CLOSED     = 3
        
        class WormsServer:
            def __init__(self) -> None:
                self.core = GameCore()
                self.clients: dict[Any, int] = {}
                self.turn_order: list[Any] = []
                self.idx = 0
                self.turn_counter = 0
                self.game_started = False
        
            async def accept(self, ws: Any) -> None:
                try:
                    raw = await asyncio.wait_for(ws.recv(), timeout=10)
                    msg = json.loads(raw)
                except (asyncio.TimeoutError, json.JSONDecodeError):
                    await ws.close(code=4000, reason="Expected CONNECT")
                    return
        
                if msg.get("type") != "CONNECT":
                    await ws.close(code=4002, reason="First message must be CONNECT")
                    return
        
                pid = len(self.clients) + 1
                self.clients[ws] = pid
                self.turn_order.append(ws)
                await ws.send(json.dumps({"type": "ASSIGN_ID", "player_id": pid}))
                logger.info("new connection: player %d (%s)", pid, msg.get("nick", "?"))
        
                if not self.game_started and len(self.turn_order) == self.core.expected_players():
                    self.game_started = True
                    logger.info("roster full (%d players), starting game", len(self.turn_order))
                    asyncio.create_task(self.game_loop())
        
                try:
                    await ws.wait_closed()
                finally:
                    self.remove(ws)
        
            async def game_loop(self) -> None:
                logger.info("game loop started")
                while self.turn_order:
                    if self.idx >= len(self.turn_order):
                        self.idx = 0
        
                    if self.core.game_over():
                        await self.broadcast({"type": "GAME_OVER", **self.core.final_info()})
                        logger.info("game over")
                        return
        
                    ws = self.turn_order[self.idx]
                    if ws.state != WSState.OPEN:
                        self.remove(ws, quiet=True)
                        continue
        
                    pid = self.clients[ws]
                    begin_msg = {
                        "type": "TURN_BEGIN",
                        "turn_index": self.turn_counter,
                        "player_id": pid,
                        "state": self.core.state,
                        "time_limit_ms": 15_000,
                    }
                    logger.trace("→ TURN_BEGIN to player %d", pid)
                    if not await self.safe_send(ws, begin_msg):
                        continue
        
                    try:
                        raw = await asyncio.wait_for(ws.recv(), timeout=15)
                        logger.trace("← raw message from %d: %s", pid, raw)
                        msg = json.loads(raw)
                        if msg.get("type") != "ACTION" or msg.get("player_id") != pid:
                            raise ValueError
                    except (asyncio.TimeoutError, ValueError):
                        await self.safe_send(ws, {"type": "ERROR", "msg": "timeout or invalid action"})
                        logger.warning("player %d timed out or sent invalid action", pid)
                        self.idx += 1
                        self.turn_counter += 1
                        continue
                    except ConnectionClosed:
                        self.remove(ws)
                        continue
        
                    action = msg.get("action", {})
                    new_state, reward = self.core.step(pid, action)
                    logger.trace("applied action %r for player %d → reward %.2f", action, pid, reward)
        
                    await self.broadcast({
                        "type": "TURN_RESULT",
                        "turn_index": self.turn_counter,
                        "player_id": pid,
                        "state": new_state,
                        "reward": reward,
                    })
        
                    self.idx += 1
                    self.turn_counter += 1
                    if self.turn_order:
                        next_ws = self.turn_order[self.idx % len(self.turn_order)]
                        await self.broadcast({
                            "type": "TURN_END",
                            "next_player_id": self.clients[next_ws],
                        })
        
                logger.info("match ended (no players left)")
        
            async def broadcast(self, msg: dict) -> None:
                data = json.dumps(msg)
                logger.trace("broadcasting %s", msg.get("type"))
                dead: list[Any] = []
                for ws in list(self.turn_order):
                    try:
                        await ws.send(data)
                    except ConnectionClosed:
                        dead.append(ws)
                for ws in dead:
                    self.remove(ws, quiet=True)
        
            async def safe_send(self, ws: Any, msg: dict) -> bool:
                try:
                    await ws.send(json.dumps(msg))
                    return True
                except ConnectionClosed:
                    self.remove(ws, quiet=True)
                    return False
        
            def remove(self, ws: Any, *, quiet: bool = False) -> None:
                if ws in self.turn_order:
                    idx = self.turn_order.index(ws)
                    self.turn_order.remove(ws)
                    if idx <= self.idx and self.idx > 0:
                        self.idx -= 1
                if ws in self.clients:
                    pid = self.clients.pop(ws)
                    if not quiet:
                        logger.info("player %d disconnected", pid)
        
        async def main() -> None:
            async with websockets.serve(WormsServer().accept, HOST, PORT):
                logger.info("Listening on ws://%s:%d", HOST, PORT)
                await asyncio.Future()  # run forever
        
        if __name__ == "__main__":
            asyncio.run(main())
        --- SLUTT INNHOLD (server.py) ---

frontend/
    |-- package-lock.json
        --- START INNHOLD (package-lock.json) ---
        {
          "name": "relearning",
          "version": "0.0.1",
          "lockfileVersion": 3,
          "requires": true,
          "packages": {
            "": {
              "name": "relearning",
              "version": "0.0.1",
              "dependencies": {
                "esbuild": "^0.25.4",
                "pixi.js": "^8.9.2"
              }
            },
            "node_modules/@esbuild/aix-ppc64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.4.tgz",
              "integrity": "sha512-1VCICWypeQKhVbE9oW/sJaAmjLxhVqacdkvPLEjwlttjfwENRSClS8EjBz0KzRyFSCPDIkuXW34Je/vk7zdB7Q==",
              "cpu": [
                "ppc64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "aix"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-arm": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.4.tgz",
              "integrity": "sha512-QNdQEps7DfFwE3hXiU4BZeOV68HHzYwGd0Nthhd3uCkkEKK7/R6MTgM0P7H7FAs5pU/DIWsviMmEGxEoxIZ+ZQ==",
              "cpu": [
                "arm"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.4.tgz",
              "integrity": "sha512-bBy69pgfhMGtCnwpC/x5QhfxAz/cBgQ9enbtwjf6V9lnPI/hMyT9iWpR1arm0l3kttTr4L0KSLpKmLp/ilKS9A==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/android-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.4.tgz",
              "integrity": "sha512-TVhdVtQIFuVpIIR282btcGC2oGQoSfZfmBdTip2anCaVYcqWlZXGcdcKIUklfX2wj0JklNYgz39OBqh2cqXvcQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "android"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/darwin-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.4.tgz",
              "integrity": "sha512-Y1giCfM4nlHDWEfSckMzeWNdQS31BQGs9/rouw6Ub91tkK79aIMTH3q9xHvzH8d0wDru5Ci0kWB8b3up/nl16g==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "darwin"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/darwin-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.4.tgz",
              "integrity": "sha512-CJsry8ZGM5VFVeyUYB3cdKpd/H69PYez4eJh1W/t38vzutdjEjtP7hB6eLKBoOdxcAlCtEYHzQ/PJ/oU9I4u0A==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "darwin"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/freebsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.4.tgz",
              "integrity": "sha512-yYq+39NlTRzU2XmoPW4l5Ifpl9fqSk0nAJYM/V/WUGPEFfek1epLHJIkTQM6bBs1swApjO5nWgvr843g6TjxuQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "freebsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/freebsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.4.tgz",
              "integrity": "sha512-0FgvOJ6UUMflsHSPLzdfDnnBBVoCDtBTVyn/MrWloUNvq/5SFmh13l3dvgRPkDihRxb77Y17MbqbCAa2strMQQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "freebsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-arm": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.4.tgz",
              "integrity": "sha512-kro4c0P85GMfFYqW4TWOpvmF8rFShbWGnrLqlzp4X1TNWjRY3JMYUfDCtOxPKOIY8B0WC8HN51hGP4I4hz4AaQ==",
              "cpu": [
                "arm"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.4.tgz",
              "integrity": "sha512-+89UsQTfXdmjIvZS6nUnOOLoXnkUTB9hR5QAeLrQdzOSWZvNSAXAtcRDHWtqAUtAmv7ZM1WPOOeSxDzzzMogiQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-ia32": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.4.tgz",
              "integrity": "sha512-yTEjoapy8UP3rv8dB0ip3AfMpRbyhSN3+hY8mo/i4QXFeDxmiYbEKp3ZRjBKcOP862Ua4b1PDfwlvbuwY7hIGQ==",
              "cpu": [
                "ia32"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-loong64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.4.tgz",
              "integrity": "sha512-NeqqYkrcGzFwi6CGRGNMOjWGGSYOpqwCjS9fvaUlX5s3zwOtn1qwg1s2iE2svBe4Q/YOG1q6875lcAoQK/F4VA==",
              "cpu": [
                "loong64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-mips64el": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.4.tgz",
              "integrity": "sha512-IcvTlF9dtLrfL/M8WgNI/qJYBENP3ekgsHbYUIzEzq5XJzzVEV/fXY9WFPfEEXmu3ck2qJP8LG/p3Q8f7Zc2Xg==",
              "cpu": [
                "mips64el"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-ppc64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.4.tgz",
              "integrity": "sha512-HOy0aLTJTVtoTeGZh4HSXaO6M95qu4k5lJcH4gxv56iaycfz1S8GO/5Jh6X4Y1YiI0h7cRyLi+HixMR+88swag==",
              "cpu": [
                "ppc64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-riscv64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.4.tgz",
              "integrity": "sha512-i8JUDAufpz9jOzo4yIShCTcXzS07vEgWzyX3NH2G7LEFVgrLEhjwL3ajFE4fZI3I4ZgiM7JH3GQ7ReObROvSUA==",
              "cpu": [
                "riscv64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-s390x": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.4.tgz",
              "integrity": "sha512-jFnu+6UbLlzIjPQpWCNh5QtrcNfMLjgIavnwPQAfoGx4q17ocOU9MsQ2QVvFxwQoWpZT8DvTLooTvmOQXkO51g==",
              "cpu": [
                "s390x"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/linux-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.4.tgz",
              "integrity": "sha512-6e0cvXwzOnVWJHq+mskP8DNSrKBr1bULBvnFLpc1KY+d+irZSgZ02TGse5FsafKS5jg2e4pbvK6TPXaF/A6+CA==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "linux"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/netbsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.4.tgz",
              "integrity": "sha512-vUnkBYxZW4hL/ie91hSqaSNjulOnYXE1VSLusnvHg2u3jewJBz3YzB9+oCw8DABeVqZGg94t9tyZFoHma8gWZQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "netbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/netbsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.4.tgz",
              "integrity": "sha512-XAg8pIQn5CzhOB8odIcAm42QsOfa98SBeKUdo4xa8OvX8LbMZqEtgeWE9P/Wxt7MlG2QqvjGths+nq48TrUiKw==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "netbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/openbsd-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.4.tgz",
              "integrity": "sha512-Ct2WcFEANlFDtp1nVAXSNBPDxyU+j7+tId//iHXU2f/lN5AmO4zLyhDcpR5Cz1r08mVxzt3Jpyt4PmXQ1O6+7A==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "openbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/openbsd-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.4.tgz",
              "integrity": "sha512-xAGGhyOQ9Otm1Xu8NT1ifGLnA6M3sJxZ6ixylb+vIUVzvvd6GOALpwQrYrtlPouMqd/vSbgehz6HaVk4+7Afhw==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "openbsd"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/sunos-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.4.tgz",
              "integrity": "sha512-Mw+tzy4pp6wZEK0+Lwr76pWLjrtjmJyUB23tHKqEDP74R3q95luY/bXqXZeYl4NYlvwOqoRKlInQialgCKy67Q==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "sunos"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-arm64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.4.tgz",
              "integrity": "sha512-AVUP428VQTSddguz9dO9ngb+E5aScyg7nOeJDrF1HPYu555gmza3bDGMPhmVXL8svDSoqPCsCPjb265yG/kLKQ==",
              "cpu": [
                "arm64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-ia32": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.4.tgz",
              "integrity": "sha512-i1sW+1i+oWvQzSgfRcxxG2k4I9n3O9NRqy8U+uugaT2Dy7kLO9Y7wI72haOahxceMX8hZAzgGou1FhndRldxRg==",
              "cpu": [
                "ia32"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@esbuild/win32-x64": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.4.tgz",
              "integrity": "sha512-nOT2vZNw6hJ+z43oP1SPea/G/6AbN6X+bGNhNuq8NtRHy4wsMhw765IKLNmnjek7GvjWBYQ8Q5VBoYTFg9y1UQ==",
              "cpu": [
                "x64"
              ],
              "license": "MIT",
              "optional": true,
              "os": [
                "win32"
              ],
              "engines": {
                "node": ">=18"
              }
            },
            "node_modules/@pixi/colord": {
              "version": "2.9.6",
              "resolved": "https://registry.npmjs.org/@pixi/colord/-/colord-2.9.6.tgz",
              "integrity": "sha512-nezytU2pw587fQstUu1AsJZDVEynjskwOL+kibwcdxsMBFqPsFFNA7xl0ii/gXuDi6M0xj3mfRJj8pBSc2jCfA==",
              "license": "MIT"
            },
            "node_modules/@types/css-font-loading-module": {
              "version": "0.0.12",
              "resolved": "https://registry.npmjs.org/@types/css-font-loading-module/-/css-font-loading-module-0.0.12.tgz",
              "integrity": "sha512-x2tZZYkSxXqWvTDgveSynfjq/T2HyiZHXb00j/+gy19yp70PHCizM48XFdjBCWH7eHBD0R5i/pw9yMBP/BH5uA==",
              "license": "MIT"
            },
            "node_modules/@types/earcut": {
              "version": "2.1.4",
              "resolved": "https://registry.npmjs.org/@types/earcut/-/earcut-2.1.4.tgz",
              "integrity": "sha512-qp3m9PPz4gULB9MhjGID7wpo3gJ4bTGXm7ltNDsmOvsPduTeHp8wSW9YckBj3mljeOh4F0m2z/0JKAALRKbmLQ==",
              "license": "MIT"
            },
            "node_modules/@webgpu/types": {
              "version": "0.1.60",
              "resolved": "https://registry.npmjs.org/@webgpu/types/-/types-0.1.60.tgz",
              "integrity": "sha512-8B/tdfRFKdrnejqmvq95ogp8tf52oZ51p3f4QD5m5Paey/qlX4Rhhy5Y8tgFMi7Ms70HzcMMw3EQjH/jdhTwlA==",
              "license": "BSD-3-Clause"
            },
            "node_modules/@xmldom/xmldom": {
              "version": "0.8.10",
              "resolved": "https://registry.npmjs.org/@xmldom/xmldom/-/xmldom-0.8.10.tgz",
              "integrity": "sha512-2WALfTl4xo2SkGCYRt6rDTFfk9R1czmBvUQy12gK2KuRKIpWEhcbbzy8EZXtz/jkRqHX8bFEc6FC1HjX4TUWYw==",
              "license": "MIT",
              "engines": {
                "node": ">=10.0.0"
              }
            },
            "node_modules/earcut": {
              "version": "2.2.4",
              "resolved": "https://registry.npmjs.org/earcut/-/earcut-2.2.4.tgz",
              "integrity": "sha512-/pjZsA1b4RPHbeWZQn66SWS8nZZWLQQ23oE3Eam7aroEFGEvwKAsJfZ9ytiEMycfzXWpca4FA9QIOehf7PocBQ==",
              "license": "ISC"
            },
            "node_modules/esbuild": {
              "version": "0.25.4",
              "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.4.tgz",
              "integrity": "sha512-8pgjLUcUjcgDg+2Q4NYXnPbo/vncAY4UmyaCm0jZevERqCHZIaWwdJHkf8XQtu4AxSKCdvrUbT0XUr1IdZzI8Q==",
              "hasInstallScript": true,
              "license": "MIT",
              "bin": {
                "esbuild": "bin/esbuild"
              },
              "engines": {
                "node": ">=18"
              },
              "optionalDependencies": {
                "@esbuild/aix-ppc64": "0.25.4",
                "@esbuild/android-arm": "0.25.4",
                "@esbuild/android-arm64": "0.25.4",
                "@esbuild/android-x64": "0.25.4",
                "@esbuild/darwin-arm64": "0.25.4",
                "@esbuild/darwin-x64": "0.25.4",
                "@esbuild/freebsd-arm64": "0.25.4",
                "@esbuild/freebsd-x64": "0.25.4",
                "@esbuild/linux-arm": "0.25.4",
                "@esbuild/linux-arm64": "0.25.4",
                "@esbuild/linux-ia32": "0.25.4",
                "@esbuild/linux-loong64": "0.25.4",
                "@esbuild/linux-mips64el": "0.25.4",
                "@esbuild/linux-ppc64": "0.25.4",
                "@esbuild/linux-riscv64": "0.25.4",
                "@esbuild/linux-s390x": "0.25.4",
                "@esbuild/linux-x64": "0.25.4",
                "@esbuild/netbsd-arm64": "0.25.4",
                "@esbuild/netbsd-x64": "0.25.4",
                "@esbuild/openbsd-arm64": "0.25.4",
                "@esbuild/openbsd-x64": "0.25.4",
                "@esbuild/sunos-x64": "0.25.4",
                "@esbuild/win32-arm64": "0.25.4",
                "@esbuild/win32-ia32": "0.25.4",
                "@esbuild/win32-x64": "0.25.4"
              }
            },
            "node_modules/eventemitter3": {
              "version": "5.0.1",
              "resolved": "https://registry.npmjs.org/eventemitter3/-/eventemitter3-5.0.1.tgz",
              "integrity": "sha512-GWkBvjiSZK87ELrYOSESUYeVIc9mvLLf/nXalMOS5dYrgZq9o5OVkbZAVM06CVxYsCwH9BDZFPlQTlPA1j4ahA==",
              "license": "MIT"
            },
            "node_modules/gifuct-js": {
              "version": "2.1.2",
              "resolved": "https://registry.npmjs.org/gifuct-js/-/gifuct-js-2.1.2.tgz",
              "integrity": "sha512-rI2asw77u0mGgwhV3qA+OEgYqaDn5UNqgs+Bx0FGwSpuqfYn+Ir6RQY5ENNQ8SbIiG/m5gVa7CD5RriO4f4Lsg==",
              "license": "MIT",
              "dependencies": {
                "js-binary-schema-parser": "^2.0.3"
              }
            },
            "node_modules/ismobilejs": {
              "version": "1.1.1",
              "resolved": "https://registry.npmjs.org/ismobilejs/-/ismobilejs-1.1.1.tgz",
              "integrity": "sha512-VaFW53yt8QO61k2WJui0dHf4SlL8lxBofUuUmwBo0ljPk0Drz2TiuDW4jo3wDcv41qy/SxrJ+VAzJ/qYqsmzRw==",
              "license": "MIT"
            },
            "node_modules/js-binary-schema-parser": {
              "version": "2.0.3",
              "resolved": "https://registry.npmjs.org/js-binary-schema-parser/-/js-binary-schema-parser-2.0.3.tgz",
              "integrity": "sha512-xezGJmOb4lk/M1ZZLTR/jaBHQ4gG/lqQnJqdIv4721DMggsa1bDVlHXNeHYogaIEHD9vCRv0fcL4hMA+Coarkg==",
              "license": "MIT"
            },
            "node_modules/parse-svg-path": {
              "version": "0.1.2",
              "resolved": "https://registry.npmjs.org/parse-svg-path/-/parse-svg-path-0.1.2.tgz",
              "integrity": "sha512-JyPSBnkTJ0AI8GGJLfMXvKq42cj5c006fnLz6fXy6zfoVjJizi8BNTpu8on8ziI1cKy9d9DGNuY17Ce7wuejpQ==",
              "license": "MIT"
            },
            "node_modules/pixi.js": {
              "version": "8.9.2",
              "resolved": "https://registry.npmjs.org/pixi.js/-/pixi.js-8.9.2.tgz",
              "integrity": "sha512-oLFBkOOA/O6OpT5T8o05AxgZB9x9yWNzEQ+WTNZZFoCvfU2GdT4sFTjpVFuHQzgZPmAm/1IFhKdNiXVnlL8PRw==",
              "license": "MIT",
              "dependencies": {
                "@pixi/colord": "^2.9.6",
                "@types/css-font-loading-module": "^0.0.12",
                "@types/earcut": "^2.1.4",
                "@webgpu/types": "^0.1.40",
                "@xmldom/xmldom": "^0.8.10",
                "earcut": "^2.2.4",
                "eventemitter3": "^5.0.1",
                "gifuct-js": "^2.1.2",
                "ismobilejs": "^1.1.1",
                "parse-svg-path": "^0.1.2"
              }
            }
          }
        }
        --- SLUTT INNHOLD (package-lock.json) ---

    |-- package.json
        --- START INNHOLD (package.json) ---
        {
          "name": "relearning",
          "version": "0.0.1",
          "author": "Ask Sødal <asksodal@gmail.com>",
          "scripts": {
            "watch": "esbuild --bundle src/script.js --outfile=public/bundle.js --watch",
            "serve": "esbuild --bundle src/script.js --outfile=public/bundle.js --servedir=public"
          },
          "dependencies": {
            "pixi.js": "^8.9.2",
            "esbuild": "^0.25.4"
          }
        }
        --- SLUTT INNHOLD (package.json) ---

    public/
    src/